{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d14fea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2198a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessed_df100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045cbebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['identifier', 'gt text', 'ocr text', 'CER', 'WER', 'source',\n",
      "       'word count gt', 'word count ocr', 'century', 'gt sentences matched',\n",
      "       'ocr sentences matched', 'CER matched sentences',\n",
      "       'WER matched sentences', 'avg sentence length gt (fuzzy matched)',\n",
      "       'avg sentence length ocr (fuzzy matched)',\n",
      "       'sentences gt (fuzzy matched)', 'sentences ocr (fuzzy matched)',\n",
      "       'word count gt (fuzzy matched)', 'word count ocr (fuzzy matched)',\n",
      "       'aligned_GT_sentences', 'aligned_OCR_sentences', 'good_alignments',\n",
      "       'bad_alignments'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# select relevant columns\n",
    "print(df.columns)\n",
    "df = df[['identifier', 'gt text org', 'ocr text org', 'gt text', 'ocr text', 'source', 'century', 'gt sentences matched', 'aligned_GT_sentences', 'aligned_OCR_sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f2bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c29ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess for models\n",
    "def replace_num(text, character):\n",
    "    numbers = re.findall(r'\\d+', text) \n",
    "    res = list(map(int, numbers))\n",
    "    for numText in res:\n",
    "        text = text.replace(str(numText), character)\n",
    "    return text\n",
    "\n",
    "def replace_proper_nouns(text, character, names): \n",
    "    res = list(map(str, names))\n",
    "    for nounText in res:\n",
    "        text = text.replace(str(nounText), character)\n",
    "    return text\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "df['gt for training'] = df['gt sentences matched']\n",
    "# replace numbers\n",
    "df['gt for training'] = df['gt for training'].apply(lambda x: replace_num(x, '%NUMBER%'))\n",
    "# replace proper nouns\n",
    "df['gt for training'] = df['gt for training'].replace_proper_nouns(x, \"%NNP%\", gt_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e638e2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parts for cross-validation, train, validation oand test set\n",
    "df_17th = df[df['century'] == '1600s']\n",
    "df_18th = df[df['century'] == '1700s']\n",
    "df_19th = df[df['century'] == '1800s']\n",
    "df_20th = df[df['century'] == '1900s']\n",
    "\n",
    "datasets = [df_17th, df_18th, df_19th, df_20th]\n",
    "# create a list of train, validation and test set\n",
    "\n",
    "train = pd.DataFrame()\n",
    "val = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "for dataset in datasets:\n",
    "    splits = np.array_split(dataset, 5)\n",
    "    train_sub = splits[:3]\n",
    "    train_sub = pd.concat(train_sub)\n",
    "    val_sub = splits[3]\n",
    "    test_sub = splits[4]\n",
    "    train = pd.concat([train, train_sub])\n",
    "    val = pd.concat([val, val_sub])\n",
    "    test = pd.concat([test, test_sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930905ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine_tuning the models\n",
    "\n",
    "word2vec_models = []\n",
    "BERT_models = []\n",
    "\n",
    "def finetune_word2vec(train, window=5):\n",
    "    sentences = train.split('.')\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sentences = [tokenizer.tokenize(i) for i in sentences]\n",
    "    total_examples = len(sentences)\n",
    "    \n",
    "    model_w2v = Word2Vec(size=160, min_count=1, window=window)\n",
    "    model_w2v.build_vocab(sentences)\n",
    "    total_examples = model_w2v.corpus_count\n",
    "    model = KeyedVectors.load_word2vec_format(r\"combined-160.txt\", binary=False)\n",
    "    model_w2v.build_vocab([list(model.vocab.keys())], update=True)\n",
    "    model_w2v.intersect_word2vec_format(r\"combined-160.txt\", binary=False, lockf=1.0)\n",
    "    model_w2v.train(sentences, total_examples=total_examples, epochs=model_w2v.iter)\n",
    "    return model_w2v\n",
    "    \n",
    "def finetune_BERT(train):\n",
    "    \n",
    "    class TrainingDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings):\n",
    "            self.encodings = encodings\n",
    "        def __getitem__(self, idx):\n",
    "            return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        def __len__(self):\n",
    "            return len(self.encodings.input_ids)\n",
    "    \n",
    "    text = train.split('.')\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "    model = BertForMaskedLM.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "    # BERT tuning code from https://github.com/jamescalam/transformers/blob/main/course/training/03_mlm_training.ipynb\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "    inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "    # create random array of floats with equal dimensions to input_ids tensor\n",
    "    rand = torch.rand(inputs.input_ids.shape)\n",
    "    # create mask array\n",
    "    mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "           (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
    "    # we take take the indices of each True value, within each individual vector.\n",
    "    selection = []\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        selection.append(\n",
    "            torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        )\n",
    "    # apply these indices to each respective row in input_ids, assigning each of the values at these indices as 103.\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        inputs.input_ids[i, selection[i]] = 103\n",
    "    # initialize data\n",
    "    dataset = TrainingDataset(inputs)\n",
    "    \n",
    "    # start training loop\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "    #device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    # and move our model over to the selected device\n",
    "    #model.to(device)\n",
    "    # activate training mode\n",
    "    model.train()\n",
    "    # initialize optimizer\n",
    "    optim = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    epochs = 2\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # setup loop with TQDM and dataloader\n",
    "        loop = tqdm(loader, leave=True)\n",
    "        for batch in loop:\n",
    "            # initialize calculated gradients (from prev step)\n",
    "            optim.zero_grad()\n",
    "            # pull all tensor batches required for training\n",
    "            #input_ids = batch['input_ids'].to(device)\n",
    "            #attention_mask = batch['attention_mask'].to(device)\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            #labels = batch['labels'].to(device)\n",
    "            labels = batch['labels']\n",
    "            # process\n",
    "            outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "            # extract loss\n",
    "            loss = outputs.loss\n",
    "            # calculate loss for every parameter that needs grad update\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optim.step()\n",
    "            # print relevant info to progress bar\n",
    "            loop.set_description(f'Epoch {epoch}')\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19d28292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d28e031fb283>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gt for training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mword2vec_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinetune_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mBERT_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinetune_BERT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mword2vec_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c7207f0746fc>\u001b[0m in \u001b[0;36mfinetune_word2vec\u001b[0;34m(train, window)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel_w2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtotal_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_w2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"combined-160.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmodel_w2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmodel_w2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersect_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"combined-160.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlockf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train models used in cross validations\n",
    "word2vec_models = []\n",
    "BERT_models = []\n",
    "\n",
    "train = '.'.join(list(train['gt for training']))\n",
    "word2vec_model = finetune_word2vec(train)\n",
    "BERT_model = finetune_BERT(train)\n",
    "word2vec_models.append(word2vec_model)\n",
    "BERT_models.append(BERT_model)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a738a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b8e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_test = []\n",
    "\n",
    "print('start')\n",
    "train = \"P L B. p l b.\" * 1000\n",
    "model = finetune_word2vec(train)\n",
    "models_test.append(model)\n",
    "print('halverwege')\n",
    "train = \"P F B. p f b.\" * 1000\n",
    "model = finetune_word2vec(train)\n",
    "models_test.append(model)\n",
    "print('klaar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d4fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_test[0].most_similar('P'))\n",
    "print(models_test[1].most_similar('P'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12144d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_test[0].save(\"word2vec_test.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a8ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59638e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test_2 = Word2Vec.load(\"word2vec_test.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6267245",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test_2.most_similar('P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c33539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/251 [00:00<?, ?it/s]/home/nvanthof/.local/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Epoch 0:  33%|███▎      | 83/251 [50:01<1:42:47, 36.71s/it, loss=0.0515]"
     ]
    }
   ],
   "source": [
    "train = \"P F B. p f b.\" * 1000\n",
    "BERT_model = finetune_BERT(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c974e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
