{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import difflib\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "import subprocess\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.dammit import EncodingDetector\n",
    "import statistics as s\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_centuries.csv')\n",
    "df['old index'] = df.index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = shuffle(df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to try a smaller dataset\n",
    "df_1600s = df.loc[df['century'] == '1600s'].head(10)\n",
    "df_1700s = df.loc[df['century'] == '1700s'].head(10)\n",
    "df_1800s = df.loc[df['century'] == '1800s'].head(10)\n",
    "df_1900s = df.loc[df['century'] == '1900s'].head(10)\n",
    "df = pd.concat([df_1600s, df_1700s, df_1800s, df_1900s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['reduced'] = [0]*len(df)\n",
    "#df['ocr text short'] = ['-']*len(df)\n",
    "\n",
    "reduced = []\n",
    "reduced_pages = []\n",
    "total_pages = []\n",
    "\n",
    "# reduce size of books\n",
    "def reduce_size(text, ocr_or_gt):\n",
    "    pages = 15 # pick amount of pages\n",
    "    one_page = 1361 # amount of characters on page\n",
    "    characters =  one_page * pages\n",
    "    #print('test:', text[0:characters])\n",
    "    if len(text) > characters:\n",
    "        first_pages = text[0:characters].split(\".\")\n",
    "        #print('startthing:', first_pages)\n",
    "        first_pages = '.'.join(first_pages[:-1])\n",
    "        #print('first pages reduced:', first_pages)\n",
    "        #reduced = 1\n",
    "        if ocr_or_gt == 'ocr':\n",
    "            total_pages_ocr = len(text) / one_page\n",
    "            total_pages.append(total_pages_ocr)\n",
    "            reduced_pages_ocr = total_pages_ocr - pages\n",
    "            reduced_pages.append(reduced_pages_ocr)\n",
    "            reduced.append(1)\n",
    "    else:\n",
    "        \n",
    "        first_pages = text\n",
    "        #reduced = 0\n",
    "        if ocr_or_gt == 'ocr':\n",
    "            total_pages_ocr = len(text) / one_page\n",
    "            total_pages.append(total_pages_ocr)\n",
    "            reduced_pages.append(0)\n",
    "            reduced.append(0)\n",
    "        #print('first pages not reduced:', first_pages)\n",
    "    #print(type(first_pages))\n",
    "    #print('end:', first_pages, reduced)\n",
    "    \n",
    "    return first_pages\n",
    "\n",
    "#df['gt text'], df['reduced'] = df['gt text'].apply(lambda x: reduce_size(x))\n",
    "#df['ocr text short'], df['reduced'] = df['ocr text'].apply(lambda x: reduce_size(x))\n",
    "df['gt text org'] = df['gt text']\n",
    "df['ocr text org'] = df['ocr text']\n",
    "df['gt text'] = df['gt text'].apply(lambda x: reduce_size(x, 'gt'))\n",
    "df['ocr text'] = df['ocr text'].apply(lambda x: reduce_size(x, 'ocr'))\n",
    "#df['gt text'], reduced = df['gt text'].apply(lambda x: reduce_size(x))\n",
    "#df['ocr text'], reduced = df['ocr text'].apply(lambda x: reduce_size(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of docs reduced: 0.25\n",
      "Percentage of dataset removed: 0.891722464866517\n",
      "sum:\n",
      "            reduced  reduced pages  docs  percentage reduced\n",
      "source                                                      \n",
      "Meertens          0       0.000000    10            0.000000\n",
      "anp               0       0.000000     3            0.000000\n",
      "books             0       0.000000     9            0.000000\n",
      "dbnl              4    2300.229978     4            1.000000\n",
      "newspapers        6      32.172667    14            0.428571\n",
      "mean:\n",
      "            total pages  reduced pages\n",
      "source                                \n",
      "Meertens       3.543057       0.000000\n",
      "anp            1.284350       0.000000\n",
      "books          1.085395       0.000000\n",
      "dbnl         590.057494     575.057494\n",
      "newspapers    14.738060       2.298048\n",
      "Percentage of dataset outside of books removed pages: 0.03987321630826994\n",
      "Percentage of dataset outside of books reduced documents: 0.16666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-45-ff0cce5656a7>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  without_books['percentage removed'] = without_books['reduced pages'] / without_books['total pages']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#print(reduced)\n",
    "#print(reduced_pages)\n",
    "#print(total_pages )\n",
    "#df.head()\n",
    "# percentage of documents reduced\n",
    "print('Percentage of docs reduced:', reduced.count(1)/len(reduced))\n",
    "# percentage of dataset removed\n",
    "print('Percentage of dataset removed:', sum(reduced_pages)/sum(total_pages))\n",
    "\n",
    "\n",
    "reduced_count_df = pd.DataFrame({'reduced': reduced, 'source': df['source'], 'reduced pages': reduced_pages, 'total pages': total_pages, 'docs': [1]*len(df)})\n",
    "sums = reduced_count_df[['reduced', 'source', 'reduced pages', 'docs']].groupby('source').sum()\n",
    "means = reduced_count_df[['total pages', 'source', 'reduced pages']].groupby('source').mean()\n",
    "sums['percentage reduced'] = sums['reduced'] / sums['docs']\n",
    "print('sum:')\n",
    "print(sums)\n",
    "print('mean:')\n",
    "print(means)\n",
    "without_books = reduced_count_df.loc[reduced_count_df['source'] != 'dbnl']\n",
    "without_books_removed = sum(list(without_books['reduced']))/ sum(list(without_books['docs']))\n",
    "without_books['percentage removed'] = without_books['reduced pages'] / without_books['total pages']\n",
    "print('Percentage of dataset outside of books removed pages:', s.mean(list(without_books['percentage removed'])))\n",
    "print('Percentage of dataset outside of books reduced documents:', without_books_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_17th = df[df['century'] == '1600s']\n",
    "df_18th = df[df['century'] == '1700s']\n",
    "df_19th = df[df['century'] == '1800s']\n",
    "df_20th = df[df['century'] == '1900s']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets = [df_17th, df_18th, df_19th, df_20th] #datasets[0] == df_17th etc.\n",
    "#datasets = [df_17th.head(), df_18th.head(), df_19th.head(), df_20th.head()]\n",
    "#print(len(df_17th), len(df_18th),len(df_19th),len(df_20th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'identifier', 'gt text', 'ocr text', 'CER', 'WER',\n",
      "       'WER (order independent)', 'dictionary lookup gt',\n",
      "       'dictionary lookup ocr', 'jaccard_coefficient',\n",
      "       'levenshtein_dist_normalized', 'source', 'word count gt',\n",
      "       'word count ocr', 'year', 'century', 'old index', 'gt text org',\n",
      "       'ocr text org'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# column selection\n",
    "print(df.columns)\n",
    "#df = df[['identifier', 'gt text', 'ocr text', 'CER', 'WER', 'source', 'word count gt',\n",
    "#       'word count ocr', 'century']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = re.sub(\"[^a-zA-Z0-9-. ]+\", '', text) # remove punctuation except for hyphens and dots\n",
    "    text = text.lower() # lowercase\n",
    "    return text\n",
    "\n",
    "# preprocess identifier\n",
    "df['identifier'] = df['identifier'].apply(lambda x: str(x))\n",
    "df['identifier'] = df['identifier'].apply(lambda x: x.replace('_', '.'))\n",
    "df['identifier'] = df['identifier'].apply(lambda x: x.replace(':', '.'))\n",
    "#df['identifier'] = df['identifier'].apply(lambda x: x + '_None')\n",
    "# preprocess OCR and GT\n",
    "df['gt text'] = df['gt text'].apply(lambda x: preprocessing(x))\n",
    "df['ocr text'] = df['gt text'].apply(lambda x: preprocessing(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parts for cross-validation, train, validation oand test set\n",
    "df_17th = df[df['century'] == '1600s']\n",
    "df_18th = df[df['century'] == '1700s']\n",
    "df_19th = df[df['century'] == '1800s']\n",
    "df_20th = df[df['century'] == '1900s']\n",
    "\n",
    "datasets = [df_17th, df_18th, df_19th, df_20th]\n",
    "# create a list of train, validation and test set\n",
    "\n",
    "train = pd.DataFrame()\n",
    "val = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "for dataset in datasets:\n",
    "    splits = np.array_split(dataset, 5)\n",
    "    train_sub = splits[:3]\n",
    "    train_sub = pd.concat(train_sub)\n",
    "    val_sub = splits[3]\n",
    "    test_sub = splits[4]\n",
    "    train = pd.concat([train, train_sub])\n",
    "    val = pd.concat([val, val_sub])\n",
    "    test = pd.concat([test, test_sub])\n",
    "\n",
    "train_indices = train.index.values.tolist() \n",
    "val_indices = val.index.values.tolist()  \n",
    "test_indices = test.index.values.tolist()  \n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index in train_indices:\n",
    "        df.at[index, 'set'] = 'train'\n",
    "    elif index in val_indices:\n",
    "        df.at[index, 'set'] = 'val'\n",
    "    elif index in test_indices:\n",
    "        df.at[index, 'set'] = 'test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_matching(row): # create files with matched sentences\n",
    "    #save_path = r\"C:\\Users\\Gebruiker\\Desktop\\Thesis\\Documenten\\Documenten\"\n",
    "    \n",
    "    ID = row['identifier']\n",
    "    page = 'None'\n",
    "    \n",
    "    # split GT and OCR-output of each row into sentences (based on full stops).\n",
    "    gt_sentences = row['gt text'].split(\".\")\n",
    "    ocr_sentences = row['ocr text'].split(\".\")\n",
    "\n",
    "    match_list = []\n",
    "\n",
    "    for sentence in gt_sentences:\n",
    "        for match in ocr_sentences:\n",
    "            if SequenceMatcher(None, sentence, match).ratio() > 0.75:\n",
    "                match_list.extend([[sentence, match]])\n",
    "                \n",
    "    matched_gt_sentences = []\n",
    "    matched_ocr_sentences = []\n",
    "\n",
    "    for sentence in match_list:\n",
    "        if len(sentence[0]) or len(sentence[1]) > 0:\n",
    "            matched_gt_sentences.append(sentence[0])\n",
    "            matched_ocr_sentences.append(sentence[1])\n",
    "    \n",
    "    # .join() with lists\n",
    "    separator = '.'\n",
    "    doc_GT = separator.join(matched_gt_sentences)\n",
    "    doc_OCR = separator.join(matched_ocr_sentences)\n",
    "    \n",
    "    #return matched_gt_sentences, matched_ocr_sentences\n",
    "    return doc_GT, doc_OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "jar_file = \"ocrevalUAtion-1.3.4-jar-with-dependencies.jar\"\n",
    "\n",
    "def evaluation(index, row, doc_GT, doc_OCR):\n",
    "    ID = row['identifier']\n",
    "    page = 'None'\n",
    "    \n",
    "    doc_OCR = doc_OCR.replace('.', '')\n",
    "    doc_OCR = re.sub(' +', ' ', doc_OCR)\n",
    "    filename_ocr = f\"{ID}_{page}_OCR.txt\"\n",
    "    #file_ocr = open(os.path.join(save_path, filename),\"w+\", encoding=\"utf-8\")\n",
    "    file_ocr = open(filename_ocr,\"w+\", encoding=\"utf-8\")\n",
    "    file_ocr.write(doc_OCR)\n",
    "    file_ocr.close()\n",
    "    \n",
    "    doc_GT = doc_GT.replace('.', '')\n",
    "    doc_GT = re.sub(' +', ' ', doc_GT)\n",
    "    filename_gt = f\"{ID}_{page}_GT.txt\"\n",
    "    #file_gt = open(os.path.join(save_path, filename),\"w+\", encoding=\"utf-8\")\n",
    "    file_gt = open(filename_gt,\"w+\", encoding=\"utf-8\")\n",
    "    file_gt.write(doc_GT)\n",
    "    file_gt.close()\n",
    "    \n",
    "    #output = ID + '_' + page + \".html\"\n",
    "    output = f\"{ID}_{page}.html\"\n",
    "    \n",
    "    #process = subprocess.call(\"/home/nvanthof/jdk-16.0.1/bin/java -cp \" + jar_file  + \" eu.digitisation.Main -gt \" + filename_gt + \" -ocr \"+ filename_ocr +\" -o \" + output + \"\")\n",
    "    #os.system(\"/home/nvanthof/jdk-16.0.1/bin/java -cp /home/nvanthof/ocrevalUAtion-1.3.4-jar-with-dependencies.jar eu.digitisation.Main -gt /home/nvanthof/ddd.010728187.mpeg21.a0005_None_GT.txt -ocr /home/nvanthof/ddd.010728187.mpeg21.a0005_None_OCR.txt  -o /home/nvanthof/OUTPUT2.html\")\n",
    "    #command = f\"/home/nvanthof/jdk-16.0.1/bin/java -cp /home/nvanthof/ocrevalUAtion-1.3.4-jar-with-dependencies.jar eu.digitisation.Main -gt /home/nvanthof/{filename_gt} -ocr /home/nvanthof/{filename_ocr}  -o /home/nvanthof/{output}\"\n",
    "    command = f\"/usr/bin/java -cp /home/nynkegpu/ocrevalUAtion-1.3.4-jar-with-dependencies.jar eu.digitisation.Main -gt /home/nynkegpu/{filename_gt} -ocr /home/nynkegpu/{filename_ocr}  -o /home/nynkegpu/{output}\"\n",
    "    os.system(command)\n",
    "    sleep(5)\n",
    "    \n",
    "    soup = BeautifulSoup(open(output, encoding='utf-8'))\n",
    "    table = soup.find(\"table\", attrs={'border': '1'})\n",
    "    # Split the filename, and extract the identifier and pagenr together as identifier \n",
    "    # Find the first table (this is the table in which the scores are stored)\n",
    "    # Find the tags in which 'CER', 'WER', and 'WER (order independent)' are stored and take the next tag to get the score \n",
    "    cer = table.find('td', text='CER')\n",
    "    cerScore = cer.findNext('td')\n",
    "    wer = table.find('td', text='WER')\n",
    "    werScore = wer.findNext('td')\n",
    "    werOI = table.find('td', text='WER (order independent)')\n",
    "    werOIScore = werOI.findNext('td')\n",
    "    \n",
    "    os.remove(filename_gt)\n",
    "    os.remove(filename_ocr)\n",
    "    os.remove(output)\n",
    "    return float(cerScore.text), float(werScore.text)   \n",
    "    \n",
    "    return cerScore.text, werScore.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#for index, row in df.loc[df['set'].isin(['train', 'val'])].iterrows():\n",
    "for index, row in df.iterrows():\n",
    "    #print(index)\n",
    "    if index%1000 == 0:\n",
    "        print(index)\n",
    "    doc_GT, doc_OCR = fuzzy_matching(row)\n",
    "    #GT_matched_docs_list.append(matched_gt_sentences)\n",
    "    #OCR_matched_docs_list.append(matched_ocr_sentences)\n",
    "    if (row['set'] == 'val') or (row['set'] == 'test'):\n",
    "        df.at[index, 'gt sentences matched'], df.at[index, 'ocr sentences matched'] = doc_GT, doc_OCR\n",
    "    if row['set'] == 'test':\n",
    "        df.at[index, 'CER matched sentences'], df.at[index, 'WER matched sentences'] = evaluation(index, row, doc_GT, doc_OCR)\n",
    "\n",
    "#for index, row in df.loc[df['set'].isin(['test'])].iterrows():\n",
    "    #print(index)\n",
    "#    if index%1000 == 0:\n",
    "#        print(index)\n",
    "#    doc_GT, doc_OCR = fuzzy_matching(row)\n",
    "    #GT_matched_docs_list.append(matched_gt_sentences)\n",
    "    #OCR_matched_docs_list.append(matched_ocr_sentences)\n",
    "#    df.at[index, 'gt sentences matched'], df.at[index, 'ocr sentences matched'] = doc_GT, doc_OCR\n",
    "    #df.at[index, 'CER matched sentences'], df.at[index, 'WER matched sentences'] = evaluation(index, row, doc_GT, doc_OCR)\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count sentences (longer than 4 words) in GT and OCR\n",
    "# count words in sentences longer than 4 words\n",
    "\n",
    "#meta_df = pd.DataFrame(columns=['identifier', 'sentences gt', 'sentences ocr', 'avg sentence length gt', ' avg sentence length ocr'])\n",
    "meta_df = pd.DataFrame()\n",
    "\n",
    "def sentence_count(row):\n",
    "# count of sentences in GT >= 4 words\n",
    "    sentences_gt = 0\n",
    "    words_GT_sentences = []\n",
    "    #characters_GT_sentences = []\n",
    "    for k in row['gt text'].split('.'):\n",
    "        k = [word for word in k.split(' ') if len(word) > 1]\n",
    "        if len(k) >= 4:\n",
    "            sentences_gt += 1\n",
    "            words_GT_sentences.append(len(k))\n",
    "            #characters_GT_sentences.append(len(' '.join(k)))\n",
    "    # count of sentences in OCR >= 4 words\n",
    "    sentences_ocr = 0\n",
    "    words_OCR_sentences = []\n",
    "    #characters_OCR_sentences = []\n",
    "    for k in row['ocr text'].split('.'):\n",
    "        k = [word for word in k.split(' ') if len(word) > 1]\n",
    "        if len(k) >= 4:\n",
    "            sentences_ocr += 1\n",
    "            words_OCR_sentences.append(len(k))\n",
    "            #characters_OCR_sentences.append(len(' '.join(k)))\n",
    "\n",
    "    return sentences_gt, sentences_ocr, words_GT_sentences, words_OCR_sentences\n",
    "\n",
    "\n",
    "#df['gt text'] = df['gt text'].apply(lambda x: preprocessing(x))\n",
    "\n",
    "for index, row in df.iterrows(): \n",
    "    sentences_gt, sentences_ocr, words_GT_sentences, words_OCR_sentences = sentence_count(row)\n",
    "    meta_df = meta_df.append({'identifier': row['identifier'], \\\n",
    "                              'sentences gt (fuzzy matched)': sentences_gt, 'sentences ocr (fuzzy matched)': sentences_ocr, \\\n",
    "                              'avg sentence length gt (fuzzy matched)': s.mean(words_GT_sentences), 'avg sentence length ocr (fuzzy matched)': s.mean(words_OCR_sentences), \\\n",
    "                              'max sentence length gt (fuzzy matched)': max(words_GT_sentences), 'max sentence length ocr (fuzzy matched)': max(words_OCR_sentences), \\\n",
    "                              'word count gt (fuzzy matched)': sum(words_GT_sentences), 'word count ocr (fuzzy matched)': sum(words_OCR_sentences)}, ignore_index = True)\n",
    "    \n",
    "df = pd.merge(left=df, right=meta_df, on='identifier')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'identifier', 'gt text', 'ocr text', 'CER', 'WER',\n",
       "       'WER (order independent)', 'dictionary lookup gt',\n",
       "       'dictionary lookup ocr', 'jaccard_coefficient',\n",
       "       'levenshtein_dist_normalized', 'source', 'word count gt',\n",
       "       'word count ocr', 'year', 'century', 'old index', 'gt text org',\n",
       "       'ocr text org', 'set', 'gt sentences matched', 'ocr sentences matched',\n",
       "       'CER matched sentences', 'WER matched sentences',\n",
       "       'avg sentence length gt (fuzzy matched)',\n",
       "       'avg sentence length ocr (fuzzy matched)',\n",
       "       'max sentence length gt (fuzzy matched)',\n",
       "       'max sentence length ocr (fuzzy matched)',\n",
       "       'sentences gt (fuzzy matched)', 'sentences ocr (fuzzy matched)',\n",
       "       'word count gt (fuzzy matched)', 'word count ocr (fuzzy matched)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CER</th>\n",
       "      <th>WER</th>\n",
       "      <th>WER (order independent)</th>\n",
       "      <th>dictionary lookup gt</th>\n",
       "      <th>dictionary lookup ocr</th>\n",
       "      <th>jaccard_coefficient</th>\n",
       "      <th>levenshtein_dist_normalized</th>\n",
       "      <th>word count gt</th>\n",
       "      <th>word count ocr</th>\n",
       "      <th>old index</th>\n",
       "      <th>CER matched sentences</th>\n",
       "      <th>WER matched sentences</th>\n",
       "      <th>avg sentence length gt (fuzzy matched)</th>\n",
       "      <th>avg sentence length ocr (fuzzy matched)</th>\n",
       "      <th>max sentence length gt (fuzzy matched)</th>\n",
       "      <th>max sentence length ocr (fuzzy matched)</th>\n",
       "      <th>sentences gt (fuzzy matched)</th>\n",
       "      <th>sentences ocr (fuzzy matched)</th>\n",
       "      <th>word count gt (fuzzy matched)</th>\n",
       "      <th>word count ocr (fuzzy matched)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>century</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1600s</th>\n",
       "      <td>185853</td>\n",
       "      <td>392.15</td>\n",
       "      <td>761.91</td>\n",
       "      <td>660.95</td>\n",
       "      <td>784.310000</td>\n",
       "      <td>470.15</td>\n",
       "      <td>1.757902</td>\n",
       "      <td>3.590000</td>\n",
       "      <td>7554</td>\n",
       "      <td>7265</td>\n",
       "      <td>185853</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>264.358836</td>\n",
       "      <td>264.358836</td>\n",
       "      <td>984.0</td>\n",
       "      <td>984.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>7270.0</td>\n",
       "      <td>7270.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700s</th>\n",
       "      <td>7206</td>\n",
       "      <td>131.91</td>\n",
       "      <td>310.46</td>\n",
       "      <td>249.64</td>\n",
       "      <td>863.600000</td>\n",
       "      <td>825.49</td>\n",
       "      <td>5.313096</td>\n",
       "      <td>1.442050</td>\n",
       "      <td>4210</td>\n",
       "      <td>4196</td>\n",
       "      <td>356853</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>322.172619</td>\n",
       "      <td>322.172619</td>\n",
       "      <td>799.0</td>\n",
       "      <td>799.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>3780.0</td>\n",
       "      <td>3780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800s</th>\n",
       "      <td>5876</td>\n",
       "      <td>389.97</td>\n",
       "      <td>340.76</td>\n",
       "      <td>219.60</td>\n",
       "      <td>835.180000</td>\n",
       "      <td>718.06</td>\n",
       "      <td>5.887226</td>\n",
       "      <td>0.729364</td>\n",
       "      <td>789626</td>\n",
       "      <td>565935</td>\n",
       "      <td>377626</td>\n",
       "      <td>0.84</td>\n",
       "      <td>2.65</td>\n",
       "      <td>206.343979</td>\n",
       "      <td>206.343979</td>\n",
       "      <td>1438.0</td>\n",
       "      <td>1438.0</td>\n",
       "      <td>1348.0</td>\n",
       "      <td>1348.0</td>\n",
       "      <td>25314.0</td>\n",
       "      <td>25314.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900s</th>\n",
       "      <td>127513</td>\n",
       "      <td>203.50</td>\n",
       "      <td>280.86</td>\n",
       "      <td>240.86</td>\n",
       "      <td>874.747159</td>\n",
       "      <td>806.79</td>\n",
       "      <td>6.018604</td>\n",
       "      <td>1.487644</td>\n",
       "      <td>29654</td>\n",
       "      <td>29824</td>\n",
       "      <td>376138</td>\n",
       "      <td>1.69</td>\n",
       "      <td>4.79</td>\n",
       "      <td>185.617386</td>\n",
       "      <td>185.617386</td>\n",
       "      <td>739.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>21059.0</td>\n",
       "      <td>21059.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0     CER     WER  WER (order independent)  \\\n",
       "century                                                        \n",
       "1600s        185853  392.15  761.91                   660.95   \n",
       "1700s          7206  131.91  310.46                   249.64   \n",
       "1800s          5876  389.97  340.76                   219.60   \n",
       "1900s        127513  203.50  280.86                   240.86   \n",
       "\n",
       "         dictionary lookup gt  dictionary lookup ocr  jaccard_coefficient  \\\n",
       "century                                                                     \n",
       "1600s              784.310000                 470.15             1.757902   \n",
       "1700s              863.600000                 825.49             5.313096   \n",
       "1800s              835.180000                 718.06             5.887226   \n",
       "1900s              874.747159                 806.79             6.018604   \n",
       "\n",
       "         levenshtein_dist_normalized  word count gt  word count ocr  \\\n",
       "century                                                               \n",
       "1600s                       3.590000           7554            7265   \n",
       "1700s                       1.442050           4210            4196   \n",
       "1800s                       0.729364         789626          565935   \n",
       "1900s                       1.487644          29654           29824   \n",
       "\n",
       "         old index  CER matched sentences  WER matched sentences  \\\n",
       "century                                                            \n",
       "1600s       185853                   0.00                   0.00   \n",
       "1700s       356853                   0.00                   0.00   \n",
       "1800s       377626                   0.84                   2.65   \n",
       "1900s       376138                   1.69                   4.79   \n",
       "\n",
       "         avg sentence length gt (fuzzy matched)  \\\n",
       "century                                           \n",
       "1600s                                264.358836   \n",
       "1700s                                322.172619   \n",
       "1800s                                206.343979   \n",
       "1900s                                185.617386   \n",
       "\n",
       "         avg sentence length ocr (fuzzy matched)  \\\n",
       "century                                            \n",
       "1600s                                 264.358836   \n",
       "1700s                                 322.172619   \n",
       "1800s                                 206.343979   \n",
       "1900s                                 185.617386   \n",
       "\n",
       "         max sentence length gt (fuzzy matched)  \\\n",
       "century                                           \n",
       "1600s                                     984.0   \n",
       "1700s                                     799.0   \n",
       "1800s                                    1438.0   \n",
       "1900s                                     739.0   \n",
       "\n",
       "         max sentence length ocr (fuzzy matched)  \\\n",
       "century                                            \n",
       "1600s                                      984.0   \n",
       "1700s                                      799.0   \n",
       "1800s                                     1438.0   \n",
       "1900s                                      739.0   \n",
       "\n",
       "         sentences gt (fuzzy matched)  sentences ocr (fuzzy matched)  \\\n",
       "century                                                                \n",
       "1600s                           274.0                          274.0   \n",
       "1700s                           170.0                          170.0   \n",
       "1800s                          1348.0                         1348.0   \n",
       "1900s                          1162.0                         1162.0   \n",
       "\n",
       "         word count gt (fuzzy matched)  word count ocr (fuzzy matched)  \n",
       "century                                                                 \n",
       "1600s                           7270.0                          7270.0  \n",
       "1700s                           3780.0                          3780.0  \n",
       "1800s                          25314.0                         25314.0  \n",
       "1900s                          21059.0                         21059.0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum of values\n",
    "df.groupby('century').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CER</th>\n",
       "      <th>WER</th>\n",
       "      <th>WER (order independent)</th>\n",
       "      <th>dictionary lookup gt</th>\n",
       "      <th>dictionary lookup ocr</th>\n",
       "      <th>jaccard_coefficient</th>\n",
       "      <th>levenshtein_dist_normalized</th>\n",
       "      <th>word count gt</th>\n",
       "      <th>word count ocr</th>\n",
       "      <th>old index</th>\n",
       "      <th>CER matched sentences</th>\n",
       "      <th>WER matched sentences</th>\n",
       "      <th>avg sentence length gt (fuzzy matched)</th>\n",
       "      <th>avg sentence length ocr (fuzzy matched)</th>\n",
       "      <th>max sentence length gt (fuzzy matched)</th>\n",
       "      <th>max sentence length ocr (fuzzy matched)</th>\n",
       "      <th>sentences gt (fuzzy matched)</th>\n",
       "      <th>sentences ocr (fuzzy matched)</th>\n",
       "      <th>word count gt (fuzzy matched)</th>\n",
       "      <th>word count ocr (fuzzy matched)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>century</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1600s</th>\n",
       "      <td>18585.3</td>\n",
       "      <td>39.215</td>\n",
       "      <td>76.191</td>\n",
       "      <td>66.095</td>\n",
       "      <td>78.431000</td>\n",
       "      <td>47.015</td>\n",
       "      <td>0.175790</td>\n",
       "      <td>0.359000</td>\n",
       "      <td>755.4</td>\n",
       "      <td>726.5</td>\n",
       "      <td>18585.3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>26.435884</td>\n",
       "      <td>26.435884</td>\n",
       "      <td>98.4</td>\n",
       "      <td>98.4</td>\n",
       "      <td>27.4</td>\n",
       "      <td>27.4</td>\n",
       "      <td>727.0</td>\n",
       "      <td>727.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700s</th>\n",
       "      <td>720.6</td>\n",
       "      <td>13.191</td>\n",
       "      <td>31.046</td>\n",
       "      <td>24.964</td>\n",
       "      <td>86.360000</td>\n",
       "      <td>82.549</td>\n",
       "      <td>0.531310</td>\n",
       "      <td>0.144205</td>\n",
       "      <td>421.0</td>\n",
       "      <td>419.6</td>\n",
       "      <td>35685.3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>32.217262</td>\n",
       "      <td>32.217262</td>\n",
       "      <td>79.9</td>\n",
       "      <td>79.9</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>378.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800s</th>\n",
       "      <td>587.6</td>\n",
       "      <td>38.997</td>\n",
       "      <td>34.076</td>\n",
       "      <td>21.960</td>\n",
       "      <td>83.518000</td>\n",
       "      <td>71.806</td>\n",
       "      <td>0.588723</td>\n",
       "      <td>0.121561</td>\n",
       "      <td>78962.6</td>\n",
       "      <td>56593.5</td>\n",
       "      <td>37762.6</td>\n",
       "      <td>0.420</td>\n",
       "      <td>1.325</td>\n",
       "      <td>20.634398</td>\n",
       "      <td>20.634398</td>\n",
       "      <td>143.8</td>\n",
       "      <td>143.8</td>\n",
       "      <td>134.8</td>\n",
       "      <td>134.8</td>\n",
       "      <td>2531.4</td>\n",
       "      <td>2531.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900s</th>\n",
       "      <td>12751.3</td>\n",
       "      <td>20.350</td>\n",
       "      <td>28.086</td>\n",
       "      <td>24.086</td>\n",
       "      <td>87.474716</td>\n",
       "      <td>80.679</td>\n",
       "      <td>0.601860</td>\n",
       "      <td>0.148764</td>\n",
       "      <td>2965.4</td>\n",
       "      <td>2982.4</td>\n",
       "      <td>37613.8</td>\n",
       "      <td>0.845</td>\n",
       "      <td>2.395</td>\n",
       "      <td>18.561739</td>\n",
       "      <td>18.561739</td>\n",
       "      <td>73.9</td>\n",
       "      <td>73.9</td>\n",
       "      <td>116.2</td>\n",
       "      <td>116.2</td>\n",
       "      <td>2105.9</td>\n",
       "      <td>2105.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0     CER     WER  WER (order independent)  \\\n",
       "century                                                        \n",
       "1600s       18585.3  39.215  76.191                   66.095   \n",
       "1700s         720.6  13.191  31.046                   24.964   \n",
       "1800s         587.6  38.997  34.076                   21.960   \n",
       "1900s       12751.3  20.350  28.086                   24.086   \n",
       "\n",
       "         dictionary lookup gt  dictionary lookup ocr  jaccard_coefficient  \\\n",
       "century                                                                     \n",
       "1600s               78.431000                 47.015             0.175790   \n",
       "1700s               86.360000                 82.549             0.531310   \n",
       "1800s               83.518000                 71.806             0.588723   \n",
       "1900s               87.474716                 80.679             0.601860   \n",
       "\n",
       "         levenshtein_dist_normalized  word count gt  word count ocr  \\\n",
       "century                                                               \n",
       "1600s                       0.359000          755.4           726.5   \n",
       "1700s                       0.144205          421.0           419.6   \n",
       "1800s                       0.121561        78962.6         56593.5   \n",
       "1900s                       0.148764         2965.4          2982.4   \n",
       "\n",
       "         old index  CER matched sentences  WER matched sentences  \\\n",
       "century                                                            \n",
       "1600s      18585.3                  0.000                  0.000   \n",
       "1700s      35685.3                  0.000                  0.000   \n",
       "1800s      37762.6                  0.420                  1.325   \n",
       "1900s      37613.8                  0.845                  2.395   \n",
       "\n",
       "         avg sentence length gt (fuzzy matched)  \\\n",
       "century                                           \n",
       "1600s                                 26.435884   \n",
       "1700s                                 32.217262   \n",
       "1800s                                 20.634398   \n",
       "1900s                                 18.561739   \n",
       "\n",
       "         avg sentence length ocr (fuzzy matched)  \\\n",
       "century                                            \n",
       "1600s                                  26.435884   \n",
       "1700s                                  32.217262   \n",
       "1800s                                  20.634398   \n",
       "1900s                                  18.561739   \n",
       "\n",
       "         max sentence length gt (fuzzy matched)  \\\n",
       "century                                           \n",
       "1600s                                      98.4   \n",
       "1700s                                      79.9   \n",
       "1800s                                     143.8   \n",
       "1900s                                      73.9   \n",
       "\n",
       "         max sentence length ocr (fuzzy matched)  \\\n",
       "century                                            \n",
       "1600s                                       98.4   \n",
       "1700s                                       79.9   \n",
       "1800s                                      143.8   \n",
       "1900s                                       73.9   \n",
       "\n",
       "         sentences gt (fuzzy matched)  sentences ocr (fuzzy matched)  \\\n",
       "century                                                                \n",
       "1600s                            27.4                           27.4   \n",
       "1700s                            17.0                           17.0   \n",
       "1800s                           134.8                          134.8   \n",
       "1900s                           116.2                          116.2   \n",
       "\n",
       "         word count gt (fuzzy matched)  word count ocr (fuzzy matched)  \n",
       "century                                                                 \n",
       "1600s                            727.0                           727.0  \n",
       "1700s                            378.0                           378.0  \n",
       "1800s                           2531.4                          2531.4  \n",
       "1900s                           2105.9                          2105.9  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean of values\n",
    "df.groupby('century').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281.0\n"
     ]
    }
   ],
   "source": [
    "# to find out longest sentence\n",
    "print(max(list(df['max sentence length gt (fuzzy matched)'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word alignment\n",
    "def word_alignment(GT, OCR):\n",
    "    shortest_list = min([GT, OCR], key=len)\n",
    "    longest_list = max([GT, OCR], key=len)\n",
    "    min_ratio = 0.65\n",
    "\n",
    "    for i in range(len(longest_list)):\n",
    "        match = False\n",
    "        #i = -i-1\n",
    "        # check if there is a direct match:\n",
    "        try:\n",
    "            if OCR[i] == '' and GT[i] == '':\n",
    "                del OCR[i]\n",
    "                del GT[i]\n",
    "            if SequenceMatcher(None, GT[i], OCR[i]).ratio() >= min_ratio:\n",
    "                match = True\n",
    "                continue\n",
    "            elif match == False:\n",
    "                # check if there is a match with a word before in OCR:\n",
    "                try:\n",
    "                    for j in range(1,10):\n",
    "                        if i-j >= 0:\n",
    "                            if j >= 5:\n",
    "                                if (GT[i] >= 5) or (OCR[i-j] >= 5):\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    continue\n",
    "                            if SequenceMatcher(None, GT[i], OCR[i-j]).ratio() >= min_ratio:\n",
    "                                if not (GT[i] == '' and OCR[i-j] == ''):\n",
    "                                    for k in range(0,j):\n",
    "                                        OCR.insert(i-j, '')\n",
    "                                    match = True\n",
    "                                    continue\n",
    "                        if i+j <= len(OCR):\n",
    "                            if j >= 5:\n",
    "                                if (GT[i] >= 5) or (OCR[i+j] >= 5):\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    continue\n",
    "                            if SequenceMatcher(None, GT[i], OCR[i+j]).ratio() >= min_ratio:\n",
    "                                if not (GT[i] == '' and OCR[i+j] == ''):\n",
    "                                    for k in range(0,j):\n",
    "                                        GT.insert(i, '')\n",
    "                                    match = True\n",
    "                                    continue\n",
    "                except:\n",
    "                    pass\n",
    "        except IndexError:\n",
    "            min([GT, OCR], key=len).append('')\n",
    "\n",
    "        if match == False:\n",
    "            try:\n",
    "                if SequenceMatcher(None, GT[i], OCR[i]).ratio() >= 0.30:\n",
    "                    pass\n",
    "            except:\n",
    "                pass\n",
    "            else:\n",
    "                #print('bad match', GT[i], OCR[i])\n",
    "                #print('before forwards match GT:', GT)\n",
    "                #print('before forwards bad match OCR:', OCR)\n",
    "                if i-1 >= 0:\n",
    "                    GT.insert(i, '')\n",
    "                else:\n",
    "                    GT.insert(0, '')\n",
    "                if i+1 <= len(GT):\n",
    "                    OCR.insert(i+1, '')\n",
    "                else:\n",
    "                    GT.append('')\n",
    "                #print('after forwards match GT:', GT)\n",
    "                #print('after forwards bad match OCR:', OCR)\n",
    "                \n",
    "                continue\n",
    "    \n",
    "   #print('after forwards GT:', GT)\n",
    "    #print('after forwards OCR:', OCR)\n",
    "    \n",
    "    while len(GT) != len(OCR):\n",
    "        min([GT, OCR], key=len).append('')\n",
    "    \n",
    "    if True:\n",
    "        \n",
    "        shortest_list = min([GT, OCR], key=len)\n",
    "        longest_list = max([GT, OCR], key=len)\n",
    "\n",
    "        for i in range(len(longest_list)):\n",
    "            i = -i-1\n",
    "            #print('i:', i)\n",
    "            try:\n",
    "\n",
    "                if (OCR[i] == '') ^ (GT[i] == ''):\n",
    "\n",
    "                    continue\n",
    "                if OCR[i] == ' ' and GT[i] == '':\n",
    "                    del OCR[i]\n",
    "                    del GT[i]\n",
    "                if SequenceMatcher(None, GT[i], OCR[i]).ratio() > 0.65:\n",
    "     \n",
    "                    continue\n",
    "                else:\n",
    "                    # check if there is a match with a word before in OCR:\n",
    "                    try:\n",
    "                        for j in range(1,len(GT)):\n",
    "                            if j >= 10:\n",
    "                                extra_ratio = 0.1\n",
    "                            else:\n",
    "                                extra_ratio = 0\n",
    "                            if i-j >= -len(OCR):\n",
    "                                if j >= 5:\n",
    "                                    if (len(GT[i]) >= 5) or (len(OCR[i-j]) >=5):\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        continue\n",
    "                            if GT[i] == '' or OCR[i-j] == '':\n",
    "                                continue\n",
    "                            if SequenceMatcher(None, GT[i], OCR[i-j]).ratio() > 0.65 + extra_ratio:\n",
    "                                for k in range(0,j):\n",
    "                                    if i+k < 0:\n",
    "                                        GT.insert(i+1, '')\n",
    "                                    else: \n",
    "                                        GT.append('')\n",
    "                                break\n",
    "                            if i+j <= -1:\n",
    "                                if j >= 5:\n",
    "                                    if (GT[i] >= 5) or (OCR[i+j] >=5):\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        continue\n",
    "                            if GT[i] == '' or OCR[i+j] == '':\n",
    "                                continue\n",
    "                            if SequenceMatcher(None, GT[i], OCR[i+j]).ratio() > 0.65 + extra_ratio:\n",
    "                                for k in range(0,j):\n",
    "                                    GT.insert(i, '')\n",
    "\n",
    "                                break\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            except IndexError:\n",
    "                min([GT, OCR], key=len).append('')\n",
    "\n",
    "            # if there is no match:\n",
    "            if SequenceMatcher(None, GT[i], OCR[i]).ratio() > 0.30:\n",
    "                pass\n",
    "            else:\n",
    "                #print('bad match', GT[i], OCR[i])\n",
    "                #print('before backwards bad match GT:',  GT)\n",
    "                #print('before backwards bad match OCR:',  OCR)\n",
    "                if (OCR[i] == '') or (GT[i] == ''):\n",
    "                    continue\n",
    "                else:\n",
    "                    GT.insert(i, '')\n",
    "                    OCR.insert(i+1, '')\n",
    "                #print('after backwards bad match GT:',  GT)\n",
    "                #print('after backwards bad match OCR:',  OCR)\n",
    "                \n",
    "                \n",
    "    #print('after backwards GT:',  GT)\n",
    "    #print('after backwards OCR:',  OCR)\n",
    "                    \n",
    "    while len(GT) != len(OCR):\n",
    "        min([GT, OCR], key=len).append('')\n",
    "\n",
    "\n",
    "    for i in range(len(GT)-1):\n",
    "        try:\n",
    "            #print(i)\n",
    "            #print(GT[i], OCR[i])\n",
    "            if (GT[i] == '') and (OCR[i] == ''):\n",
    "                del GT[i]\n",
    "                del OCR[i]\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "    try:\n",
    "        if SequenceMatcher(None, GT[-1], OCR[-2]).ratio() >= 0.65 and ((GT[-1] != '') or ((OCR[-2] != ''))):\n",
    "            #print('FOUND:', GT[-1], OCR[-2])\n",
    "            OCR.insert(-2, '')\n",
    "            GT.append('')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        if SequenceMatcher(None, GT[-2], OCR[-1]).ratio() >= 0.65 and ((GT[-2] != '') or ((OCR[-1] != ''))):\n",
    "            #print('FOUND:', OCR[-1], GT[-2])\n",
    "            GT.insert(-2, '')\n",
    "            OCR.append('')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for i in range(len(OCR)):\n",
    "        if GT[i] == '' and OCR[i] == '':\n",
    "            GT[i] = 'REMOVE'\n",
    "            OCR[i] = 'REMOVE'\n",
    "\n",
    "    GT = list(filter(lambda a: a != 'REMOVE', GT))\n",
    "    OCR = list(filter(lambda a: a != 'REMOVE', OCR))\n",
    "    \n",
    "    \n",
    "    #print('end GT:',  GT)\n",
    "    #print('end OCR:',  OCR)\n",
    "    return GT, OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'identifier', 'gt text', 'ocr text', 'CER', 'WER',\n",
       "       'WER (order independent)', 'dictionary lookup gt',\n",
       "       'dictionary lookup ocr', 'jaccard_coefficient',\n",
       "       'levenshtein_dist_normalized', 'source', 'word count gt',\n",
       "       'word count ocr', 'year', 'century', 'old index', 'gt text org',\n",
       "       'ocr text org', 'set', 'gt sentences matched', 'ocr sentences matched',\n",
       "       'CER matched sentences', 'WER matched sentences',\n",
       "       'avg sentence length gt (fuzzy matched)',\n",
       "       'avg sentence length ocr (fuzzy matched)',\n",
       "       'max sentence length gt (fuzzy matched)',\n",
       "       'max sentence length ocr (fuzzy matched)',\n",
       "       'sentences gt (fuzzy matched)', 'sentences ocr (fuzzy matched)',\n",
       "       'word count gt (fuzzy matched)', 'word count ocr (fuzzy matched)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify word alignment quality\n",
    "def verify_alignment(GT, OCR):\n",
    "    \n",
    "    if len(GT) != len(OCR):\n",
    "        alignment_validation = False\n",
    "        return alignment_validation\n",
    "    \n",
    "    min_word_length = 8 # the higher this number, the more likely a alignment will be seen as valid, thus higher chance at false positive, lower chance at false negative. \n",
    "    match_ratio = 0.70\n",
    "    anchors_list = []\n",
    "    for i in range(len(GT)):\n",
    "        if len(GT[i]) >= min_word_length:  \n",
    "            anchors_list.append(GT[i])\n",
    "        if len(OCR[i]) >= min_word_length:\n",
    "            anchors_list.append(OCR[i])\n",
    "    #print(anchors_list)\n",
    "\n",
    "\n",
    "    alignment_validation = True\n",
    "\n",
    "    for anchor in anchors_list:\n",
    "        #print('ANCHOR:', anchor)\n",
    "        matches_GT = set()\n",
    "        matches_OCR = set()\n",
    "        match = False\n",
    "        for i in range(len(GT)):\n",
    "            if (SequenceMatcher(None, GT[i], anchor).ratio() >= 0.65) and (len(GT[i]) >= min_word_length or len(GT[i]) == 0):\n",
    "                matches_GT.add(i)\n",
    "                #print('added as GT match:', GT[i])\n",
    "            if SequenceMatcher(None, OCR[i], anchor).ratio() >= 0.65 and (len(OCR[i]) >= min_word_length or len(OCR[i]) == 0):\n",
    "                matches_OCR.add(i)\n",
    "                #print('added as OCR match:', OCR[i])\n",
    "        #print('anchor:', anchor)\n",
    "        #print(matches_GT)\n",
    "        #print(matches_OCR)\n",
    "        if matches_GT == matches_OCR:\n",
    "            match = True\n",
    "        if match == False:\n",
    "            match = True\n",
    "            points_set1 = set() # counts when the matched opposite word is '' for OCR\n",
    "            points_set2 = set() # counts when the matched opposite word is '' for GT\n",
    "            for index in matches_GT.difference(matches_OCR): # elements in matches_GT that are not in matches_OCR\n",
    "                if OCR[index] == '':\n",
    "                    points_set1.add(index)\n",
    "                if OCR[index] != '' and len(matches_OCR.difference(matches_GT)) != 0:\n",
    "                    match = False\n",
    "            for index in matches_OCR.difference(matches_GT): # elements in matches_OCR that are not in matches_GT\n",
    "                if GT[index] == '':\n",
    "                    points_set2.add(index)\n",
    "                if GT[index] != '' and len(matches_GT.difference(matches_OCR)) != 0:\n",
    "                    match = False\n",
    "            if len(points_set1.intersection(points_set2)) != 0: # if an anchor words was match in both OCR and GT with an '', it's a bad match.\n",
    "                match = False\n",
    "        if match == False:\n",
    "            for index in (matches_GT.difference(matches_OCR)):\n",
    "                index += 1\n",
    "                if index in matches_OCR.difference(matches_GT):\n",
    "                    if (GT[index-1] == '' or OCR[index-1] == '') and (SequenceMatcher(None, GT[index], OCR[index]).ratio() >= match_ratio):\n",
    "                        match = True\n",
    "            for index in (matches_OCR.difference(matches_GT)):\n",
    "                index += 1\n",
    "                if index in matches_GT.difference(matches_OCR):\n",
    "                    if (GT[index] == '' or OCR[index] == '') and (SequenceMatcher(None, GT[index-1], OCR[index-1]).ratio() >= match_ratio):\n",
    "                        match = True\n",
    "\n",
    "        if match == False:\n",
    "            alignment_validation = False # bad alignment\n",
    "\n",
    "    return alignment_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         good alignments percentage\n",
      "century                            \n",
      "1600s                      1.000000\n",
      "1700s                      1.000000\n",
      "1800s                      1.000000\n",
      "1900s                      0.997024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "good_alignments_docs = []\n",
    "bad_alignments_docs = []\n",
    "aligned_OCR_sentences_docs = []\n",
    "aligned_GT_sentences_docs = []\n",
    "\n",
    "#df['aligned_GT_sentences'] = [np.nan] * len(df)\n",
    "#df['aligned_OCR_sentences'] = [np.nan] * len(df)\n",
    "#df['good_alignments'] = [np.nan] * len(df)\n",
    "#df['bad_alignments'] = [np.nan] * len(df)\n",
    "\n",
    "#for index, row in df.loc[df['set'].isin(['val','test'])].iterrows():\n",
    "for index, row in df.iterrows():\n",
    "    if (row['set'] == 'val') or (row['set'] == 'test'):\n",
    "        GT_sentences = row['gt sentences matched'].split('.')\n",
    "        OCR_sentences = row['ocr sentences matched'].split('.')\n",
    "        good_alignments = 0\n",
    "        bad_alignments = 0\n",
    "        aligned_GT = []\n",
    "        aligned_OCR = []\n",
    "        for i in range(len(GT_sentences)):\n",
    "            GT = GT_sentences[i].split(' ')\n",
    "            OCR = OCR_sentences[i].split(' ')\n",
    "            GT, OCR = word_alignment(GT, OCR)\n",
    "            verification = verify_alignment(GT, OCR)\n",
    "            if verification == False: # bad alignment\n",
    "                GT = ['REMOVED'] * len(GT)\n",
    "                bad_alignments += 1\n",
    "            else:\n",
    "                good_alignments += 1\n",
    "            aligned_GT.append(GT)\n",
    "            aligned_OCR.append(OCR)\n",
    "        # an alignment has been made for one document\n",
    "        #df.loc[index, 'aligned_GT_sentences'] = str(aligned_GT)\n",
    "        #df.loc[index, 'aligned_OCR_sentences'] = str(aligned_OCR)\n",
    "        #df.at[index, 'good alignments'] = good_alignments\n",
    "        #df.at[index, 'bad alignments'] = bad_alignments\n",
    "        good_alignments_docs.append(good_alignments)\n",
    "        bad_alignments_docs.append(bad_alignments)\n",
    "        aligned_OCR_sentences_docs.append(aligned_OCR)\n",
    "        aligned_GT_sentences_docs.append(aligned_GT)\n",
    "        \n",
    "    else:\n",
    "        #df.loc[index, 'aligned_GT_sentences'] = df.loc[index, 'aligned_OCR_sentences'] = df.at[index, 'good alignments'] = df.at[index, 'bad alignments'] = np.nan\n",
    "        good_alignments_docs.append(np.nan)\n",
    "        bad_alignments_docs.append(np.nan)\n",
    "        aligned_OCR_sentences_docs.append(np.nan)\n",
    "        aligned_GT_sentences_docs.append(np.nan)\n",
    "\n",
    "    # this list should only apply to rows in the val and test set\n",
    "df['aligned_GT_sentences'] = aligned_GT_sentences_docs\n",
    "df['aligned_OCR_sentences'] = aligned_OCR_sentences_docs\n",
    "df['good_alignments'] = good_alignments_docs\n",
    "df['bad_alignments'] = bad_alignments_docs\n",
    "df['good alignments percentage'] = df['good_alignments'] / (df['good_alignments'] + df['bad_alignments'])\n",
    "#print('percentage of sentences in dataset that is well aligned:', good_alignments_percentage)\n",
    "print(df[['good alignments percentage', 'century']].groupby('century').mean())\n",
    "\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#df['longest_streak'], df['avg_longest_streaks'] = df['avg_total_missing_words'] = df['avg_perc_missing_words '] = [np.nan] * len(df)\n",
    "longest_streak_docs = []\n",
    "avg_longest_streaks_docs = []\n",
    "avg_total_missing_words_docs = []\n",
    "avg_perc_missing_words_docs = [] \n",
    "print(len(longest_streak_docs))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if (row['set'] == 'val') or (row['set'] == 'test'):\n",
    "        ocr_aligned_sentences = row['aligned_OCR_sentences']\n",
    "        # analysing the word alignment\n",
    "        # word alignment for each sentence\n",
    "        longest_streaks = []\n",
    "        total_missing_words = []\n",
    "        perc_missing_words = []\n",
    "        for sentence in ocr_aligned_sentences: # for each sentence\n",
    "            streaks = []\n",
    "            streak = 0\n",
    "            for word in sentence: # for each word\n",
    "                if word == '':\n",
    "                    streak = streak + 1\n",
    "                else:\n",
    "                    streaks.append(streak)\n",
    "                    streak = 0\n",
    "            try:\n",
    "                longest_streaks.append(max(streaks))# longest streak of missing word in sentence\n",
    "                total_missing_words.append(sum(streaks)) # total of missing words in a sentence\n",
    "                perc_missing_words.append(sum(streaks) / len(ocr_aligned_sentences))\n",
    "            except ValueError:\n",
    "                longest_streaks.append(0)# longest streak of missing word in sentence\n",
    "                total_missing_words.append(0) # total of missing words in a sentence\n",
    "                perc_missing_words.append(0)\n",
    "        # statistics word alignment doc level:\n",
    "        # find longest streak of all longest streaks:\n",
    "        import statistics as s\n",
    "        longest_streak = max(longest_streaks)\n",
    "        avg_longest_streaks = s.mean(longest_streaks)\n",
    "        avg_total_missing_words = s.mean(total_missing_words)\n",
    "        avg_perc_missing_words = s.mean(perc_missing_words)\n",
    "        longest_streak_docs.append(longest_streak)\n",
    "        avg_longest_streaks_docs.append(avg_longest_streaks)\n",
    "        avg_total_missing_words_docs.append(avg_total_missing_words)\n",
    "        avg_perc_missing_words_docs.append(avg_perc_missing_words)\n",
    "    elif (row['set'] == 'train'):\n",
    "        longest_streak_docs.append(np.nan)\n",
    "        avg_longest_streaks_docs.append(np.nan)\n",
    "        avg_total_missing_words_docs.append(np.nan)\n",
    "        avg_perc_missing_words_docs.append(np.nan)\n",
    "   \n",
    "df['longest_streak'] = longest_streak_docs\n",
    "df['avg_longest_streaks'] = avg_longest_streaks_docs\n",
    "df['avg_total_missing_words'] = avg_total_missing_words_docs\n",
    "df['avg_perc_missing_words '] = avg_perc_missing_words_docs\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "#df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nynkegpu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nynkegpu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def find_names(text, names):\n",
    "    NNP_tags = \"ik je jij jou u hij zij ze het wij we jullie mij me hem haar ons hen hun\".split(' ')\n",
    "    text = word_tokenize(text)\n",
    "    tags = nltk.pos_tag(text)\n",
    "    \n",
    "    for tag in tags:\n",
    "        if (tag[1] == 'NNP'):\n",
    "            # we want words that are NN(P)s, but are not in NNP_tags\n",
    "            NN = tag[0]\n",
    "            NN = NN.lower()\n",
    "            if NN not in NNP_tags:\n",
    "                names.append(NN)\n",
    "    return names\n",
    "\n",
    "# find proper pronouns\n",
    "gt_names = []\n",
    "ocr_names = []\n",
    "# these both can look into train, val, and test set\n",
    "for text in list(df['gt text']):\n",
    "    gt_names_org = find_names(text, gt_names)\n",
    "for text in list(df['ocr text']):\n",
    "    ocr_names_org = find_names(text, ocr_names)\n",
    "\n",
    "gt_names = []\n",
    "for name in gt_names_org:\n",
    "    if len(name) >= 5:\n",
    "        gt_names.append(name)\n",
    "ocr_names = []\n",
    "for name in ocr_names_org:\n",
    "    if len(name) >= 5:\n",
    "        ocr_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "df.to_csv('preprocessed_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lists:\n",
    "with open(\"gt_names.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(gt_names, fp)\n",
    "with open(\"ocr_names.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(ocr_names, fp)\n",
    "#with open(\"test.txt\", \"rb\") as fp:   # Unpickling\n",
    "#    b = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
