{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3140d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import difflib\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "import subprocess\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.dammit import EncodingDetector\n",
    "import statistics as s\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cc046999",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_centuries.csv')\n",
    "df['old index'] = df.index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6fed9ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     -\n",
      "1     -\n",
      "2     -\n",
      "3     -\n",
      "4     -\n",
      "5     -\n",
      "6     -\n",
      "7     -\n",
      "8     -\n",
      "9     -\n",
      "10    -\n",
      "11    -\n",
      "12    -\n",
      "13    -\n",
      "14    -\n",
      "15    -\n",
      "16    -\n",
      "17    -\n",
      "18    -\n",
      "19    -\n",
      "20    -\n",
      "21    -\n",
      "22    -\n",
      "23    -\n",
      "24    -\n",
      "25    -\n",
      "26    -\n",
      "27    -\n",
      "28    -\n",
      "29    -\n",
      "30    -\n",
      "31    -\n",
      "32    -\n",
      "33    -\n",
      "34    -\n",
      "35    -\n",
      "36    -\n",
      "37    -\n",
      "38    -\n",
      "39    -\n",
      "40    -\n",
      "41    -\n",
      "42    -\n",
      "43    -\n",
      "44    -\n",
      "45    -\n",
      "46    -\n",
      "47    -\n",
      "48    -\n",
      "49    -\n",
      "50    -\n",
      "51    -\n",
      "52    -\n",
      "53    -\n",
      "54    -\n",
      "55    -\n",
      "56    -\n",
      "57    -\n",
      "58    -\n",
      "59    -\n",
      "60    -\n",
      "61    -\n",
      "62    -\n",
      "63    -\n",
      "64    -\n",
      "65    -\n",
      "66    -\n",
      "67    -\n",
      "68    -\n",
      "69    -\n",
      "70    -\n",
      "71    -\n",
      "72    -\n",
      "73    -\n",
      "74    -\n",
      "75    -\n",
      "76    -\n",
      "77    -\n",
      "78    -\n",
      "79    -\n",
      "80    -\n",
      "81    -\n",
      "82    -\n",
      "83    -\n",
      "84    -\n",
      "85    -\n",
      "86    -\n",
      "87    -\n",
      "88    -\n",
      "89    -\n",
      "90    -\n",
      "91    -\n",
      "92    -\n",
      "93    -\n",
      "94    -\n",
      "95    -\n",
      "96    -\n",
      "97    -\n",
      "98    -\n",
      "Name: ocr text short, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['ocr text short'] = ['-']*len(df)\n",
    "print(df['ocr text short'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a4b325d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Meertens' 'newspapers' 'books' 'dbnl']\n",
      "['1600s' '1800s' '1700s' '1900s']\n"
     ]
    }
   ],
   "source": [
    "#df = shuffle(df).reset_index(drop=True)\n",
    "df = df.head(99)\n",
    "#df = df.sample(n = 100)\n",
    "#df = df.reset_index(drop=True)\n",
    "print(df['source'].unique())\n",
    "print(df['century'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "56e20a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     POOLEN, PRUYSSEN &c. ..<:-..:- 't. V*.vVr.. JÂ...\n",
       "1     De ingenieur Jannen, die eenige maanden op den...\n",
       "2     SWITSERLANT, Bafel den 18 December. De 4 Gevan...\n",
       "3     POOLEN, PRUYSSEN, &c. Danzighdenz3july. ©en tr...\n",
       "4     SPANGIEN. Madrid den 3 Juoy. Volgens de Brieve...\n",
       "5     ITALIEN. NA pels den n September. Benige! Sche...\n",
       "6     VRANCKRYCK. Paiiis den li Tuay. Ãœyt 't Land v...\n",
       "7     ENGELANDT,&c. lu''^i^^'^']^^^''^^\"^kJ^4^'/ twÃ...\n",
       "8     Duytslant en d'aengransende Rijcken. Weenen de...\n",
       "9     ENGELANT, &C. EJenburgden t r Augufti, Wy hebb...\n",
       "10    TUR.CKYEN. SMkna den 3 J uny. Den oerden defes...\n",
       "11    NEDERLANDEN. BruiTelden 30 iuly. ©e peeren ÏCm...\n",
       "12    VRANCKRYCK. Parijs den 6 July.Monfiear den Her...\n",
       "13    Men laet een ie*dijck wctenTdÂ«t tÃ¶t EnkhÃ¼yf...\n",
       "14    NEDERLANDEN. 1 :>iiiÃŸrupideVi 'ÃSÃpttmberi ...\n",
       "15    SLAVONIEN. \\ EflÃ¨ck den ii-Jariunryi' Met Reg...\n",
       "16    VRANCKRYCK. Parijs demi September. Den Grave v...\n",
       "17    Op Vrydag, den 21 November,fal men in 's Grave...\n",
       "18    NEDERLANDEN. . 1 Gent den 17 February. f Gifte...\n",
       "19    VRANCKRYCK. Parijs den S Juny. D' Advyfcn uyt ...\n",
       "20    VOORREDE. tot Buitenleden des Genootfehaps, de...\n",
       "21    NEDERLANDEN â€¢v - * -de. De Vergaderingen ten...\n",
       "22    VRANCKRYCK. . livt 't Franfle Leger onder Salu...\n",
       "23    H. TOLLENS, Cz. GEZAMENLIJKE DICHTWERKEN. \"ii!...\n",
       "24    ENGELANDT,&c. ,|f alliiet Scht- â– : .jen kron...\n",
       "25    Wt Milanen den 1 April. ©encgftencompagntcn / ...\n",
       "26    Duytflant en d'sengrenfende Rijcken. Luyck den...\n",
       "27    NEDERLANDEN. 'a Gravenhage den J Mey. Den Baro...\n",
       "28    ENGELANT, &c. Pleymuyen dea 8 September. -Vier...\n",
       "29    NEDERLANDEN. , Cortrijck den 17 Juny.l4 Mannen...\n",
       "30    Parys den 14 May. -. i'rj%*ÃªÃ©l*Ã¶iurp ASÃJa...\n",
       "31    IPO PHILADELPHUS wat zijn nu de Christenen in ...\n",
       "32    ENGELANT, &c. Dublin den 2 5 Juny. OnlÃ¨ Armee...\n",
       "33    Duytslandt en d'aengresende Rijcken. iVtStraet...\n",
       "34    NEDERLANDEN. BrÃ¼flcl den 27 Maert. Den Grave ...\n",
       "35    POOLEN, PRUYSSEN, &c. ,Â».',Warfchouwdenij.lan...\n",
       "36    Duytslant en d'aengrensende Rijcken. ■• \\\\'een...\n",
       "37    Amsterdam den 2 Mey. Met de Brieven van den 26...\n",
       "38    Wt Ceulen den 15. dito. , Het ©icclomhufctk bo...\n",
       "39    POOLEN,PRUYSSEN,&c. â– 1* Warftbouwcferi 7 bai...\n",
       "40    Daer zijn geftolen 2 Stucken Farendijn,'t eene...\n",
       "41    VRANCKRYCK. â– - h. k.v, ~..v- â–  â– â– -'â– ...\n",
       "42    Wt Gendt den 10 dito. P Â©p 't l. erken bp Â£>...\n",
       "43    Wtte Over-Palts den 20 dito. ©e Rebellen ban h...\n",
       "44    POOLEN, PRUYSSEN, &c. SrinJrÃ¯Rurr.rr drnff, f...\n",
       "45    ENGELANT, & c. Londen den ii tfoveinbet. *ijn ...\n",
       "46    SWITSERLANT. v Bafel den 17 Dccembcr. De Verga...\n",
       "47    Uyt Thoorn, den 8 dito. men/ bat fp boo? gifte...\n",
       "48    Duytslandt en d'aengrensende Rijcken. ' ':' We...\n",
       "49    POOLEN, PRUYSSEN, &c. â€” __â–  *t. j . m-m. v...\n",
       "50    NEDERLANDEN. j Uyt het Hooftquaitier van 'tLcg...\n",
       "51    NEDERLANDEN. Bruflfcl den 24 Fcbruary. 't Is n...\n",
       "52    Wt Munster den 10 dito. ztepferlijche Maejeste...\n",
       "53    ZEVENBERGEN. ! Uyt het Leger onder den Generae...\n",
       "54    DUYTSLANDT, en d'aenpalende Rijcken. B'rhjn de...\n",
       "55    D 't nummer bestaat uit 12 bladzijden Pondei d...\n",
       "56    ITALIEN. Napelt den 14. lanturyJ. Het is nouwe...\n",
       "57    POLEN, PRUYSSEN, &c. * den 11 April. OnfiÂ» Kr...\n",
       "58    Wt Dresden den 19 dito. Q;£crgvffcrm cm.,,., h...\n",
       "59    Wt Parijs den 15. Iannuario. <9efe 3©eecße i& ...\n",
       "60    Wt Milanen den 24 Octobris 1646. D© jpranfcöe ...\n",
       "61    Wt het Legher van sijn Doorl. Hoogheyt den Pri...\n",
       "62    POLEN, PRUYSSEN, &c. -j War ft hou den nOdobci...\n",
       "63    NEDERLANDEN. RvÃŸeldtn 12 September. DeTroupen...\n",
       "64    Een ander van den selven dito. JBonfieur \\ Â©e...\n",
       "65    VVt Venetien, den 7. September. ©anonfe ©loott...\n",
       "66    Wttet Leger van Sijn Hoogheyt by Hulft in Vlae...\n",
       "67    Wt Buxtehoe den 6. Iuny. j&abat £>tabcnaenbe t...\n",
       "68    STADSNIEUWS Alg. rijstdistributie Van 2 kg voo...\n",
       "69    Duytfland en d'aengrenfende Rijcken. I Weeoen ...\n",
       "70    INGEZONDEN MEDEDEELINGEN. HEUP-RHEUMATIEK. Nu ...\n",
       "71    42 Verzameling van Stukken betrekkelyk tot del...\n",
       "72    DE DICHTWERKEN VAN BILDERDIJK. XIII. DE DICHTW...\n",
       "73    VRANCKRYCK. â–  'H titwinbtrl-Ã¯, Dier sr/nant...\n",
       "74    Van den Rhijnstroom den 3 dito. ©en ftfnr-©o?f...\n",
       "75    Wt Spier, den 24 dito. ©eSte.recfcfietM^mUpfbu...\n",
       "76    NEDERLANDEN. Gent de* 18 November. De Franflch...\n",
       "77    Duytfland en dâ€™aengrenfende Rijcken. 'Munche...\n",
       "78    Wt Swaben, den 26 dito. ©e Cfteur-ïBepcrfdjc B...\n",
       "79    Wt Traerbach den 4 dito. ©en Hcrtogß ban Hottr...\n",
       "80    LETT EROEFENINGEN. AL GEM E E NE VAD E RLANDSC...\n",
       "81    Duytflant en d'aengrenfende Rijcken. â€¢ Auguf...\n",
       "82    ENGELANT, &c. , Londen da 24 Aug.Niet al! cc n...\n",
       "83    Uyt Romen, den 27 October 1567. M€n bertfatt b...\n",
       "84    SWITSERLANT. ,Ã¯eio den Â« Mey. Ons Camon heef...\n",
       "85    Wt Lisbon, den 8 Aprilis, 1642. ©néDefe* acrlb...\n",
       "86    POLEN, PRUYSSEN, &c. 'Sambortten *.} September...\n",
       "87    Uyt Texel, den 14 dito. ©en 11 beier / sfln al...\n",
       "88    NEDERLANDEN. , ,\\ Bruag* den p lanuary. -. JRm...\n",
       "89    DUYTSLANT en d'aengrenzende RYKEN. , j 7-1 'ty...\n",
       "90    Wt Ceulen en der Spreu den 3 Meert. l^eft toef...\n",
       "91    Wt Turino den 20 dito. Certoölc be boenbe ?tjn...\n",
       "92    Wt Weenen den 14 April. ©en ©enerael ©tccoïomn...\n",
       "93    Duytslandt en d'aengresende Rijcken. '~;' Cebl...\n",
       "94    Vanden Rhynstroom den 14 dito. _ ©f toel Den ï...\n",
       "95    EERSTE BLAD 7WENTSCH DAGBLAD TUBANTIA . MAANDA...\n",
       "96    Wt Basel den 6 dito. ,-J?J* rtÂ°oÃ¶ bfln Ã¯Ã¯o...\n",
       "97    Uyt Turin, den 3 dito. ïKt Ueatt I bebint fief...\n",
       "98    We Schafhuysenden 15. Decemb. ©e^toabtfcßcCrcp...\n",
       "Name: ocr text, dtype: object"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "df['ocr text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5ebeaeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "[0]\n",
      "start\n",
      "[0, 0]\n",
      "start\n",
      "[0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#df['reduced'] = [0]*len(df)\n",
    "df['ocr text short'] = ['-']*len(df)\n",
    "\n",
    "reduced = []\n",
    "\n",
    "# reduce size of books\n",
    "def reduce_size(text):\n",
    "    print('start')\n",
    "    pages = 50 # pick amount of pages\n",
    "    characters =  3000 * pages\n",
    "    #print('test:', text[0:characters])\n",
    "    if len(text) > characters:\n",
    "        first_pages = text[0:characters].split(\".\")\n",
    "        #print('startthing:', first_pages)\n",
    "        first_pages = '.'.join(first_pages[:-1])\n",
    "        #print('first pages reduced:', first_pages)\n",
    "        #reduced = 1\n",
    "        reduced.append(1)\n",
    "    else:\n",
    "        first_pages = text\n",
    "        #reduced = 0\n",
    "        reduced.append(0)\n",
    "        #print('first pages not reduced:', first_pages)\n",
    "    print(reduced)\n",
    "    #print(type(first_pages))\n",
    "    #print('end:', first_pages, reduced)\n",
    "    return first_pages\n",
    "\n",
    "#df['gt text'], df['reduced'] = df['gt text'].apply(lambda x: reduce_size(x))\n",
    "#df['ocr text short'], df['reduced'] = df['ocr text'].apply(lambda x: reduce_size(x))\n",
    "df['ocr text short'] = df['ocr text'].apply(lambda x: reduce_size(x))\n",
    "#df['gt text'], reduced = df['gt text'].apply(lambda x: reduce_size(x))\n",
    "#df['ocr text'], reduced = df['ocr text'].apply(lambda x: reduce_size(x))\n",
    "print(list(df['reduced']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b0b57506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(reduced)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24b675a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_17th = df[df['century'] == '1600s']\n",
    "df_18th = df[df['century'] == '1700s']\n",
    "df_19th = df[df['century'] == '1800s']\n",
    "df_20th = df[df['century'] == '1900s']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58c0063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets = [df_17th, df_18th, df_19th, df_20th] #datasets[0] == df_17th etc.\n",
    "#datasets = [df_17th.head(), df_18th.head(), df_19th.head(), df_20th.head()]\n",
    "#print(len(df_17th), len(df_18th),len(df_19th),len(df_20th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7dcaf83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'identifier', 'gt text', 'ocr text', 'CER', 'WER',\n",
      "       'WER (order independent)', 'dictionary lookup gt',\n",
      "       'dictionary lookup ocr', 'jaccard_coefficient',\n",
      "       'levenshtein_dist_normalized', 'source', 'word count gt',\n",
      "       'word count ocr', 'year', 'century', 'old index', 'gt text org',\n",
      "       'ocr text org', 'gt sentences matched', 'ocr sentences matched'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# column selection\n",
    "print(df.columns)\n",
    "#df = df[['identifier', 'gt text', 'ocr text', 'CER', 'WER', 'source', 'word count gt',\n",
    "#       'word count ocr', 'century']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e84ac15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Meertens' 'books' 'newspapers' 'anp' 'dbnl']\n",
      "['1600s' '1700s' '1900s' '1800s']\n"
     ]
    }
   ],
   "source": [
    "print(df['source'].unique())\n",
    "print(df['century'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "356a2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_names(text, names):\n",
    "    NNP_tags = \"ik je jij jou u hij zij ze het wij we jullie mij me hem haar ons hen hun\".split(' ')\n",
    "    text = word_tokenize(text)\n",
    "    tags = nltk.pos_tag(text)\n",
    "    \n",
    "    for tag in tags:\n",
    "        if (tag[1] == 'NNP'):\n",
    "            # we want words that are NN(P)s, but are not in NNP_tags\n",
    "            NN = tag[0]\n",
    "            NN = NN.lower()\n",
    "            if NN not in NNP_tags:\n",
    "                names.append(NN)\n",
    "    return names\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = re.sub(\"[^a-zA-Z0-9-. ]+\", '', text) # remove punctuation except for hyphens and dots\n",
    "    text = text.lower() # lowercase\n",
    "    return text\n",
    "\n",
    "#for i in range(len(datasets)):\n",
    "#    # preprocess identifier\n",
    "#    datasets[i]['identifier'] = datasets[i]['identifier'].apply(lambda x: str(x))\n",
    "#    datasets[i]['identifier'] = datasets[i]['identifier'].apply(lambda x: x.replace('_', ':'))\n",
    "#    datasets[i]['identifier'] = datasets[i]['identifier'].apply(lambda x: x + '_None')\n",
    "    # preprocess OCR and GT\n",
    "#    datasets[i]['gt text'] = datasets[i]['gt text'].apply(lambda x: preprocessing(x))\n",
    "#    datasets[i]['ocr text'] = datasets[i]['gt text'].apply(lambda x: preprocessing(x))\n",
    "\n",
    "# find proper pronouns\n",
    "gt_names = []\n",
    "ocr_names = []\n",
    "for text in list(df['gt text']):\n",
    "    gt_names = find_names(text, gt_names)\n",
    "for text in list(df['ocr text']):\n",
    "    ocr_names = find_names(text, ocr_names)\n",
    "\n",
    "df['gt text org'] = df['gt text']\n",
    "df['ocr text org'] = df['ocr text']\n",
    "\n",
    "# preprocess identifier\n",
    "df['identifier'] = df['identifier'].apply(lambda x: str(x))\n",
    "df['identifier'] = df['identifier'].apply(lambda x: x.replace('_', '.'))\n",
    "df['identifier'] = df['identifier'].apply(lambda x: x.replace(':', '.'))\n",
    "#df['identifier'] = df['identifier'].apply(lambda x: x + '_None')\n",
    "# preprocess OCR and GT\n",
    "df['gt text'] = df['gt text'].apply(lambda x: preprocessing(x))\n",
    "df['ocr text'] = df['gt text'].apply(lambda x: preprocessing(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e015af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2f9695ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_matching(row): # create files with matched sentences\n",
    "    #save_path = r\"C:\\Users\\Gebruiker\\Desktop\\Thesis\\Documenten\\Documenten\"\n",
    "    \n",
    "    ID = row['identifier']\n",
    "    page = 'None'\n",
    "    \n",
    "    # split GT and OCR-output of each row into sentences (based on full stops).\n",
    "    gt_sentences = row['gt text'].split(\".\")\n",
    "    ocr_sentences = row['ocr text'].split(\".\")\n",
    "\n",
    "    match_list = []\n",
    "\n",
    "    for sentence in gt_sentences:\n",
    "        for match in ocr_sentences:\n",
    "            if SequenceMatcher(None, sentence, match).ratio() > 0.75:\n",
    "                match_list.extend([[sentence, match]])\n",
    "                \n",
    "    matched_gt_sentences = []\n",
    "    matched_ocr_sentences = []\n",
    "\n",
    "    for sentence in match_list:\n",
    "        if len(sentence[0]) or len(sentence[1]) > 0:\n",
    "            matched_gt_sentences.append(sentence[0])\n",
    "            matched_ocr_sentences.append(sentence[1])\n",
    "    \n",
    "    # .join() with lists\n",
    "    separator = '.'\n",
    "    doc_GT = separator.join(matched_gt_sentences)\n",
    "    doc_OCR = separator.join(matched_ocr_sentences)\n",
    "    \n",
    "    #return matched_gt_sentences, matched_ocr_sentences\n",
    "    return doc_GT, doc_OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c14da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a015f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jar_file = \"ocrevalUAtion-1.3.4-jar-with-dependencies.jar\"\n",
    "\n",
    "def evaluation(index, row, doc_GT, doc_OCR):\n",
    "    ID = row['identifier']\n",
    "    page = 'None'\n",
    "    \n",
    "    doc_OCR = doc_OCR.replace('.', '')\n",
    "    doc_OCR = re.sub(' +', ' ', doc_OCR)\n",
    "    filename_ocr = f\"{ID}_{page}_OCR.txt\"\n",
    "    #file_ocr = open(os.path.join(save_path, filename),\"w+\", encoding=\"utf-8\")\n",
    "    file_ocr = open(filename_ocr,\"w+\", encoding=\"utf-8\")\n",
    "    file_ocr.write(doc_OCR)\n",
    "    file_ocr.close()\n",
    "    \n",
    "    doc_GT = doc_GT.replace('.', '')\n",
    "    doc_GT = re.sub(' +', ' ', doc_GT)\n",
    "    filename_gt = f\"{ID}_{page}_GT.txt\"\n",
    "    #file_gt = open(os.path.join(save_path, filename),\"w+\", encoding=\"utf-8\")\n",
    "    file_gt = open(filename_gt,\"w+\", encoding=\"utf-8\")\n",
    "    file_gt.write(doc_GT)\n",
    "    file_gt.close()\n",
    "    \n",
    "    #output = ID + '_' + page + \".html\"\n",
    "    output = f\"{ID}_{page}.html\"\n",
    "    \n",
    "    #process = subprocess.call(\"/home/nvanthof/jdk-16.0.1/bin/java -cp \" + jar_file  + \" eu.digitisation.Main -gt \" + filename_gt + \" -ocr \"+ filename_ocr +\" -o \" + output + \"\")\n",
    "    #os.system(\"/home/nvanthof/jdk-16.0.1/bin/java -cp /home/nvanthof/ocrevalUAtion-1.3.4-jar-with-dependencies.jar eu.digitisation.Main -gt /home/nvanthof/ddd.010728187.mpeg21.a0005_None_GT.txt -ocr /home/nvanthof/ddd.010728187.mpeg21.a0005_None_OCR.txt  -o /home/nvanthof/OUTPUT2.html\")\n",
    "    command = f\"/home/nvanthof/jdk-16.0.1/bin/java -cp /home/nvanthof/ocrevalUAtion-1.3.4-jar-with-dependencies.jar eu.digitisation.Main -gt /home/nvanthof/{filename_gt} -ocr /home/nvanthof/{filename_ocr}  -o /home/nvanthof/{output}\"\n",
    "    os.system(command)\n",
    "    sleep(5)\n",
    "    \n",
    "    soup = BeautifulSoup(open(output, encoding='utf-8'))\n",
    "    table = soup.find(\"table\", attrs={'border': '1'})\n",
    "    # Split the filename, and extract the identifier and pagenr together as identifier \n",
    "    # Find the first table (this is the table in which the scores are stored)\n",
    "    # Find the tags in which 'CER', 'WER', and 'WER (order independent)' are stored and take the next tag to get the score \n",
    "    cer = table.find('td', text='CER')\n",
    "    cerScore = cer.findNext('td')\n",
    "    wer = table.find('td', text='WER')\n",
    "    werScore = wer.findNext('td')\n",
    "    werOI = table.find('td', text='WER (order independent)')\n",
    "    werOIScore = werOI.findNext('td')\n",
    "    \n",
    "    os.remove(filename_gt)\n",
    "    os.remove(filename_ocr)\n",
    "    os.remove(output)\n",
    "    \n",
    "    df.at[index, 'CER matched sentences'], df.at[index, 'WER matched sentences'] = cerScore.text, werScore.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3a7d7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-a21774212aee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdoc_GT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_OCR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuzzy_matching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m#GT_matched_docs_list.append(matched_gt_sentences)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#OCR_matched_docs_list.append(matched_ocr_sentences)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-92740ac21602>\u001b[0m in \u001b[0;36mfuzzy_matching\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgt_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mocr_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mSequenceMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0mmatch_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/difflib.py\u001b[0m in \u001b[0;36mratio\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \"\"\"\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtriple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_calculate_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/difflib.py\u001b[0m in \u001b[0;36mget_matching_blocks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0malo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mahi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbhi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_longest_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mahi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbhi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m             \u001b[0;31m# a[alo:i] vs b[blo:j] unknown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0;31m# a[i:i+k] same as b[j:j+k]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/difflib.py\u001b[0m in \u001b[0;36mfind_longest_match\u001b[0;34m(self, alo, ahi, blo, bhi)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mj2lenget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj2len\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mnewj2len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb2j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# a[i] matches b[j]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mblo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    print(index)\n",
    "    if index%1000 == 0:\n",
    "        print(index)\n",
    "    doc_GT, doc_OCR = fuzzy_matching(row)\n",
    "    #GT_matched_docs_list.append(matched_gt_sentences)\n",
    "    #OCR_matched_docs_list.append(matched_ocr_sentences)\n",
    "    df.at[index, 'gt sentences matched'], df.at[index, 'ocr sentences matched'] = doc_GT, doc_OCR\n",
    "    #evaluation(index, row, doc_GT, doc_OCR)\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a35421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count sentences (longer than 4 words) in GT and OCR\n",
    "# count words in sentences longer than 4 words\n",
    "\n",
    "#meta_df = pd.DataFrame(columns=['identifier', 'sentences gt', 'sentences ocr', 'avg sentence length gt', ' avg sentence length ocr'])\n",
    "meta_df = pd.DataFrame()\n",
    "\n",
    "def sentence_count(row):\n",
    "# count of sentences in GT >= 4 words\n",
    "    sentences_gt = 0\n",
    "    words_GT_sentences = []\n",
    "    #characters_GT_sentences = []\n",
    "    for k in row['gt text'].split('.'):\n",
    "        k = [word for word in k.split(' ') if len(word) > 1]\n",
    "        if len(k) >= 4:\n",
    "            sentences_gt += 1\n",
    "            words_GT_sentences.append(len(k))\n",
    "            #characters_GT_sentences.append(len(' '.join(k)))\n",
    "    # count of sentences in OCR >= 4 words\n",
    "    sentences_ocr = 0\n",
    "    words_OCR_sentences = []\n",
    "    #characters_OCR_sentences = []\n",
    "    for k in row['ocr text'].split('.'):\n",
    "        k = [word for word in k.split(' ') if len(word) > 1]\n",
    "        if len(k) >= 4:\n",
    "            sentences_ocr += 1\n",
    "            words_OCR_sentences.append(len(k))\n",
    "            #characters_OCR_sentences.append(len(' '.join(k)))\n",
    "\n",
    "    return sentences_gt, sentences_ocr, words_GT_sentences, words_OCR_sentences\n",
    "\n",
    "\n",
    "df['gt text'] = df['gt text'].apply(lambda x: preprocessing(x))\n",
    "\n",
    "for index, row in df.iterrows(): \n",
    "    sentences_gt, sentences_ocr, words_GT_sentences, words_OCR_sentences = sentence_count(row)\n",
    "    meta_df = meta_df.append({'identifier': row['identifier'], \\\n",
    "                              'sentences gt (fuzzy matched)': sentences_gt, 'sentences ocr (fuzzy matched)': sentences_ocr, \\\n",
    "                              'avg sentence length gt (fuzzy matched)': s.mean(words_GT_sentences), 'avg sentence length ocr (fuzzy matched)': s.mean(words_OCR_sentences), \\\n",
    "                              'max sentence length gt (fuzzy matched)': max(words_GT_sentences), 'max sentence length ocr (fuzzy matched)': max(words_OCR_sentences), \\\n",
    "                              'word count gt (fuzzy matched)': sum(words_GT_sentences), 'word count ocr (fuzzy matched)': sum(words_OCR_sentences)}, ignore_index = True)\n",
    "    \n",
    "df = pd.merge(left=df, right=meta_df, on='identifier')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc9743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of values\n",
    "df.groupby('century').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aea07b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of values\n",
    "df.groupby('century').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba13f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word alignment\n",
    "def word_alignment(GT, OCR):\n",
    "    shortest_list = min([GT, OCR], key=len)\n",
    "    longest_list = max([GT, OCR], key=len)\n",
    "    min_ratio = 0.65\n",
    "\n",
    "    for i in range(len(longest_list)):\n",
    "        match = False\n",
    "        #i = -i-1\n",
    "        # check if there is a direct match:\n",
    "        try:\n",
    "            if OCR[i] == '' and GT[i] == '':\n",
    "                del OCR[i]\n",
    "                del GT[i]\n",
    "            if SequenceMatcher(None, GT[i], OCR[i]).ratio() >= min_ratio:\n",
    "                match = True\n",
    "                continue\n",
    "            elif match == False:\n",
    "                # check if there is a match with a word before in OCR:\n",
    "                try:\n",
    "                    for j in range(1,10):\n",
    "                        if i-j >= 0:\n",
    "                            if j >= 5:\n",
    "                                if (GT[i] >= 5) or (OCR[i-j] >= 5):\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    continue\n",
    "                            if SequenceMatcher(None, GT[i], OCR[i-j]).ratio() >= min_ratio:\n",
    "                                if not (GT[i] == '' and OCR[i-j] == ''):\n",
    "                                    for k in range(0,j):\n",
    "                                        OCR.insert(i-j, '')\n",
    "                                    match = True\n",
    "                                    continue\n",
    "                        if i+j <= len(OCR):\n",
    "                            if j >= 5:\n",
    "                                if (GT[i] >= 5) or (OCR[i+j] >= 5):\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    continue\n",
    "                            if SequenceMatcher(None, GT[i], OCR[i+j]).ratio() >= min_ratio:\n",
    "                                if not (GT[i] == '' and OCR[i+j] == ''):\n",
    "                                    for k in range(0,j):\n",
    "                                        GT.insert(i, '')\n",
    "                                    match = True\n",
    "                                    continue\n",
    "                except:\n",
    "                    pass\n",
    "        except IndexError:\n",
    "            min([GT, OCR], key=len).append('')\n",
    "\n",
    "        if match == False:\n",
    "            try:\n",
    "                if SequenceMatcher(None, GT[i], OCR[i]).ratio() >= 0.30:\n",
    "                    pass\n",
    "            except:\n",
    "                pass\n",
    "            else:\n",
    "                #print('bad match', GT[i], OCR[i])\n",
    "                #print('before forwards match GT:', GT)\n",
    "                #print('before forwards bad match OCR:', OCR)\n",
    "                if i-1 >= 0:\n",
    "                    GT.insert(i, '')\n",
    "                else:\n",
    "                    GT.insert(0, '')\n",
    "                if i+1 <= len(GT):\n",
    "                    OCR.insert(i+1, '')\n",
    "                else:\n",
    "                    GT.append('')\n",
    "                #print('after forwards match GT:', GT)\n",
    "                #print('after forwards bad match OCR:', OCR)\n",
    "                \n",
    "                continue\n",
    "    \n",
    "   #print('after forwards GT:', GT)\n",
    "    #print('after forwards OCR:', OCR)\n",
    "    \n",
    "    while len(GT) != len(OCR):\n",
    "        min([GT, OCR], key=len).append('')\n",
    "    \n",
    "    if True:\n",
    "        \n",
    "        shortest_list = min([GT, OCR], key=len)\n",
    "        longest_list = max([GT, OCR], key=len)\n",
    "\n",
    "        for i in range(len(longest_list)):\n",
    "            i = -i-1\n",
    "            #print('i:', i)\n",
    "            try:\n",
    "\n",
    "                if (OCR[i] == '') ^ (GT[i] == ''):\n",
    "\n",
    "                    continue\n",
    "                if OCR[i] == ' ' and GT[i] == '':\n",
    "                    del OCR[i]\n",
    "                    del GT[i]\n",
    "                if SequenceMatcher(None, GT[i], OCR[i]).ratio() > 0.65:\n",
    "     \n",
    "                    continue\n",
    "                else:\n",
    "                    # check if there is a match with a word before in OCR:\n",
    "                    try:\n",
    "                        for j in range(1,len(GT)):\n",
    "                            if j >= 10:\n",
    "                                extra_ratio = 0.1\n",
    "                            else:\n",
    "                                extra_ratio = 0\n",
    "                            if i-j >= -len(OCR):\n",
    "                                if j >= 5:\n",
    "                                    if (len(GT[i]) >= 5) or (len(OCR[i-j]) >=5):\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        continue\n",
    "                            if GT[i] == '' or OCR[i-j] == '':\n",
    "                                continue\n",
    "                            if SequenceMatcher(None, GT[i], OCR[i-j]).ratio() > 0.65 + extra_ratio:\n",
    "                                for k in range(0,j):\n",
    "                                    if i+k < 0:\n",
    "                                        GT.insert(i+1, '')\n",
    "                                    else: \n",
    "                                        GT.append('')\n",
    "                                break\n",
    "                            if i+j <= -1:\n",
    "                                if j >= 5:\n",
    "                                    if (GT[i] >= 5) or (OCR[i+j] >=5):\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        continue\n",
    "                            if GT[i] == '' or OCR[i+j] == '':\n",
    "                                continue\n",
    "                            if SequenceMatcher(None, GT[i], OCR[i+j]).ratio() > 0.65 + extra_ratio:\n",
    "                                for k in range(0,j):\n",
    "                                    GT.insert(i, '')\n",
    "\n",
    "                                break\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            except IndexError:\n",
    "                min([GT, OCR], key=len).append('')\n",
    "\n",
    "            # if there is no match:\n",
    "            if SequenceMatcher(None, GT[i], OCR[i]).ratio() > 0.30:\n",
    "                pass\n",
    "            else:\n",
    "                #print('bad match', GT[i], OCR[i])\n",
    "                #print('before backwards bad match GT:',  GT)\n",
    "                #print('before backwards bad match OCR:',  OCR)\n",
    "                if (OCR[i] == '') or (GT[i] == ''):\n",
    "                    continue\n",
    "                else:\n",
    "                    GT.insert(i, '')\n",
    "                    OCR.insert(i+1, '')\n",
    "                #print('after backwards bad match GT:',  GT)\n",
    "                #print('after backwards bad match OCR:',  OCR)\n",
    "                \n",
    "                \n",
    "    #print('after backwards GT:',  GT)\n",
    "    #print('after backwards OCR:',  OCR)\n",
    "                    \n",
    "    while len(GT) != len(OCR):\n",
    "        min([GT, OCR], key=len).append('')\n",
    "\n",
    "\n",
    "    for i in range(len(GT)-1):\n",
    "        try:\n",
    "            #print(i)\n",
    "            #print(GT[i], OCR[i])\n",
    "            if (GT[i] == '') and (OCR[i] == ''):\n",
    "                del GT[i]\n",
    "                del OCR[i]\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "    try:\n",
    "        if SequenceMatcher(None, GT[-1], OCR[-2]).ratio() >= 0.65 and ((GT[-1] != '') or ((OCR[-2] != ''))):\n",
    "            #print('FOUND:', GT[-1], OCR[-2])\n",
    "            OCR.insert(-2, '')\n",
    "            GT.append('')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        if SequenceMatcher(None, GT[-2], OCR[-1]).ratio() >= 0.65 and ((GT[-2] != '') or ((OCR[-1] != ''))):\n",
    "            #print('FOUND:', OCR[-1], GT[-2])\n",
    "            GT.insert(-2, '')\n",
    "            OCR.append('')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for i in range(len(OCR)):\n",
    "        if GT[i] == '' and OCR[i] == '':\n",
    "            GT[i] = 'REMOVE'\n",
    "            OCR[i] = 'REMOVE'\n",
    "\n",
    "    GT = list(filter(lambda a: a != 'REMOVE', GT))\n",
    "    OCR = list(filter(lambda a: a != 'REMOVE', OCR))\n",
    "    \n",
    "    \n",
    "    #print('end GT:',  GT)\n",
    "    #print('end OCR:',  OCR)\n",
    "    return GT, OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed156354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify word alignment quality\n",
    "def verify_alignment(GT, OCR):\n",
    "    \n",
    "    if len(GT) != len(OCR):\n",
    "        alignment_validation = False\n",
    "        return alignment_validation\n",
    "    \n",
    "    min_word_length = 8 # the higher this number, the more likely a alignment will be seen as valid, thus higher chance at false positive, lower chance at false negative. \n",
    "    match_ratio = 0.70\n",
    "    anchors_list = []\n",
    "    for i in range(len(GT)):\n",
    "        if len(GT[i]) >= min_word_length:  \n",
    "            anchors_list.append(GT[i])\n",
    "        if len(OCR[i]) >= min_word_length:\n",
    "            anchors_list.append(OCR[i])\n",
    "    #print(anchors_list)\n",
    "\n",
    "\n",
    "    alignment_validation = True\n",
    "\n",
    "    for anchor in anchors_list:\n",
    "        #print('ANCHOR:', anchor)\n",
    "        matches_GT = set()\n",
    "        matches_OCR = set()\n",
    "        match = False\n",
    "        for i in range(len(GT)):\n",
    "            if (SequenceMatcher(None, GT[i], anchor).ratio() >= 0.65) and (len(GT[i]) >= min_word_length or len(GT[i]) == 0):\n",
    "                matches_GT.add(i)\n",
    "                #print('added as GT match:', GT[i])\n",
    "            if SequenceMatcher(None, OCR[i], anchor).ratio() >= 0.65 and (len(OCR[i]) >= min_word_length or len(OCR[i]) == 0):\n",
    "                matches_OCR.add(i)\n",
    "                #print('added as OCR match:', OCR[i])\n",
    "        #print('anchor:', anchor)\n",
    "        #print(matches_GT)\n",
    "        #print(matches_OCR)\n",
    "        if matches_GT == matches_OCR:\n",
    "            match = True\n",
    "        if match == False:\n",
    "            match = True\n",
    "            points_set1 = set() # counts when the matched opposite word is '' for OCR\n",
    "            points_set2 = set() # counts when the matched opposite word is '' for GT\n",
    "            for index in matches_GT.difference(matches_OCR): # elements in matches_GT that are not in matches_OCR\n",
    "                if OCR[index] == '':\n",
    "                    points_set1.add(index)\n",
    "                if OCR[index] != '' and len(matches_OCR.difference(matches_GT)) != 0:\n",
    "                    match = False\n",
    "            for index in matches_OCR.difference(matches_GT): # elements in matches_OCR that are not in matches_GT\n",
    "                if GT[index] == '':\n",
    "                    points_set2.add(index)\n",
    "                if GT[index] != '' and len(matches_GT.difference(matches_OCR)) != 0:\n",
    "                    match = False\n",
    "            if len(points_set1.intersection(points_set2)) != 0: # if an anchor words was match in both OCR and GT with an '', it's a bad match.\n",
    "                match = False\n",
    "        if match == False:\n",
    "            for index in (matches_GT.difference(matches_OCR)):\n",
    "                index += 1\n",
    "                if index in matches_OCR.difference(matches_GT):\n",
    "                    if (GT[index-1] == '' or OCR[index-1] == '') and (SequenceMatcher(None, GT[index], OCR[index]).ratio() >= match_ratio):\n",
    "                        match = True\n",
    "            for index in (matches_OCR.difference(matches_GT)):\n",
    "                index += 1\n",
    "                if index in matches_GT.difference(matches_OCR):\n",
    "                    if (GT[index] == '' or OCR[index] == '') and (SequenceMatcher(None, GT[index-1], OCR[index-1]).ratio() >= match_ratio):\n",
    "                        match = True\n",
    "\n",
    "        if match == False:\n",
    "            alignment_validation = False # bad alignment\n",
    "\n",
    "    return alignment_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_aligned_GT_docs = []\n",
    "list_aligned_OCR_docs = []\n",
    "good_alignments_doc = []\n",
    "bad_alignments_doc = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    GT_sentences = row['gt sentences matched'].split('.')\n",
    "    OCR_sentences = row['ocr sentences matched'].split('.')\n",
    "    good_alignments = 0\n",
    "    bad_alignments = 0\n",
    "    aligned_GT = []\n",
    "    aligned_OCR = []\n",
    "    for i in range(len(GT_sentences)):\n",
    "        GT = GT_sentences[i].split(' ')\n",
    "        OCR = OCR_sentences[i].split(' ')\n",
    "        GT, OCR = word_alignment(GT, OCR)\n",
    "        verification = verify_alignment(GT, OCR)\n",
    "        if verification == False: # bad alignment\n",
    "            GT = ['REMOVED'] * len(GT)\n",
    "            bad_alignments += 1\n",
    "        else:\n",
    "            if len(OCR) >= 4:\n",
    "                good_alignments += 1\n",
    "        aligned_GT.append(GT)\n",
    "        aligned_OCR.append(OCR)\n",
    "    list_aligned_GT_docs.append(aligned_GT)\n",
    "    list_aligned_OCR_docs.append(aligned_OCR)\n",
    "    good_alignments_doc.append(good_alignments)\n",
    "    bad_alignments_doc.append(bad_alignments)\n",
    "    \n",
    "df['aligned_GT_sentences'] = list_aligned_GT_docs\n",
    "df['aligned_OCR_sentences'] = list_aligned_OCR_docs\n",
    "df['good_alignments'] = good_alignments_doc\n",
    "df['bad_alignments'] = bad_alignments_doc    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de30e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_words_count(row):\n",
    "    ocr_aligned_sentences = row['aligned_OCR_sentences']\n",
    "    # analysing the word alignment\n",
    "    # word alignment for each sentence\n",
    "    longest_streaks = []\n",
    "    total_missing_words = []\n",
    "    perc_missing_words = []\n",
    "    for k in range(len(aligned_datasets_OCR[i][j])): # for each sentence\n",
    "        streaks = []\n",
    "        streak = 0\n",
    "        for l in range(len(aligned_datasets_OCR[i][j][k])): # for each word\n",
    "            if aligned_datasets_OCR[i][j][k][l] == '':\n",
    "                streak += 1\n",
    "            else:\n",
    "                streaks.append(streak)\n",
    "                streak = 0\n",
    "        longest_streaks.append(max(streaks))# longest streak of missing word in sentence\n",
    "        total_missing_words.append(sum(streaks)) # total of missing words in a sentence\n",
    "        perc_missing_words.append(sum(streaks) / len(aligned_datasets_OCR[i][j]))\n",
    "    # statistics word alignment doc level:\n",
    "    # find longest streak of all longest streaks:\n",
    "    import statistics as s\n",
    "    longest_streak = max(longest_streaks)\n",
    "    avg_longest_streaks = s.mean(longest_streaks)\n",
    "    avg_total_missing_words = s.mean(total_missing_words)\n",
    "    avg_perc_missing_words = s.mean(perc_missing_words)\n",
    "    \n",
    "for index, row in df.iterrows():\n",
    "    print(row['aligned_OCR_sentences'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fef7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ecf18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e74fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4952f951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51581433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775969e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256277f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778615a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('preprocessed_df100.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f23c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(list(df['max sentence length gt (fuzzy matched)'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4664967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
