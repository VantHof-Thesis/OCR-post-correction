{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e147a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# used for final analysis\n",
    "# baseline detection and correction, no final task\n",
    "\n",
    "print('ok')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle\n",
    "import ast\n",
    "import statistics as s\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "import copy\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import subprocess\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.dammit import EncodingDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361c3781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96279a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#transformers.__version__\n",
    "#torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57212865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48ae3849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_5K.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f5c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e12d5a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('all_lists_tokens.txt', 'rb') as f:\n",
    "#    all_lists_tokens = pickle.load(f)\n",
    "    \n",
    "#all_lists_tokens = ast.literal_eval(all_lists_tokens)\n",
    "#vocab_BERT, vocab_word2vec, hist_expressions, modern_vocab, dictionary = all_lists_tokens\n",
    "\n",
    "with open('homonyms.txt', 'rb') as f:\n",
    "    homonyms = pickle.load(f)\n",
    "with open('vocab_BERT', 'rb') as f:\n",
    "    vocab_BERT = pickle.load(f)\n",
    "with open('vocab_word2vec.txt', 'rb') as f:\n",
    "    vocab_word2vec = pickle.load(f)\n",
    "with open('hist_expressions.txt', 'rb') as f:\n",
    "    hist_expressions = pickle.load(f)\n",
    "with open('infrequent_expressions.txt', 'rb') as f:\n",
    "    infrequent_expressions = pickle.load(f)\n",
    "with open('dictionary.txt', 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3007e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lists_tokens = [homonyms, vocab_BERT, vocab_word2vec, hist_expressions, infrequent_expressions, dictionary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bf56803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skiplist (words that should not be corrected: names)\n",
    "with open(\"ocr_names.txt\", \"rb\") as fp:   # Unpickling\n",
    "    ocr_names = pickle.load(fp)\n",
    "\n",
    "ocr_names = []\n",
    "for name in ocr_names:\n",
    "    if len(name) >= 5:\n",
    "        ocr_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "355614e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_merger(lists):\n",
    "    #normal_list = False\n",
    "    #for elem in lists:\n",
    "    #    if type(elem) != list:\n",
    "    #        normal_list = True\n",
    "    #if normal_list == True:\n",
    "    #    return lists\n",
    "    #else:\n",
    "    new_list = []\n",
    "    for elem in lists:\n",
    "        new_list = new_list + elem\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3c56d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sorted(candidates, sim_or_probs, LD): # sorts first by LD, then by similarity/probability\n",
    "    paired_sorted = sorted(zip(LD,sim_or_probs,candidates),key = lambda x: (x[0],x[1]), reverse=True)\n",
    "    LD,sim_or_probs,candidates = zip(*paired_sorted)\n",
    "    correction = candidates[0]\n",
    "    return correction\n",
    "    \n",
    "def correct_calculated(candidates, sim_or_probs, LD): # calculates a score from LD and normalised similarity/probability\n",
    "    inv_LD = 1 - LD\n",
    "    sim_or_probs = np.array(sim_or_probs)\n",
    "    sim_or_probs = np.interp(sim_or_probs, (sim_or_probs.min(), sim_or_probs.max()), (0, 1)).tolist()\n",
    "    score = sim_or_probs / inv_LD\n",
    "    zipped_pairs = zip(score.tolist(), candidates)\n",
    "    sorted_by_score = [x for _, x in sorted(zipped_pairs, reverse=True)]\n",
    "    correction = sorted_by_score[0]\n",
    "    return correction\n",
    "\n",
    "def remove_stopwords(candidates, cosine, LD):\n",
    "    #nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('dutch'))\n",
    "    candidates_nostopwords = []\n",
    "    cosine_nostopwords = []\n",
    "    LD_nostopwords = []\n",
    "    for i in range(len(candidates)):\n",
    "        if candidates[i] not in stop_words:\n",
    "            candidates_nostopwords.append(candidates[i])\n",
    "            cosine_nostopwords.append(cosine[i])\n",
    "            LD_nostopwords.append(LD[i])\n",
    "    LD_nostopwords = np.array(LD_nostopwords)\n",
    "    return candidates_nostopwords, cosine_nostopwords, LD_nostopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a4cc6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of all TP, FN, FP, TN detection:\n",
    "homonyms_detection_list_baseline = [[],[],[],[]]\n",
    "histexp_detection_list_baseline = [[],[],[],[]]\n",
    "OOV_detection_list_baseline = [[],[],[],[]]\n",
    "infreq_detection_list_baseline = [[],[],[],[]]\n",
    "RWE_detection_list_baseline = [[],[],[],[]]\n",
    "all_detection_list_baseline = [[],[],[],[]]\n",
    "none_detection_list_baseline = [[],[],[],[]]\n",
    "\n",
    "# list of all right / wrong correction\n",
    "homonyms_correction_list_baseline = [[],[]]\n",
    "histexp_correction_list_baseline = [[],[]]\n",
    "OOV_correction_list_baseline = [[],[]]\n",
    "infreq_correction_list_baseline = [[],[]]\n",
    "RWE_correction_list_baseline = [[],[]]\n",
    "all_correction_list_baseline = [[],[]]\n",
    "none_correction_list_baseline = [[],[],[],[]]\n",
    "\n",
    "#list of outputs corrected texts\n",
    "#new_documents = []\n",
    "\n",
    "#list of improved and worsened\n",
    "improved_all = []\n",
    "worsened_all = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd1dd3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_result(predicted_error, actual_error):\n",
    "    if actual_error == True:\n",
    "        if predicted_error == True: # TP\n",
    "            result = 'TP'\n",
    "        if predicted_error == False: # FN\n",
    "            result = 'FN'\n",
    "    if actual_error == False:\n",
    "        if predicted_error == True: # FP\n",
    "            result = 'FP'\n",
    "        if predicted_error == False: # TN\n",
    "            result = 'TN'\n",
    "    return result\n",
    "\n",
    "def special_tokens_detection_word(ocr_word, gt_word, detection_list_baseline, all_lists_tokens, result): \n",
    "    homonyms, vocab_BERT, vocab_word2vec, hist_expressions, infrequent_expressions, dictionary = all_lists_tokens\n",
    "    special_token = False\n",
    "    homonym, hist_exp, OOV, infreq, RWE = False, False, False, False, False\n",
    "    # check if word is homonym\n",
    "    if gt_word in homonyms:\n",
    "        homonym = True\n",
    "        special_token = True\n",
    "    # check if word is historical expression\n",
    "    if gt_word in hist_expressions:\n",
    "        hist_exp = True\n",
    "        special_token = True\n",
    "    # check if word is OOV\n",
    "    if gt_word not in dictionary:\n",
    "        OOV = True\n",
    "        special_token = True\n",
    "    # check if word is infrequent\n",
    "    if gt_word in infrequent_expressions:\n",
    "        infreq = True\n",
    "        special_token = True\n",
    "    # check if word is RWE\n",
    "    if (ocr_word in dictionary) and ((result == 'TP') or (result == 'FN')):\n",
    "        RWE = True\n",
    "        special_token = True\n",
    "    # adding the results to the right list\n",
    "    if result == 'TP': # TP = [0]\n",
    "        # all = detection_lit[5]\n",
    "        detection_list_baseline[5][0] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_baseline[0][0] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_baseline[1][0] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_baseline[2][0] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_baseline[3][0] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_baseline[4][0] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_baseline[6][0] += 1\n",
    "    if result == 'FN': # FN = [1]\n",
    "        # all = detection_lit[5]\n",
    "        detection_list_baseline[5][1] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_baseline[0][1] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_baseline[1][1] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_baseline[2][1] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_baseline[3][1] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_baseline[4][1] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_baseline[6][1] += 1\n",
    "    if result == 'FP': # FP = [2]\n",
    "        # all = detection_lit[5]\n",
    "        detection_list_baseline[5][2] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_baseline[0][2] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_baseline[1][2] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_baseline[2][2] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_baseline[3][2] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_baseline[4][2] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_baseline[6][2] += 1\n",
    "    if result == 'TN': # TN = [3]\n",
    "        # all = detection_list[5]\n",
    "        detection_list_baseline[5][3] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_baseline[0][3] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_baseline[1][3] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_baseline[2][3] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_baseline[3][3] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_baseline[4][3] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_baseline[6][3] += 1\n",
    "    return detection_list_baseline\n",
    "\n",
    "    \n",
    "\n",
    "def special_tokens_correction_word(ocr_word, gt_word, correction_list_baseline, all_lists_tokens, result): \n",
    "    homonyms, vocab_BERT, vocab_word2vec, hist_expressions, infrequent_expressions, dictionary = all_lists_tokens\n",
    "    special_token = False\n",
    "    homonym, hist_exp, OOV, infreq, RWE = False, False, False, False, False\n",
    "    # check if word is homonym\n",
    "    if gt_word in homonyms:\n",
    "        homonym = True\n",
    "        special_token = True\n",
    "    # check if word is historical expression\n",
    "    if gt_word in hist_expressions:\n",
    "        hist_exp = True\n",
    "        special_token = True\n",
    "    # check if word is OOV\n",
    "    if gt_word not in vocab_word2vec:\n",
    "        OOV = True\n",
    "        special_token = True\n",
    "    # check if word is infrequent\n",
    "    if gt_word in infrequent_expressions:\n",
    "        infreq = True\n",
    "        special_token = True\n",
    "    # check if word is RWE\n",
    "    if ocr_word in dictionary:\n",
    "        RWE = True\n",
    "        special_token = True\n",
    "    # adding the results to the right list\n",
    "    if result == 'right': # wrong = [0]\n",
    "        # all = detection_lit[5]\n",
    "        correction_list_baseline[5][0] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            correction_list_baseline[0][0] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            correction_list_baseline[1][0] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            correction_list_baseline[2][0] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            correction_list_baseline[3][0] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            correction_list_baseline[4][0] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            correction_list_baseline[6][0] += 1\n",
    "    if result == 'wrong': # right = [1]\n",
    "        # all = detection_lit[5]\n",
    "        correction_list_baseline[5][1] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            correction_list_baseline[0][1] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            correction_list_baseline[1][1] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            correction_list_baseline[2][1] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            correction_list_baseline[3][1] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            correction_list_baseline[4][1] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            correction_list_baseline[6][1] += 1\n",
    "    return correction_list_baseline\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08dc0e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n",
      "0\n",
      "20\n",
      "140\n",
      "210\n",
      "230\n",
      "250\n",
      "260\n",
      "370\n",
      "460\n",
      "470\n",
      "490\n",
      "580\n",
      "600\n",
      "620\n",
      "640\n",
      "660\n",
      "710\n",
      "720\n",
      "910\n",
      "930\n",
      "940\n",
      "1020\n",
      "1040\n",
      "1140\n",
      "1180\n",
      "1190\n",
      "1230\n",
      "1380\n",
      "1410\n",
      "1420\n",
      "1500\n",
      "1540\n",
      "1660\n",
      "1680\n",
      "1730\n",
      "1780\n",
      "1820\n",
      "1830\n",
      "1860\n",
      "1870\n",
      "1910\n",
      "1970\n",
      "2020\n",
      "2140\n",
      "2150\n",
      "2180\n",
      "2210\n",
      "2230\n",
      "2240\n",
      "2270\n",
      "2300\n",
      "2310\n",
      "2340\n",
      "2370\n",
      "2380\n",
      "2390\n",
      "2460\n",
      "2510\n",
      "2550\n",
      "2560\n",
      "2590\n",
      "2640\n",
      "2670\n",
      "2720\n",
      "2730\n",
      "2800\n",
      "2850\n",
      "3040\n",
      "3230\n",
      "3260\n",
      "3340\n",
      "3370\n",
      "3410\n",
      "3440\n",
      "3450\n",
      "3480\n",
      "3510\n",
      "3570\n",
      "3610\n",
      "3650\n",
      "3660\n",
      "3720\n",
      "3740\n",
      "3810\n",
      "3850\n",
      "3900\n",
      "3960\n",
      "4010\n",
      "4040\n",
      "4050\n",
      "4080\n",
      "4220\n",
      "4250\n",
      "4260\n",
      "4270\n",
      "4320\n",
      "4330\n",
      "4340\n",
      "4350\n",
      "4490\n",
      "4500\n",
      "4510\n",
      "4590\n",
      "4640\n",
      "4670\n",
      "4700\n",
      "4730\n",
      "4740\n",
      "4790\n",
      "4840\n",
      "4860\n",
      "4890\n",
      "4990\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_documents = []\n",
    "dictionary = list(set(dictionary))\n",
    "\n",
    "#detection test word2vec\n",
    "def detection_and_correction_dict(row, dictionary, ocr_names, all_lists_token, window=5, topn_detection=1000, topn_correction=1000, correction_method = 'sorted'):  # choose 'sorted'/ 'sorted_nosw', 'calculated'\n",
    "    if row['set'] != 'test':\n",
    "        return np.nan\n",
    "    else:\n",
    "        #print('arrived new')\n",
    "        identifier = row['identifier']\n",
    "        OCR_text = row['aligned_OCR_sentences']\n",
    "        GT_text = row['aligned_GT_sentences']\n",
    "        OCR_text = ast.literal_eval(OCR_text)\n",
    "        GT_text = ast.literal_eval(GT_text)\n",
    "        OCR_text = list_merger(OCR_text)\n",
    "        GT_text = list_merger(GT_text)\n",
    "\n",
    "        # keep track of performance detection\n",
    "        homonyms_detection_baseline = [0,0,0,0]\n",
    "        histexp_detection_baseline = [0,0,0,0]\n",
    "        OOV_detection_baseline = [0,0,0,0]\n",
    "        infreq_detection_baseline = [0,0,0,0]\n",
    "        RWE_detection_baseline = [0,0,0,0]\n",
    "        all_detection_baseline = [0,0,0,0]\n",
    "        none_detection_baseline = [0,0,0,0]\n",
    "        \n",
    "        # keep track of performance correction right / wrong\n",
    "        homonyms_correction_baseline = [0,0]\n",
    "        histexp_correction_baseline = [0,0]\n",
    "        OOV_correction_baseline = [0,0]\n",
    "        infreq_correction_baseline = [0,0]\n",
    "        RWE_correction_baseline = [0,0]\n",
    "        all_correction_baseline = [0,0]\n",
    "        none_correction_baseline = [0,0]\n",
    "        \n",
    "        # create lists that save evaluation scores for this documents\n",
    "        detection_list_baseline = [homonyms_detection_baseline, histexp_detection_baseline, OOV_detection_baseline, infreq_detection_baseline, RWE_detection_baseline, all_detection_baseline, none_detection_baseline]\n",
    "        correction_list_baseline = [homonyms_correction_baseline, histexp_correction_baseline, OOV_correction_baseline, infreq_correction_baseline, RWE_correction_baseline, all_correction_baseline, none_correction_baseline]\n",
    "                \n",
    "        improved = 0 # when actual error is detected, and corrected rightly\n",
    "        worsened  = 0 # when actual non error is wrongfully detected, and corrected wrongly\n",
    "        \n",
    "        # create corrected file\n",
    "        new_document = []\n",
    "        \n",
    "        \n",
    "        for i in range(len(OCR_text)):\n",
    "            if (OCR_text[i] in ocr_names) or (OCR_text[i].isalpha() == False) or (len(OCR_text[i]) <= 2)  or (GT_text[i] == 'REMOVED'):\n",
    "                # add word to document if left unchanged\n",
    "                #new_document.append(OCR_text[i])\n",
    "                continue\n",
    "            # determine if token is predicted error or not\n",
    "            if OCR_text[i] in dictionary:\n",
    "                predicted_error = False\n",
    "            elif OCR_text[i] not in dictionary:\n",
    "                predicted_error = True\n",
    "            # determine if token is actual error or not\n",
    "            if OCR_text[i] != GT_text[i]:\n",
    "                actual_error = True\n",
    "            elif OCR_text[i] == GT_text[i]:\n",
    "                actual_error = False\n",
    "            result_det = calculate_result(predicted_error, actual_error)\n",
    "            # evaluate detection\n",
    "            detection_list_baseline = special_tokens_detection_word(OCR_text[i], GT_text[i], detection_list_baseline, all_lists_token, result_det)\n",
    "            # correction evaluation\n",
    "            if actual_error == True:\n",
    "                # calculate positions detection task\n",
    "                # try two correction methods\n",
    "                # first calculate the normalized LDs:\n",
    "                same_length_words = [word for word in dictionary if (((len(word) == len(OCR_text[i])) or (len(word) == len(OCR_text[i])+1) or (len(word) == len(OCR_text[i])-1)))]\n",
    "                LD = list(np.array([fuzz.ratio(OCR_text[i], word)/100 for word in same_length_words]))\n",
    "                max_index = LD.index(max(LD))\n",
    "                correction = dictionary[max_index]\n",
    "                if correction == GT_text[i]:\n",
    "                    result_cor = 'right'\n",
    "                elif correction != GT_text[i]:\n",
    "                    result_cor = 'wrong'\n",
    "                correction_list_baseline = special_tokens_correction_word(OCR_text[i], GT_text[i], detection_list_baseline, all_lists_token, result_cor)\n",
    "                \n",
    "            # perform whole task\n",
    "            # first, add OCR-word to file if skipped (see above)\n",
    "            # add word to document if not detected as an error\n",
    "            if predicted_error == False:\n",
    "                new_document.append(OCR_text[i])\n",
    "                continue\n",
    "            # if predicted to be an error, perform correction:\n",
    "            if actual_error == True:\n",
    "                correction = correction # correction was already created\n",
    "            elif actual_error == False:\n",
    "                same_length_words = [word for word in dictionary if (((len(word) == len(OCR_text[i])) or (len(word) == len(OCR_text[i])+1) or (len(word) == len(OCR_text[i])-1)))]\n",
    "                LD = list(np.array([fuzz.ratio(OCR_text[i], word)/100 for word in same_length_words]))\n",
    "                max_index = LD.index(max(LD))\n",
    "                correction = dictionary[max_index]\n",
    "                if correction == GT_text[i]:\n",
    "                    result_cor = 'right'\n",
    "                elif correction != GT_text[i]:\n",
    "                    result_cor = 'wrong'\n",
    "            new_document.append(correction)\n",
    "            #print(correction)\n",
    "            \n",
    "            if (result_det == 'TP') and (result_cor == 'right'):\n",
    "                improved += 1\n",
    "            elif (result_det == 'FP') and (result_cor == 'wrong'):\n",
    "                worsened += 1\n",
    "                \n",
    "        improved_all.append(improved)\n",
    "        worsened_all.append(worsened)\n",
    "            \n",
    "        new_document = (' ').join(new_document)\n",
    "        new_document = re.sub(' +', ' ', new_document)\n",
    "        new_documents.append(new_document)\n",
    "        \n",
    "        \n",
    "        for k in range(len(detection_list_baseline[0])): # for each result: 0 = TP, 1 = TN, 2 = FP, 3 = TN\n",
    "                # homonyms = index 0 in detection_list_baseline    \n",
    "                homonyms_detection_list_baseline[k].append(detection_list_baseline[0][k])\n",
    "                # hist_exp = index 1\n",
    "                histexp_detection_list_baseline[k].append(detection_list_baseline[1][k])\n",
    "                # OOV = index 2\n",
    "                OOV_detection_list_baseline[k].append(detection_list_baseline[2][k])\n",
    "                # infreq = index 3\n",
    "                infreq_detection_list_baseline[k].append(detection_list_baseline[3][k])\n",
    "                # RWE = index 4\n",
    "                RWE_detection_list_baseline[k].append(detection_list_baseline[4][k])\n",
    "                # all = index 5\n",
    "                all_detection_list_baseline[k].append(detection_list_baseline[5][k])\n",
    "                # non = index 6\n",
    "                none_detection_list_baseline[k].append(detection_list_baseline[6][k])\n",
    "        \n",
    "        # return correction evaluation values:\n",
    "        for k in range(2): # for each result: 0 = right, 1 = wrong\n",
    "                # homonyms = index 0 in detection_list_baseline    \n",
    "                homonyms_correction_list_baseline[k].append(correction_list_baseline[0][k])\n",
    "                # hist_exp = index 1\n",
    "                histexp_correction_list_baseline[k].append(correction_list_baseline[1][k])\n",
    "                # OOV = index 2\n",
    "                OOV_correction_list_baseline[k].append(correction_list_baseline[2][k])\n",
    "                # infreq = index 3\n",
    "                infreq_correction_list_baseline[k].append(correction_list_baseline[3][k])\n",
    "                # RWE = index 4\n",
    "                RWE_correction_list_baseline[k].append(correction_list_baseline[4][k])\n",
    "                # all = index 5\n",
    "                all_correction_list_baseline[k].append(correction_list_baseline[5][k])\n",
    "                # non = index 6\n",
    "                none_correction_list_baseline[k].append(correction_list_baseline[6][k])\n",
    "        \n",
    "        \n",
    "#for index, row in df.iterrows():\n",
    "#    detection_word2vec(row)\n",
    "# df.loc[70]\n",
    "fake_test_list_GT_aligned = \"\"\"12 Een koekenpan of kortweg pan is een platte pan met een lang handvat.\n",
    "De pan ontleent zijn naam aan het feit dat in zo'n pan 12 pannenkoeken worden gebakken. Ook ander voedsel, zoals vlees, wordt in een koekenpan gebraden 12 coninghs-merck\"\"\"\n",
    "fake_test_list_OCR_aligned = \"\"\"12 Een hoekenpan of kortweg pan is een platte pan met een hang handvat.\n",
    "De pan ontleent zijn naam haan het feit dat in zo'n pan 12 pannenkoeken horden gebakken. Ook ander voedsel, zoals vlees, word in een hoekenpan gebraden 12 coninghs-merck\"\"\"\n",
    "fake_test_list_GT_aligned = fake_test_list_GT_aligned.split('.')\n",
    "fake_test_list_OCR_aligned = fake_test_list_OCR_aligned.split('.')\n",
    "fake_test_list_GT_aligned = [x.split(' ') for x in fake_test_list_GT_aligned]\n",
    "fake_test_list_OCR_aligned = [x.split(' ') for x in fake_test_list_OCR_aligned]\n",
    "d = {'identifier': ['111'], 'aligned_OCR_sentences': [str(fake_test_list_OCR_aligned)], 'aligned_GT_sentences': [str(fake_test_list_GT_aligned)], 'set': ['test'], 'century': ['1600s'], 'source': ['Meertens']}\n",
    "\n",
    "df_probeer = pd.DataFrame(data=d)\n",
    "print('started')\n",
    "for index, row in df.loc[df['set'].isin(['test'])].iterrows():\n",
    "    if index%10==0:\n",
    "        print(index)\n",
    "    detection_and_correction_dict(row, dictionary, ocr_names, all_lists_tokens)  # choose 'sorted'/\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dad94eb4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae1132a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(new_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71eb9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(new_documents[0].split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097c26c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b9212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90df7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(homonyms_detection_list_baseline)\n",
    "#print(histexp_detection_list_baseline)\n",
    "#print(OOV_detection_list_baseline)\n",
    "#print(infreq_detection_list_baseline)\n",
    "#print(RWE_detection_list_baseline)\n",
    "#print(all_detection_list_baseline)\n",
    "#print(none_detection_list_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0a00f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#list(df_probeer[df_probeer[\"set\"] == 'test']['identifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f5245b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'homonyms_detection TP': homonyms_detection_list_baseline[0], 'homonyms_detection FN': homonyms_detection_list_baseline[1], 'homonyms_detection FP': homonyms_detection_list_baseline[2], 'homonyms_detection TN': homonyms_detection_list_baseline[3], \\\n",
    "    'histexp_detection TP': histexp_detection_list_baseline[0], 'histexp_detection FN': histexp_detection_list_baseline[1], 'histexp_detection FP': histexp_detection_list_baseline[2], 'histexp_detection TN': histexp_detection_list_baseline[3], \\\n",
    "    'OOV_detection TP': OOV_detection_list_baseline[0], 'OOV_detection FN': OOV_detection_list_baseline[1], 'OOV_detection FP': OOV_detection_list_baseline[2], 'OOV_detection TN': OOV_detection_list_baseline[3], \\\n",
    "    'infreq_detection TP': infreq_detection_list_baseline[0], 'infreq_detection FN': infreq_detection_list_baseline[1], 'infreq_detection FP': infreq_detection_list_baseline[2], 'infreq_detection TN': infreq_detection_list_baseline[3], \\\n",
    "    'RWE_detection TP': RWE_detection_list_baseline[0], 'RWE_detection FN': RWE_detection_list_baseline[1], 'RWE_detection FP': RWE_detection_list_baseline[2], 'RWE_detection TN': RWE_detection_list_baseline[3], \\\n",
    "    'all_detection TP': all_detection_list_baseline[0], 'all_detection FN': all_detection_list_baseline[1], 'all_detection FP': all_detection_list_baseline[2], 'all_detection TN': all_detection_list_baseline[3], \\\n",
    "    'none_detection TP': none_detection_list_baseline[0], 'none_detection FN': none_detection_list_baseline[1], 'none_detection FP': none_detection_list_baseline[2], 'none_detection TN': none_detection_list_baseline[3], \\\n",
    "    'identifier': list(df[df[\"set\"] == 'test']['identifier']), 'century': list(df[df[\"set\"] == 'test']['century']), 'source': list(df[df[\"set\"] == 'test']['source'])  }\n",
    "baseline_detection = pd.DataFrame(data=d)\n",
    "\n",
    "#baseline_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6e5d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'homonyms_correction right': homonyms_correction_list_baseline[0], 'homonyms_correction wrong': homonyms_correction_list_baseline[1],\\\n",
    "    'histexp_correction right': histexp_correction_list_baseline[0], 'histexp_correction wrong': histexp_correction_list_baseline[1], \\\n",
    "    'OOV_correction right': OOV_correction_list_baseline[0], 'OOV_correction wrong': OOV_correction_list_baseline[1],\\\n",
    "    'infreq_correction right': infreq_correction_list_baseline[0], 'infreq_correction wrong': infreq_correction_list_baseline[1],\\\n",
    "    'RWE_correction right': RWE_correction_list_baseline[0], 'RWE_correction wrong': RWE_correction_list_baseline[1],\\\n",
    "    'all_correction right': all_correction_list_baseline[0], 'all_correction wrong': all_correction_list_baseline[1],\\\n",
    "    'none_correction right': none_correction_list_baseline[0], 'none_correction wrong': none_correction_list_baseline[1],\\\n",
    "     'identifier': list(df[df[\"set\"] == 'test']['identifier']), 'century': list(df[df[\"set\"] == 'test']['century']), 'source': list(df[df[\"set\"] == 'test']['source'])}\n",
    "baseline_correction = pd.DataFrame(data=d)\n",
    "\n",
    "#baseline_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccb6e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_categories_baseline = \"homonyms_detection_baseline, histexp_detection_baseline, OOV_detection_baseline, infreq_detection_baseline, RWE_detection_baseline, all_detection_baseline, none_detection_baseline\".replace('_baseline', '').split(', ')\n",
    "\n",
    "for category in detection_categories_baseline:\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    F1s = []\n",
    "    accuracies = []\n",
    "\n",
    "    def calc_scores(row, category):\n",
    "        TP, FN, FP, TN = int(row[f'{category} TP']), int(row[f'{category} FN']),  int(row[f'{category} FP']),  int(row[f'{category} TN']),    \n",
    "        try:\n",
    "            precision = TP / (TP + FP)\n",
    "            recall = TP / (TP + FN)\n",
    "            F1 = 2*((precision*recall)/(precision+recall))\n",
    "        except ZeroDivisionError:\n",
    "            if (TP == 0) and (FP == 0) and (FN == 0):\n",
    "                precision = recall = F1 = 1\n",
    "            elif (TP == 0) and ((FP > 0) or (FN > 0)):\n",
    "                precision = recall = F1 = 0 \n",
    "        try:\n",
    "            accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "        except ZeroDivisionError:\n",
    "            accuracy = np.nan\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        F1s.append(F1)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    for index, row in baseline_detection.iterrows():\n",
    "        calc_scores(row, category)\n",
    "    \n",
    "    baseline_detection[f'{category} precision'] = precisions\n",
    "    baseline_detection[f'{category} recall'] = recalls\n",
    "    baseline_detection[f'{category} F1'] = F1s\n",
    "    baseline_detection[f'{category} accuracy'] = accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb5893ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvanthof/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "correction_categories_baseline = \"homonyms_correction_baseline, histexp_correction_baseline, OOV_correction_baseline, infreq_correction_baseline, RWE_correction_baseline, all_correction_baseline, none_correction_baseline\".replace('_baseline', '').split(', ')\n",
    "\n",
    "for category in correction_categories_baseline:\n",
    "    right, wrong = np.array(baseline_correction[f'{category} right']), np.array(baseline_correction[f'{category} wrong'])    \n",
    "    #try:\n",
    "    accuracy = right/(right+wrong)\n",
    "    #except ZeroDivisionError:\n",
    "    #    accuracy = np.nan*len(w2v_correction)\n",
    "    baseline_correction[f'{category} accuracy'] = accuracy\n",
    "    baseline_correction[f'{category} total'] = right + wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5182ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "#baseline_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "721b6952",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_orgs = list(df[df[\"set\"] == 'test']['gt text'])\n",
    "WER_orgs = list(df[df[\"set\"] == 'test']['WER matched sentences'])\n",
    "CER_orgs = list(df[df[\"set\"] == 'test']['CER matched sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec109d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47d3f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = baseline_detection.filter(regex='homonyms|OOV|all').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c19d04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'corrected document': new_documents, 'gt text': gt_orgs,'identifier': list(df[df[\"set\"] == 'test']['identifier']), 'century': list(df[df[\"set\"] == 'test']['century']), 'source': list(df[df[\"set\"] == 'test']['source']), \\\n",
    "    'improved': improved_all, 'worsened': worsened_all, 'old WER': WER_orgs, 'old CER': CER_orgs}\n",
    "whole_task_baseline = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1253a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63835226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f2fb72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jar_file = \"ocrevalUAtion-1.3.4-jar-with-dependencies.jar\"\n",
    "\n",
    "def evaluation(index, row):\n",
    "    ID = row['identifier']\n",
    "    page = 'None'\n",
    "    corrected_OCR = re.sub(' +', ' ', str(row['corrected document'].replace('.', '')))\n",
    "    gt_text = re.sub(' +', ' ', str(row['gt text'].replace('.', '')))\n",
    "    filename_ocr = f\"{ID}_{page}_OCR.txt\"\n",
    "    #file_ocr = open(os.path.join(save_path, filename),\"w+\", encoding=\"utf-8\")\n",
    "    file_ocr = open(filename_ocr,\"w+\", encoding=\"utf-8\")\n",
    "    file_ocr.write(corrected_OCR)\n",
    "    file_ocr.close()\n",
    "    \n",
    "    filename_gt = f\"{ID}_{page}_GT.txt\"\n",
    "    #file_gt = open(os.path.join(save_path, filename),\"w+\", encoding=\"utf-8\")\n",
    "    file_gt = open(filename_gt,\"w+\", encoding=\"utf-8\")\n",
    "    file_gt.write(gt_text)\n",
    "    file_gt.close()\n",
    "    \n",
    "    #output = ID + '_' + page + \".html\"\n",
    "    output = f\"{ID}_{page}.html\"\n",
    "    \n",
    "    #process = subprocess.call(\"/home/nvanthof/jdk-16.0.1/bin/java -cp \" + jar_file  + \" eu.digitisation.Main -gt \" + filename_gt + \" -ocr \"+ filename_ocr +\" -o \" + output + \"\")\n",
    "    #os.system(\"/home/nvanthof/jdk-16.0.1/bin/java -cp /home/nvanthof/ocrevalUAtion-1.3.4-jar-with-dependencies.jar eu.digitisation.Main -gt /home/nvanthof/ddd.010728187.mpeg21.a0005_None_GT.txt -ocr /home/nvanthof/ddd.010728187.mpeg21.a0005_None_OCR.txt  -o /home/nvanthof/OUTPUT2.html\")\n",
    "    command = f\"/home/nvanthof/jdk-16.0.1/bin/java -cp /home/nvanthof/ocrevalUAtion-1.3.4-jar-with-dependencies.jar eu.digitisation.Main -gt /home/nvanthof/{filename_gt} -ocr /home/nvanthof/{filename_ocr}  -o /home/nvanthof/{output}\"\n",
    "    #command = f\"/usr/bin/java -cp /home/nynkegpu/ocrevalUAtion-1.3.4-jar-with-dependencies.jar eu.digitisation.Main -gt /home/nynkegpu/{filename_gt} -ocr /home/nynkegpu/{filename_ocr}  -o /home/nynkegpu/{output}\"\n",
    "    os.system(command)\n",
    "    sleep(5)\n",
    "    \n",
    "    soup = BeautifulSoup(open(output, encoding='utf-8'))\n",
    "    table = soup.find(\"table\", attrs={'border': '1'})\n",
    "    # Split the filename, and extract the identifier and pagenr together as identifier \n",
    "    # Find the first table (this is the table in which the scores are stored)\n",
    "    # Find the tags in which 'CER', 'WER', and 'WER (order independent)' are stored and take the next tag to get the score \n",
    "    cer = table.find('td', text='CER')\n",
    "    cerScore = cer.findNext('td')\n",
    "    wer = table.find('td', text='WER')\n",
    "    werScore = wer.findNext('td')\n",
    "    werOI = table.find('td', text='WER (order independent)')\n",
    "    werOIScore = werOI.findNext('td')\n",
    "    \n",
    "    os.remove(filename_gt)\n",
    "    os.remove(filename_ocr)\n",
    "    os.remove(output)\n",
    "    return float(cerScore.text), float(werScore.text)   \n",
    "    \n",
    "    return cerScore.text, werScore.text\n",
    "\n",
    "#for index, row in whole_task_baseline.iterrows():\n",
    "#    if index%1000 == 0:\n",
    "#        print(index)\n",
    "#    whole_task_baseline.at[index, 'CER after correction'], whole_task_baseline.at[index, 'WER after correction'] = evaluation(index, row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2daf1711",
   "metadata": {},
   "outputs": [],
   "source": [
    "#whole_task_baseline['WER reduced'] = whole_task_baseline['old WER'] - whole_task_baseline['WER after correction']\n",
    "#whole_task_baseline['CER reduced'] = whole_task_baseline['old CER'] - whole_task_baseline['CER after correction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f75c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#whole_task_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd3c1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f3f7c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframes\n",
    "# detection dataframe\n",
    "baseline_detection.to_csv('detection_test_baseline_5K.csv')\n",
    "# correction dataframe\n",
    "baseline_correction.to_csv('correction_test_baseline_5K.csv')\n",
    "# whole task dataframe\n",
    "whole_task_baseline.to_csv('whole_task_test_baseline_5K.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8345bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393f55e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b1f0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0cfbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
