{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "80ad5fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle\n",
    "import ast\n",
    "import statistics as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f7bd4531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec model\n",
    "#w2v_model = ...\n",
    "# load BERT model\n",
    "#BERT_model = ...\n",
    "# load dataframe\n",
    "#df = ...\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "BERT_model = BertForMaskedLM.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "#w2v_model.intersect_word2vec_format(r\"combined-160.txt\", binary=False, lockf=1.0)\n",
    "# https://github.com/clips/dutchembeddings\n",
    "\n",
    "df = pd.read_csv('preprocessed_df100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dbe809e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvanthof/.local/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "def finetune_word2vec(train, window=5):\n",
    "    sentences = train.split('.')\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sentences = [tokenizer.tokenize(i) for i in sentences]\n",
    "    total_examples = len(sentences)\n",
    "    \n",
    "    model_w2v = Word2Vec(size=160, min_count=1, window=window)\n",
    "    model_w2v.build_vocab(sentences)\n",
    "    total_examples = model_w2v.corpus_count\n",
    "    model = KeyedVectors.load_word2vec_format(r\"combined-160.txt\", binary=False)\n",
    "    model_w2v.build_vocab([list(model.vocab.keys())], update=True)\n",
    "    model_w2v.intersect_word2vec_format(r\"combined-160.txt\", binary=False, lockf=1.0)\n",
    "    model_w2v.train(sentences, total_examples=total_examples, epochs=model_w2v.iter)\n",
    "    return model_w2v\n",
    "\n",
    "train = df['gt text'][0]\n",
    "word2vec_model = finetune_word2vec(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b09f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adc520c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "df.columns\n",
    "print(df.index.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b52a3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters for validation\n",
    "#topn_detection = 1000\n",
    "#topn_correction = 1000\n",
    "#window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8ef226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_merger(lists):\n",
    "    new_list = []\n",
    "    for elem in lists:\n",
    "        new_list = new_list + elem\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22d1e632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: ['engelant', 'ampc']\n",
      "ELEM: ['engelant', 'ampc']\n",
      "ELEM: ['dublin', 'den', '24', 'maert']\n",
      "ELEM: ['tegens', 'de', 'charters', 'van', 'dese', 'stadt']\n",
      "ELEM: []\n",
      "['engelant', 'ampc', 'dublin', 'den', '24', 'maert', 'tegens', 'de', 'charters', 'van', 'dese', 'stadt']\n"
     ]
    }
   ],
   "source": [
    "#lijst = [['aa','bb','cc'],['dd','ee','ff'],[7,8,9]]\n",
    "lijst = [['engelant', 'ampc'], ['dublin', 'den', '24', 'maert'], ['tegens', 'de', 'charters', 'van', 'dese', 'stadt'], []]\n",
    "lijst = list_merger(lijst)\n",
    "print(lijst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da419976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[['engelant', 'ampc'], ['dublin', 'den', '24', 'maert'], ['tegens', 'de', 'charters', 'van', 'dese', 'stadt'], []]\n",
      "engelant\n"
     ]
    }
   ],
   "source": [
    "string = \"[['engelant', 'ampc'], ['dublin', 'den', '24', 'maert'], ['tegens', 'de', 'charters', 'van', 'dese', 'stadt'], []]\"\n",
    "lijst = ast.literal_eval(string)\n",
    "print(type(lijst))\n",
    "print(lijst)\n",
    "print(lijst[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "37aa3ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skiplist (words that should not be corrected: names)\n",
    "with open(\"ocr_names.txt\", \"rb\") as fp:   # Unpickling\n",
    "    ocr_names = pickle.load(fp)\n",
    "\n",
    "ocr_names = []\n",
    "for name in ocr_names:\n",
    "    if len(name) >= 5:\n",
    "        ocr_names.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "186e8415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val\n",
      "[['Een', 'hoekenpan', 'of', 'hortweg', 'pan', 'is', 'een', 'hlatte', 'pan', 'met', 'een', 'hang', 'handvat'], ['\\nDe', 'pan', 'ontleent', 'zijn', 'naam', 'haan', 'het', 'heit', 'dat', 'in', \"ho'n\", 'pan', 'pannenkoeken', 'horden', 'gebakken'], ['', 'Ook', 'hnder', 'voedsel,', 'zoals', 'vlees,', 'word', 'in', 'een', 'koekenpan', 'gebraden']]\n",
      "['Een', 'hoekenpan', 'of', 'hortweg', 'pan', 'is', 'een', 'hlatte', 'pan', 'met']\n",
      "[[1000, 1000, 1000]]\n",
      "[[1000, 1000, 1000, 1000, 5]]\n",
      "errors: 1000\n",
      "non errors: 801\n",
      "right token in prediction: 1000\n"
     ]
    }
   ],
   "source": [
    "#detection and correction validation\n",
    "\n",
    "detection_df = pd.DataFrame()\n",
    "error_positions = [] # the position of a word when it is an error in predictions\n",
    "non_error_positions = [] # the position of a word when it is not an error in predictions\n",
    "right_token_positions = []\n",
    "\n",
    "def detection_and_correction_word2vec(row, w2v_model, ocr_names, window=5, topn_detection=1000):\n",
    "    if row['set'] != 'val':\n",
    "        return np.nan\n",
    "    else:\n",
    "        print('Val')\n",
    "        OCR_text = row['aligned_OCR_sentences']\n",
    "        GT_text = row['aligned_GT_sentences']\n",
    "        print(OCR_text)\n",
    "        OCR_text = ast.literal_eval(OCR_text)\n",
    "        GT_text = ast.literal_eval(GT_text)\n",
    "        OCR_text = list_merger(OCR_text)\n",
    "        GT_text = list_merger(GT_text)\n",
    "        OCR_text = OCR_text[:10]\n",
    "        print(OCR_text)\n",
    "        window_range = list(range(0,window))\n",
    "        window_range = np.array(window_range) - ((window - 1) / 2)\n",
    "        error_positions_doc = []\n",
    "        non_error_positions_doc = []\n",
    "        right_token_positions_doc = []\n",
    "        for i in range(len(OCR_text)):\n",
    "            if (OCR_text[i] in ocr_names) or (OCR_text[i].isalpha() == False) or (len(OCR_text[i]) <= 2):\n",
    "                continue\n",
    "            error = True\n",
    "            if OCR_text[i] == GT_text[i]:\n",
    "                error = False\n",
    "            context = []\n",
    "            for j in window_range:\n",
    "                if (i+j >= 0) and (i+j < len(OCR_text)) and j != 0:\n",
    "                    context.append(OCR_text[i+int(j)])\n",
    "                else:\n",
    "                    pass\n",
    "            predictions = []\n",
    "            # calculate positions detection task\n",
    "            for prediction in w2v_model.predict_output_word(context, topn=topn_detection):\n",
    "                predictions.append(prediction[0])\n",
    "            try:\n",
    "                position = predictions.index(OCR_text[i])\n",
    "            except ValueError:\n",
    "                position = topn_detection\n",
    "            if error == True:\n",
    "                error_positions_doc.append(position)\n",
    "            elif error == False:\n",
    "                non_error_positions_doc.append(position)\n",
    "            # calculate positions correction task\n",
    "            if error == True:\n",
    "                try:\n",
    "                    right_token_position = predictions.index(GT_text[i])\n",
    "                    right_token_positions_doc.append(right_token_position)\n",
    "                except ValueError:\n",
    "                    right_token_positions_doc.append(topn_detection)\n",
    "                right_token_positions.append(right_token_positions_doc)    \n",
    "            \n",
    "            \n",
    "    #return error_positions, non_error_positions\n",
    "    error_positions.append(error_positions_doc)\n",
    "    non_error_positions.append(non_error_positions_doc)\n",
    "    \n",
    "    \n",
    "    \n",
    "#for index, row in df.iterrows():\n",
    "#    detection_word2vec(row)\n",
    "# df.loc[70]\n",
    "fake_test_list_GT_aligned = \"\"\"Een koekenpan of kortweg pan is een platte pan met een lang handvat.\n",
    "De pan ontleent zijn naam aan het feit dat in zo'n pan pannenkoeken worden gebakken. Ook ander voedsel, zoals vlees, wordt in een koekenpan gebraden\"\"\"\n",
    "fake_test_list_OCR_aligned = \"\"\"Een hoekenpan of hortweg pan is een hlatte pan met een hang handvat.\n",
    "De pan ontleent zijn naam haan het heit dat in ho'n pan pannenkoeken horden gebakken. Ook hnder voedsel, zoals vlees, word in een koekenpan gebraden\"\"\"\n",
    "fake_test_list_GT_aligned = fake_test_list_GT_aligned.split('.')\n",
    "fake_test_list_OCR_aligned = fake_test_list_OCR_aligned.split('.')\n",
    "fake_test_list_GT_aligned = [x.split(' ') for x in fake_test_list_GT_aligned]\n",
    "fake_test_list_OCR_aligned = [x.split(' ') for x in fake_test_list_OCR_aligned]\n",
    "d = {'aligned_OCR_sentences': [str(fake_test_list_OCR_aligned)], 'aligned_GT_sentences': [str(fake_test_list_GT_aligned)], 'set': ['val']}\n",
    "df_probeer = pd.DataFrame(data=d)\n",
    "detection_and_correction_word2vec(df_probeer.loc[0], word2vec_model, ocr_names)\n",
    "print(error_positions)\n",
    "print(non_error_positions)\n",
    "error_positions = list_merger(error_positions)\n",
    "non_error_positions = list_merger(non_error_positions)\n",
    "right_token_positions = list_merger(right_token_positions)\n",
    "print('errors:', s.mean(error_positions))\n",
    "print('non errors:', s.mean(non_error_positions))\n",
    "print('right token in prediction:', s.mean(right_token_positions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcbdf909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[-2. -1.  0.  1.  2.]\n",
      "0\n",
      "Dit1\n",
      "context: ['is2', 'een3']\n",
      "1\n",
      "is2\n",
      "context: ['Dit1', 'een3', 'zin4']\n",
      "2\n",
      "een3\n",
      "context: ['Dit1', 'is2', 'zin4', 'die5']\n",
      "3\n",
      "zin4\n",
      "context: ['is2', 'een3', 'die5', 'lang6']\n",
      "4\n",
      "die5\n",
      "context: ['een3', 'zin4', 'lang6', 'genoeg7']\n",
      "5\n",
      "lang6\n",
      "context: ['zin4', 'die5', 'genoeg7', 'is8']\n",
      "6\n",
      "genoeg7\n",
      "context: ['die5', 'lang6', 'is8']\n",
      "7\n",
      "is8\n",
      "context: ['lang6', 'genoeg7']\n"
     ]
    }
   ],
   "source": [
    "text = \"Dit1 is2 een3 zin4 die5 lang6 genoeg7 is8\"\n",
    "text = text.split(' ')\n",
    "\n",
    "window = 5\n",
    "window_range = list(range(0,window))\n",
    "print(window_range)\n",
    "window_range = np.array(window_range) - ((window - 1) / 2)\n",
    "print(window_range)\n",
    "\n",
    "context = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    print(i)\n",
    "    print(text[i])\n",
    "    context = []\n",
    "    for j in window_range:\n",
    "        if (i+j >= 0) and (i+j < len(text)) and j != 0:\n",
    "            context.append(text[i+int(j)])\n",
    "        else:\n",
    "            pass\n",
    "    print('context:', context)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "04b469b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('den', 7.57105e-07),\n",
       " ('ende', 7.248402e-07),\n",
       " ('de', 7.188712e-07),\n",
       " ('noch', 7.1387456e-07),\n",
       " ('is', 7.0737497e-07),\n",
       " ('hebben', 7.070108e-07),\n",
       " ('vyand', 7.065082e-07),\n",
       " ('eenige', 7.0626095e-07),\n",
       " ('met', 7.061281e-07),\n",
       " ('wt', 7.042607e-07)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.predict_output_word(['Een', 'of', 'kortweg'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "86dbf89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('karton', 0.8278681039810181),\n",
       " ('inkt', 0.8026106357574463),\n",
       " ('rubberdoek', 0.7930652499198914),\n",
       " ('krantenpapier', 0.792934775352478),\n",
       " ('perkament', 0.7911171913146973),\n",
       " ('vloeipapier', 0.7774406671524048),\n",
       " ('vel', 0.7763333320617676),\n",
       " ('handgeschept', 0.771336555480957),\n",
       " ('potlood', 0.760221540927887),\n",
       " ('bedrukken', 0.7578753232955933)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar('papier')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
