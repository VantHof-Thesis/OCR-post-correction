{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5497b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvanthof/.local/lib/python3.6/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "# do not forget to set the parameters topn_detection, topn_correction, and method\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle\n",
    "import ast\n",
    "import statistics as s\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "import copy\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24846e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec model\n",
    "#w2v_model = ...\n",
    "# load BERT model\n",
    "#BERT_model = ...\n",
    "# load dataframe\n",
    "#df = ...\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "BERT_model = BertForMaskedLM.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "#w2v_model.intersect_word2vec_format(r\"combined-160.txt\", binary=False, lockf=1.0)\n",
    "# https://github.com/clips/dutchembeddings\n",
    "\n",
    "df = pd.read_csv('preprocessed_df100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aa14e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvanthof/.local/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "def finetune_word2vec(train, window=5):\n",
    "    sentences = train.split('.')\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sentences = [tokenizer.tokenize(i) for i in sentences]\n",
    "    total_examples = len(sentences)\n",
    "    \n",
    "    model_w2v = Word2Vec(size=160, min_count=1, window=window)\n",
    "    model_w2v.build_vocab(sentences)\n",
    "    total_examples = model_w2v.corpus_count\n",
    "    model = KeyedVectors.load_word2vec_format(r\"combined-160.txt\", binary=False)\n",
    "    model_w2v.build_vocab([list(model.vocab.keys())], update=True)\n",
    "    model_w2v.intersect_word2vec_format(r\"combined-160.txt\", binary=False, lockf=1.0)\n",
    "    model_w2v.train(sentences, total_examples=total_examples, epochs=model_w2v.iter)\n",
    "    return model_w2v\n",
    "\n",
    "train = df['gt text'][0]\n",
    "word2vec_model = finetune_word2vec(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3d1252d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'all_lists_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5b4a4444fb14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all_lists_tokens'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mall_lists_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'all_lists_tokens'"
     ]
    }
   ],
   "source": [
    "with open('all_lists_tokens', 'rb') as f:\n",
    "    all_lists_tokens = pickle.load(f)\n",
    "    \n",
    "all_lists_token = ast.literal_eval(all_lists_token)\n",
    "vocab_BERT, vocab_word2vec, hist_expressions, modern_vocab, dictionary = all_lists_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18402dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldef correct_sorted(candidates, sim_or_probs, LD): # sorts first by LD, then by similarity/probability\n",
    "    paired_sorted = sorted(zip(LD,sim_or_probs,candidates),key = lambda x: (x[0],x[1]), reverse=True)\n",
    "    LD,sim_or_probs,candidates = zip(*paired_sorted)\n",
    "    correction = candidates[0]\n",
    "    return correction\n",
    "    \n",
    "def correct_calculated(candidates, sim_or_probs, LD): # calculates a score from LD and normalised similarity/probability\n",
    "    inv_LD = 1 - LD\n",
    "    sim_or_probs = np.array(sim_or_probs)\n",
    "    sim_or_probs = np.interp(sim_or_probs, (sim_or_probs.min(), sim_or_probs.max()), (0, 1)).tolist()\n",
    "    score = sim_or_probs / inv_LD\n",
    "    zipped_pairs = zip(score.tolist(), candidates)\n",
    "    sorted_by_score = [x for _, x in sorted(zipped_pairs, reverse=True)]\n",
    "    correction = sorted_by_score[0]\n",
    "    return correction\n",
    "\n",
    "def remove_stopwords(candidates, cosine, LD):\n",
    "    #nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('dutch'))\n",
    "    candidates_nostopwords = []\n",
    "    cosine_nostopwords = []\n",
    "    LD_nostopwords = []\n",
    "    for i in range(len(candidates)):\n",
    "        if candidates[i] not in stop_words:\n",
    "            candidates_nostopwords.append(candidates[i])\n",
    "            cosine_nostopwords.append(cosine[i])\n",
    "            LD_nostopwords.append(LD[i])\n",
    "    LD_nostopwords = np.array(LD_nostopwords)\n",
    "    return candidates_nostopwords, cosine_nostopwords, LD_nostopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fe1f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of all FN, TN, FP, TPs detection:\n",
    "homonyms_detection_list_w2c = [[],[],[],[]]\n",
    "homonyms_detection_context_list_w2c = [[],[],[],[]]\n",
    "histexp_detection_list_w2c = [[],[],[],[]]\n",
    "histexp_detection_context_list_w2c = [[],[],[],[]]\n",
    "OOV_detection_list_w2c = [[],[],[],[]]\n",
    "OOV_detection_context_list_w2c = [[],[],[],[]]\n",
    "infreq_detection_list_w2c = [[],[],[],[]]\n",
    "infreq_detection_context_list_w2c = [[],[],[],[]]\n",
    "RWE_detection_list_w2c = [[],[],[],[]]\n",
    "RWE_detection_context_list_w2c = [[],[],[],[]]\n",
    "all_detection_list_w2c = [[],[],[],[]]\n",
    "\n",
    "# list of all right / wrong correction\n",
    "homonyms_correction_list_w2c = [[],[]]\n",
    "homonyms_correction_context_list_w2c = [[],[]]\n",
    "histexp_correction_list_w2c = [[],[]]\n",
    "histexp_correction_context_list_w2c = [[],[]]\n",
    "OOV_correction_list_w2c = [[],[]]\n",
    "OOV_correction_context_list_w2c = [[],[]]\n",
    "infreq_correction_list_w2c = [[],[]]\n",
    "infreq_correction_context_list_w2c = [[],[]]\n",
    "RWE_correction_list_w2c = [[],[]]\n",
    "RWE_correction_context_list_w2c = [[],[]]\n",
    "all_correction_list_w2c = [[],[]]\n",
    "\n",
    "#list of outputs corrected texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec73722",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_detection_nums_w2c = [0,0,0,0]\n",
    "\n",
    "def calculate_result(predicted_error, actual_error):\n",
    "    if actual_error == True:\n",
    "        if predicted_error == True: # TP\n",
    "            result = 'TP'\n",
    "        if predicted_error == False: # FN\n",
    "            result = 'FN'\n",
    "    if actual_error == False:\n",
    "        if predicted_error == True: # FP\n",
    "            result = 'FP'\n",
    "        if predicted_error == False: # TN\n",
    "            result = 'TN'\n",
    "    return result\n",
    "\n",
    "def special_tokens_detection_word(gt_word, detection_list_w2c, all_lists_tokens, result): \n",
    "    # check if word is homonym\n",
    "    if gt_word in all_lists_tokens[0]:\n",
    "        homonym = True\n",
    "    # check if word is OOV\n",
    "    \n",
    "    # check if word is historical expression\n",
    "    \n",
    "    # check if word is infrequent\n",
    "    \n",
    "    # check if word is RWE\n",
    "    \n",
    "    if result == 'TP': # actual error == True, predicted error == True\n",
    "        \n",
    "    elif result == 'FN': # actual error == True, predicted error == False\n",
    "        \n",
    "    elif result == 'FP': # actual error == False, predicted error == True\n",
    "         \n",
    "    elif result == 'TN': # actual error == False, predicted error == False\n",
    "     \n",
    "    \n",
    "def special_tokens_detection_context(predicted_error, word_GT, word_OCR, context_GT):\n",
    "    \n",
    "    \n",
    "def special_tokens_correction_word(predicted_error, word_GT, word_OCR):\n",
    "    \n",
    "    \n",
    "def special_tokens_correction_context(predicted_error, word_GT, word_OCR, context_GT):\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42930192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#detection test word2vec\n",
    "def detection_and_correction_word2vec(row, w2v_model, ocr_names, window=5, topn_detection=1000, topn_correction=1000, all_lists_token):\n",
    "    if row['set'] != 'test':\n",
    "        return np.nan\n",
    "    else:\n",
    "        print('detection')\n",
    "        identifier = row['identifier']\n",
    "        OCR_text = row['aligned_OCR_sentences']\n",
    "        GT_text = row['aligned_GT_sentences']\n",
    "        OCR_text = ast.literal_eval(OCR_text)\n",
    "        GT_text = ast.literal_eval(GT_text)\n",
    "        OCR_text = list_merger(OCR_text)\n",
    "        GT_text = list_merger(GT_text)\n",
    "        window_range = list(range(0,window))\n",
    "        window_range = np.array(window_range) - ((window - 1) / 2)\n",
    "\n",
    "        # keep track of performance\n",
    "        homonyms_detection_list_w2c = [0,0,0,0]\n",
    "        homonyms_detection_context_list_w2c = [0,0,0,0]\n",
    "        histexp_detection_list_w2c = [0,0,0,0]\n",
    "        histexp_detection_context_list_w2c = [0,0,0,0]\n",
    "        OOV_detection_list_w2c = [0,0,0,0]\n",
    "        OOV_detection_context_list_w2c = [0,0,0,0]\n",
    "        infreq_detection_list_w2c = [0,0,0,0]\n",
    "        infreq_detection_context_list_w2c = [0,0,0,0]\n",
    "        RWE_detection_list_w2c = [0,0,0,0]\n",
    "        RWE_detection_context_list_w2c = [0,0,0,0]\n",
    "        all_detection_list_w2c = [0,0,0,0]\n",
    "        \n",
    "        detection_list_w2c = [homonyms_detection_list_w2c, histexp_detection_list_w2c, OOV_detection_list_w2c, infreq_detection_list_w2c, RWE_detection_list_w2c, all_detection_list_w2c]\n",
    "        detection_list_context_w2c = [homonyms_detection_context_list_w2c, histexp_detection_context_list_w2c, OOV_detection_context_list_w2c, infreq_detection_context_list_w2c, RWE_detection_context_list_w2c]\n",
    "\n",
    "        \n",
    "        for i in range(len(OCR_text)):\n",
    "            try:\n",
    "                if (OCR_text[i] in ocr_names) or (OCR_text[i].isalpha() == False) or (len(OCR_text[i]) <= 2):\n",
    "                    continue\n",
    "                context = []\n",
    "                GT_context = []\n",
    "                for j in window_range:\n",
    "                    if (i+j >= 0) and (i+j < len(OCR_text)) and j != 0:\n",
    "                        context.append(OCR_text[i+int(j)])\n",
    "                        GT_context.append(OCR_text[i+int(j)])\n",
    "                    else:\n",
    "                        pass\n",
    "                candidates = []\n",
    "                cosines = []\n",
    "                # calculate positions detection task\n",
    "                for prediction in w2v_model.predict_output_word(context, topn=topn_detection):\n",
    "                    candidates.append(prediction[0])\n",
    "                    cosines.append(prediction[1]) \n",
    "                # remove punctuation except for hyphen from candidates\n",
    "                candidates = [re.sub(r'[^\\w\\d\\s\\-]+', '', x) for x in candidates]\n",
    "                # determine if token is predicted error or not\n",
    "                if OCR_text[i] in candidates:\n",
    "                    predicted_error = False\n",
    "                elif OCR_text[i] not in candidates:\n",
    "                    predicted_error = True\n",
    "                # determine if token is actual error or not\n",
    "                if OCR_text[i] =! GT_text[i]:\n",
    "                    actual_error = True\n",
    "                elif OCR_text[i] == GT_text[i]:\n",
    "                    actual_error = False\n",
    "                result = calculate_result(predicted_error, actual_error)\n",
    "                # evaluate detection\n",
    "                detection_list_w2c = special_tokens_detection_word(detection_list_w2c, all_lists_token, result)\n",
    "                \n",
    "                \n",
    "                \n",
    "                if error == True:\n",
    "                    \n",
    "                    # try two correction methods\n",
    "                    # first calculate the normalized LDs:\n",
    "                    LD = np.array([fuzz.ratio(OCR_text[i], word)/100 for word in candidates])\n",
    "                    # try sorting method\n",
    "                    correction = correct_sorted(candidates, cosines, LD)\n",
    "                    if correction == GT_text[i]:\n",
    "                        rights_correct_sorted += 1\n",
    "                    elif correction != GT_text[i]:\n",
    "                        wrongs_correct_sorted += 1\n",
    "                    print('OCR:', OCR_text[i])\n",
    "                    print('correction sorted:', correction)\n",
    "                    # try again the sorting methods, but without stopwords\n",
    "                    candidates_nostopwords, cosine_nostopwords, LD_nostopwords = remove_stopwords(candidates, cosines, LD)\n",
    "                    correction = correct_sorted(candidates_nostopwords, cosine_nostopwords, LD_nostopwords)\n",
    "                    if correction == GT_text[i]:\n",
    "                        rights_correct_sorted_nosw += 1\n",
    "                    elif correction != GT_text[i]:\n",
    "                        wrongs_correct_sorted_nosw += 1\n",
    "                    # try score calculation method\n",
    "                    correction = correct_calculated(candidates, cosines, LD)\n",
    "                    if correction == GT_text[i]:\n",
    "                        rights_correct_calculated += 1\n",
    "                    elif correction != GT_text[i]:\n",
    "                        wrongs_correct_calculated += 1\n",
    "                    print('OCR:', OCR_text[i])\n",
    "                    print('correction score:', correction)\n",
    "                elif error == False:\n",
    "                    # where the non error is in the detection list\n",
    "                    non_error_positions_doc.append(position)\n",
    "                # calculate positions correction task\n",
    "                if error == True:\n",
    "                    try:\n",
    "                        right_token_position = candidates.index(GT_text[i])\n",
    "                        right_token_positions_doc.append(right_token_position)\n",
    "                    except ValueError:\n",
    "                        right_token_positions_doc.append(topn_detection)\n",
    "                    right_token_positions.append(right_token_positions_doc)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "    #return error_positions, non_error_positions\n",
    "    error_positions.append(error_positions_doc)\n",
    "    non_error_positions.append(non_error_positions_doc)\n",
    "    right_token_positions.append(right_token_positions_doc)\n",
    "    \n",
    "    # add performance to the list\n",
    "    rights_correct_sorted_list.append(rights_correct_sorted)\n",
    "    wrongs_correct_sorted_list.append(wrongs_correct_sorted)\n",
    "    rights_correct_sorted_nosw_list.append(rights_correct_sorted_nosw)\n",
    "    wrongs_correct_sorted_nosw_list.append(wrongs_correct_sorted_nosw)\n",
    "    rights_correct_calculated_list.append(rights_correct_calculated)\n",
    "    wrongs_correct_calculated_list.append(wrongs_correct_calculated)\n",
    "    \n",
    "    return identifier, error_positions, non_error_positions, right_token_positions, rights_correct_sorted_list, wrongs_correct_sorted_list, rights_correct_sorted_nosw_list, wrongs_correct_sorted_nosw_list, rights_correct_calculated_list, wrongs_correct_calculated_list   \n",
    "    \n",
    "#for index, row in df.iterrows():\n",
    "#    detection_word2vec(row)\n",
    "# df.loc[70]\n",
    "fake_test_list_GT_aligned = \"\"\"Een koekenpan of kortweg pan is een platte pan met een lang handvat.\n",
    "De pan ontleent zijn naam aan het feit dat in zo'n pan pannenkoeken worden gebakken. Ook ander voedsel, zoals vlees, wordt in een koekenpan gebraden\"\"\"\n",
    "fake_test_list_OCR_aligned = \"\"\"Een hoekenpan of kortweg pan is een platte pan met een hang handvat.\n",
    "De pan ontleent zijn naam haan het feit dat in zo'n pan pannenkoeken horden gebakken. Ook ander voedsel, zoals vlees, word in een hoekenpan gebraden\"\"\"\n",
    "fake_test_list_GT_aligned = fake_test_list_GT_aligned.split('.')\n",
    "fake_test_list_OCR_aligned = fake_test_list_OCR_aligned.split('.')\n",
    "fake_test_list_GT_aligned = [x.split(' ') for x in fake_test_list_GT_aligned]\n",
    "fake_test_list_OCR_aligned = [x.split(' ') for x in fake_test_list_OCR_aligned]\n",
    "d = {'identifier': ['111'], 'aligned_OCR_sentences': [str(fake_test_list_OCR_aligned)], 'aligned_GT_sentences': [str(fake_test_list_GT_aligned)], 'set': ['val']}\n",
    "df_probeer = pd.DataFrame(data=d)\n",
    "\n",
    "\n",
    "def perform_task(row, w2v_model, ocr_names, window=5):\n",
    "    if row['set'] != 'test':\n",
    "        return np.nan\n",
    "    else:\n",
    "        print('detection')\n",
    "        identifier = row['identifier']\n",
    "        OCR_text = row['aligned_OCR_sentences']\n",
    "        GT_text = row['aligned_GT_sentences']\n",
    "        OCR_text = ast.literal_eval(OCR_text)\n",
    "        GT_text = ast.literal_eval(GT_text)\n",
    "        OCR_text = list_merger(OCR_text)\n",
    "        GT_text = list_merger(GT_text)\n",
    "        window_range = list(range(0,window))\n",
    "        window_range = np.array(window_range) - ((window - 1) / 2)\n",
    "\n",
    "        # keep track of performance\n",
    "        homonyms_detection_list_w2c = [0,0,0,0]\n",
    "        homonyms_detection_context_list_w2c = [0,0,0,0]\n",
    "        histexp_detection_list_w2c = [0,0,0,0]\n",
    "        histexp_detection_context_list_w2c = [0,0,0,0]\n",
    "        OOV_detection_list_w2c = [0,0,0,0]\n",
    "        OOV_detection_context_list_w2c = [0,0,0,0]\n",
    "        infreq_detection_list_w2c = [0,0,0,0]\n",
    "        infreq_detection_context_list_w2c = [0,0,0,0]\n",
    "        RWE_detection_list_w2c = [0,0,0,0]\n",
    "        RWE_detection_context_list_w2c = [0,0,0,0]\n",
    "        all_detection_list_w2c = [0,0,0,0]\n",
    "        all_detection_context_list_w2c = [0,0,0,0]\n",
    "        \n",
    "        for i in range(len(OCR_text)):\n",
    "            try:\n",
    "                if (OCR_text[i] in ocr_names) or (OCR_text[i].isalpha() == False) or (len(OCR_text[i]) <= 2):\n",
    "                    continue\n",
    "                context = []\n",
    "                GT_context = []\n",
    "                for j in window_range:\n",
    "                    if (i+j >= 0) and (i+j < len(OCR_text)) and j != 0:\n",
    "                        context.append(OCR_text[i+int(j)])\n",
    "                        GT_context.append(OCR_text[i+int(j)])\n",
    "                    else:\n",
    "                        pass\n",
    "            \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
