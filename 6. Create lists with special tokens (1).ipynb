{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e027cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle\n",
    "import ast\n",
    "import statistics as s\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "import copy\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97a1f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec model\n",
    "#w2v_model = ...\n",
    "# load BERT model\n",
    "#BERT_model = ...\n",
    "# load dataframe\n",
    "#df = ...\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "BERT_model = BertForMaskedLM.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "#w2v_model.intersect_word2vec_format(r\"combined-160.txt\", binary=False, lockf=1.0)\n",
    "# https://github.com/clips/dutchembeddings\n",
    "\n",
    "df = pd.read_csv('preprocessed_df100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a3d2f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d18bc520",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gt for training'] = df['gt sentences matched']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e863c21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30073\n",
      "1442950\n",
      "67231\n",
      "1480108\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary lists\n",
    "vocab_BERT = list(tokenizer.vocab.keys())\n",
    "print(len(vocab_BERT))\n",
    "train = df[df['set']=='train']\n",
    "#df['gt for training'] = df['gt text']\n",
    "train = '.'.join(list(train['gt for training']))\n",
    "train = (train.replace('.', '')).split(' ')\n",
    "vocab_BERT = vocab_BERT + train\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(r\"combined-160.txt\", binary=False)\n",
    "vocab_word2vec = list(model.vocab.keys())\n",
    "print(len(vocab_word2vec))\n",
    "vocab_word2vec = vocab_word2vec + train\n",
    "print(len(vocab_BERT))\n",
    "print(len(vocab_word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f170c6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bloedspuwingen', 'tón-bèèzen', 'menschengemeenschap', 'ieiunium', 'jong-getal', 'Nassouwe', 'polslag', 'mutamur', \"rey'\", 'vogelpruimbast', 'Tractrat', 'synaesthetische', 'sausbaars', 'kievitseijeren', 'imbijte', 'impossibele', 'gerepouseert', 'chocolaadje', 'vlierkappers', 'zielsverheffende', 'uitstrominge', 'reuzentoppen', 'Arminiaanschen', 'natuureigenste', 'vlotafstand', 'aptae', 'latynsche', 'anaemische', 'avanturen', 'lauas', 'toovernij', 'beweluen', 'Geschreven', 'Gon', 'wijdloopige', 'racaille', 'duynbosjes', 'kacheldroogen', 'vischkotter', 'hatelike', 'Tuigt', 'voorpeynst', 'vloeiijzerwerk', 'huusmul', 'bouteinden', 'knipvoeg', 'geitekaes', 'verwoesteden', 'rosenknopje', 'vier-boete', 'ducatontjes', 'végétale', 'wippelpaard', 'sonduloet', 'lozeleur', 'volmachtzbryeve', 'omwingerdt', 'wapenscouwinge', 'U.R.S.S', 'bloss', 'weynichken', 'perceptieregel', 'dibberij', 'wegwaaijing', 'Braunbuch', 'midlantse', 'letterdienders', \"gehemelt'\", 'roedraegher', 'kaasverw', 'geltgierig', 'ijemandt', 'weerkruipertje', 'bezorghsaem', 'helaaci', 'wazeghelt', 'toege-eygent', 'nersticheit', 'amatrice', 'praktischheid', 'vorstelicke', 'putwaterspomp', 'Vijfhoek', 'peerdefeeste', 'avendt', 'forschig', 'uitschauwen', 'streckten', 'Klagers', 'daghelijcks', 'verrichters', 'flinte', 'onbelezen', 'mucidus', 'Galdrade', 'trabi', 'eydsel', 'pothuys', 'Papilionibus', 'justiesie']\n",
      "656457\n",
      "413284\n"
     ]
    }
   ],
   "source": [
    "# create list of historical expressions\n",
    "with open('dictionaryBasedDutchDictionary_1.0.type_frequency.txt', 'r', encoding=\"utf8\") as f: # part 1 historical expressions\n",
    "    words = f.readlines()\n",
    "hist_words1 = [word[0 : word.index(\"\\t\")] for word in words]\n",
    "#print(woorden)\n",
    "\n",
    "with open('dutchCorpusBasedDictionary_1.1.tf.txt', 'r', encoding=\"utf8\") as f: # part 2 historical expressions\n",
    "    words = f.readlines()\n",
    "hist_words2 = [word[0 : word.index(\"\\t\")] for word in words]\n",
    "#print(woorden)\n",
    "\n",
    "expressions = list(set(hist_words1).union(set(hist_words2))) # all historical expressions\n",
    "\n",
    "with open('wordlist.txt', 'r', encoding=\"utf8\") as f: # modern_vocab\n",
    "    words = f.read().split('\\n')\n",
    "    #words3 = [word[0 : word.index(\"\\n\")] for word in words]\n",
    "    modern_vocab = words\n",
    "    \n",
    "# historical expressions is everything in hist_expression that is not in modern_vocab\n",
    "hist_expressions = list(set(expressions).difference(set(modern_vocab)))\n",
    "print(hist_expressions[:100])\n",
    "print(len(hist_expressions))\n",
    "print(len(modern_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f56089e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "219be22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['horen', 'vuil', 'vizier', 'militant', 'naturel', 'primetime', 'koperdraad', 'tel', 'zondagmiddag', 'plastiek']\n"
     ]
    }
   ],
   "source": [
    "# create list with homonyms\n",
    "with open('homoniemen.txt', 'r', encoding=\"utf8\") as f: # part 1 historical expressions\n",
    "    homonyms = f.readlines()\n",
    "homonyms = [word[0 : word.index(\":\")] for word in homonyms]\n",
    "print(homonyms[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3999dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list with infrequent expressions\n",
    "# = domain expressions etc. Not in original vocab, but a few times in the new training set.\n",
    "model_vocabs = list(tokenizer.vocab.keys()) + list(model.vocab.keys())\n",
    "# find infrequent expressions\n",
    "# merge all GT text to list of words\n",
    "# count unique words\n",
    "gt_texts = df['gt for training'].to_list()\n",
    "gt_texts = '.'.join(gt_texts)\n",
    "\n",
    "import re\n",
    "#regex = re.compile('[^a-zA-Z]')\n",
    "#gt_texts = regex.sub('', gt_texts)\n",
    "gt_texts = re.sub(r'[^A-Za-z ]+', '', gt_texts).lower()\n",
    "gt_texts = gt_texts.split(' ')\n",
    "\n",
    "#print(gt_texts)\n",
    "from collections import Counter\n",
    "occurrences = Counter(word for word in gt_texts)\n",
    "occurrences_counts = Counter(occ for occ in occurrences.values())\n",
    "infrequent_expressions = [key for key , value in occurrences.items() if value == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b32a5d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(tokens_list):\n",
    "    tokens_list = [re.sub(r'[^\\w\\d\\s\\-]+', '', x) for x in tokens_list]\n",
    "    tokens_list = [x.lower() for x in tokens_list]\n",
    "    return tokens_list\n",
    "\n",
    "# make sure all lists are in lowercase and without punctuation except for hyphens\n",
    "# homonyms\n",
    "homonyms = process(homonyms)\n",
    "# BERTs vocab:\n",
    "vocab_BERT = process(vocab_BERT)\n",
    "# word2vecs vocab:\n",
    "vocab_word2vec = process(vocab_word2vec)\n",
    "# historical expressions\n",
    "hist_expressions = process(hist_expressions)\n",
    "# the modern vocabulary\n",
    "modern_vocab = process(modern_vocab)\n",
    "# infrequent expressions\n",
    "infrequent_expressions = process(infrequent_expressions)\n",
    "# the dictionary for the baseline model\n",
    "dictionary = process(modern_vocab + hist_expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00b192cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save al lists and later unpack:\n",
    "all_lists_tokens = [homonyms, vocab_BERT, vocab_word2vec, hist_expressions, modern_vocab, infrequent_expressions, dictionary]\n",
    "\n",
    "with open('all_lists_tokens.txt', 'wb') as f:\n",
    "    pickle.dump(all_lists_tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a424b6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
