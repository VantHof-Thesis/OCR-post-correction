{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "0bf8f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle\n",
    "import ast\n",
    "import statistics as s\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "import copy\n",
    "rom nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "be68590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec model\n",
    "#w2v_model = ...\n",
    "# load BERT model\n",
    "#BERT_model = ...\n",
    "# load dataframe\n",
    "#df = ...\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "BERT_model = BertForMaskedLM.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "#w2v_model.intersect_word2vec_format(r\"combined-160.txt\", binary=False, lockf=1.0)\n",
    "# https://github.com/clips/dutchembeddings\n",
    "\n",
    "df = pd.read_csv('preprocessed_df100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "df980f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'identifier', 'gt text', 'ocr text', 'CER', 'WER',\n",
       "       'WER (order independent)', 'dictionary lookup gt',\n",
       "       'dictionary lookup ocr', 'jaccard_coefficient',\n",
       "       'levenshtein_dist_normalized', 'source', 'word count gt',\n",
       "       'word count ocr', 'year', 'century', 'old index', 'gt text org',\n",
       "       'ocr text org', 'set', 'gt sentences matched', 'ocr sentences matched',\n",
       "       'CER matched sentences', 'WER matched sentences',\n",
       "       'avg sentence length gt (fuzzy matched)_x',\n",
       "       'avg sentence length ocr (fuzzy matched)_x',\n",
       "       'max sentence length gt (fuzzy matched)_x',\n",
       "       'max sentence length ocr (fuzzy matched)_x',\n",
       "       'sentences gt (fuzzy matched)_x', 'sentences ocr (fuzzy matched)_x',\n",
       "       'word count gt (fuzzy matched)_x', 'word count ocr (fuzzy matched)_x',\n",
       "       'longest_streak', 'avg_longest_streaks', 'avg_total_missing_words',\n",
       "       'avg_perc_missing_words ', 'gt sentences matchated',\n",
       "       'avg sentence length gt (fuzzy matched)_y',\n",
       "       'avg sentence length ocr (fuzzy matched)_y',\n",
       "       'max sentence length gt (fuzzy matched)_y',\n",
       "       'max sentence length ocr (fuzzy matched)_y',\n",
       "       'sentences gt (fuzzy matched)_y', 'sentences ocr (fuzzy matched)_y',\n",
       "       'word count gt (fuzzy matched)_y', 'word count ocr (fuzzy matched)_y',\n",
       "       'aligned_GT_sentences', 'aligned_OCR_sentences', 'good_alignments',\n",
       "       'bad_alignments'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2dda19ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L']\n",
      "False\n",
      "False\n",
      "30073\n",
      "1442950\n",
      "64997\n",
      "1477874\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary lists\n",
    "vocab_BERT = list(tokenizer.vocab.keys())\n",
    "print(len(vocab_BERT))\n",
    "train = df[df['set']=='train']\n",
    "#df['gt for training'] = df['gt text']\n",
    "train = '.'.join(list(train['gt for training']))\n",
    "train = (train.replace('.', '')).split(' ')\n",
    "vocab_BERT = vocab_BERT + train\n",
    "vocab_word2vec = list(model.vocab.keys())\n",
    "print(len(vocab_word2vec))\n",
    "vocab_word2vec = vocab_word2vec + train\n",
    "print(len(vocab_BERT))\n",
    "print(len(vocab_word2vec))\n",
    "\n",
    "# create list of historical expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e4498252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvanthof/.local/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "def finetune_word2vec(train, window=5):\n",
    "    sentences = train.split('.')\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sentences = [tokenizer.tokenize(i) for i in sentences]\n",
    "    total_examples = len(sentences)\n",
    "    \n",
    "    model_w2v = Word2Vec(size=160, min_count=1, window=window)\n",
    "    model_w2v.build_vocab(sentences)\n",
    "    total_examples = model_w2v.corpus_count\n",
    "    model = KeyedVectors.load_word2vec_format(r\"combined-160.txt\", binary=False)\n",
    "    model_w2v.build_vocab([list(model.vocab.keys())], update=True)\n",
    "    model_w2v.intersect_word2vec_format(r\"combined-160.txt\", binary=False, lockf=1.0)\n",
    "    model_w2v.train(sentences, total_examples=total_examples, epochs=model_w2v.iter)\n",
    "    return model_w2v\n",
    "\n",
    "train = df['gt text'][0]\n",
    "word2vec_model = finetune_word2vec(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640dd677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "813710d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "df.columns\n",
    "print(df.index.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0fe92d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters for validation\n",
    "#topn_detection = 1000\n",
    "#topn_correction = 1000\n",
    "#window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "3716f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_merger(lists):\n",
    "    normal_list = False\n",
    "    for elem in lists:\n",
    "        if type(elem) != list:\n",
    "            normal_list = True\n",
    "    if normal_list == True:\n",
    "        return lists\n",
    "    else:\n",
    "        new_list = []\n",
    "        for elem in lists:\n",
    "            new_list = new_list + elem\n",
    "        return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc603209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: ['engelant', 'ampc']\n",
      "ELEM: ['engelant', 'ampc']\n",
      "ELEM: ['dublin', 'den', '24', 'maert']\n",
      "ELEM: ['tegens', 'de', 'charters', 'van', 'dese', 'stadt']\n",
      "ELEM: []\n",
      "['engelant', 'ampc', 'dublin', 'den', '24', 'maert', 'tegens', 'de', 'charters', 'van', 'dese', 'stadt']\n"
     ]
    }
   ],
   "source": [
    "#lijst = [['aa','bb','cc'],['dd','ee','ff'],[7,8,9]]\n",
    "lijst = [['engelant', 'ampc'], ['dublin', 'den', '24', 'maert'], ['tegens', 'de', 'charters', 'van', 'dese', 'stadt'], []]\n",
    "lijst = list_merger(lijst)\n",
    "print(lijst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03a277ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[['engelant', 'ampc'], ['dublin', 'den', '24', 'maert'], ['tegens', 'de', 'charters', 'van', 'dese', 'stadt'], []]\n",
      "engelant\n"
     ]
    }
   ],
   "source": [
    "string = \"[['engelant', 'ampc'], ['dublin', 'den', '24', 'maert'], ['tegens', 'de', 'charters', 'van', 'dese', 'stadt'], []]\"\n",
    "lijst = ast.literal_eval(string)\n",
    "print(type(lijst))\n",
    "print(lijst)\n",
    "print(lijst[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "851f15ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skiplist (words that should not be corrected: names)\n",
    "with open(\"ocr_names.txt\", \"rb\") as fp:   # Unpickling\n",
    "    ocr_names = pickle.load(fp)\n",
    "\n",
    "ocr_names = []\n",
    "for name in ocr_names:\n",
    "    if len(name) >= 5:\n",
    "        ocr_names.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "91621947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sorted(candidates, sim_or_probs, LD): # sorts first by LD, then by similarity/probability\n",
    "    paired_sorted = sorted(zip(LD,sim_or_probs,candidates),key = lambda x: (x[0],x[1]), reverse=True)\n",
    "    LD,sim_or_probs,candidates = zip(*paired_sorted)\n",
    "    correction = candidates[0]\n",
    "    return correction\n",
    "    \n",
    "def correct_calculated(candidates, sim_or_probs, LD): # calculates a score from LD and normalised similarity/probability\n",
    "    inv_LD = 1 - LD\n",
    "    sim_or_probs = np.array(sim_or_probs)\n",
    "    sim_or_probs = np.interp(sim_or_probs, (sim_or_probs.min(), sim_or_probs.max()), (0, 1)).tolist()\n",
    "    score = sim_or_probs / inv_LD\n",
    "    zipped_pairs = zip(score.tolist(), candidates)\n",
    "    sorted_by_score = [x for _, x in sorted(zipped_pairs, reverse=True)]\n",
    "    correction = sorted_by_score[0]\n",
    "    return correction\n",
    "\n",
    "def remove_stopwords(candidates, cosine, LD):\n",
    "    #nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('dutch'))\n",
    "    candidates_nostopwords = []\n",
    "    cosine_nostopwords = []\n",
    "    LD_nostopwords = []\n",
    "    for i in range(len(candidates)):\n",
    "        if candidates[i] not in stop_words:\n",
    "            candidates_nostopwords.append(candidates[i])\n",
    "            cosine_nostopwords.append(cosine[i])\n",
    "            LD_nostopwords.append(LD[i])\n",
    "    LD_nostopwords = np.array(LD_nostopwords)\n",
    "    return candidates_nostopwords, cosine_nostopwords, LD_nostopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d46e013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#detection and correction validation word2vec\n",
    "error_positions = [] # the position of a word when it is an error in predictions\n",
    "non_error_positions = [] # the position of a word when it is not an error in predictions\n",
    "right_token_positions = []\n",
    "\n",
    "rights_correct_sorted_list = []\n",
    "wrongs_correct_sorted_list = []\n",
    "rights_correct_sorted_nosw_list = []\n",
    "wrongs_correct_sorted_nosw_list = []\n",
    "rights_correct_calculated_list = []\n",
    "wrongs_correct_calculated_list = []\n",
    "\n",
    "def detection_and_correction_word2vec(row, w2v_model, ocr_names, window=5, topn_detection=1000):\n",
    "    if row['set'] != 'val':\n",
    "        return np.nan\n",
    "    else:\n",
    "        print('start')\n",
    "        identifier = row['identifier']\n",
    "        OCR_text = row['aligned_OCR_sentences']\n",
    "        GT_text = row['aligned_GT_sentences']\n",
    "        OCR_text = ast.literal_eval(OCR_text)\n",
    "        GT_text = ast.literal_eval(GT_text)\n",
    "        OCR_text = list_merger(OCR_text)\n",
    "        GT_text = list_merger(GT_text)\n",
    "        window_range = list(range(0,window))\n",
    "        window_range = np.array(window_range) - ((window - 1) / 2)\n",
    "        \n",
    "        # keep track of positions in candidates\n",
    "        error_positions_doc = []\n",
    "        non_error_positions_doc = []\n",
    "        right_token_positions_doc = []\n",
    "        \n",
    "        # keep track of performance\n",
    "        rights_correct_sorted = 0\n",
    "        wrongs_correct_sorted = 0\n",
    "        rights_correct_sorted_nosw = 0\n",
    "        wrongs_correct_sorted_nosw = 0\n",
    "        rights_correct_calculated = 0\n",
    "        wrongs_correct_calculated = 0\n",
    "        \n",
    "        for i in range(len(OCR_text)):\n",
    "            try:\n",
    "                if (OCR_text[i] in ocr_names) or (OCR_text[i].isalpha() == False) or (len(OCR_text[i]) <= 2):\n",
    "                    continue\n",
    "                error = True\n",
    "                if OCR_text[i] == GT_text[i]:\n",
    "                    error = False\n",
    "                context = []\n",
    "                for j in window_range:\n",
    "                    if (i+j >= 0) and (i+j < len(OCR_text)) and j != 0:\n",
    "                        context.append(OCR_text[i+int(j)])\n",
    "                    else:\n",
    "                        pass\n",
    "                candidates = []\n",
    "                cosines = []\n",
    "                # calculate positions detection task\n",
    "                for prediction in w2v_model.predict_output_word(context, topn=topn_detection):\n",
    "                    candidates.append(prediction[0])\n",
    "                    cosines.append(prediction[1]) \n",
    "                # remove punctuation except for hyphen from candidates\n",
    "                candidates = [re.sub(r'[^\\w\\d\\s\\-]+', '', x) for x in candidates]\n",
    "                try:\n",
    "                    position = candidates.index(OCR_text[i])\n",
    "                except ValueError:\n",
    "                    position = topn_detection\n",
    "                if error == True:\n",
    "                    # where the error is in the detection list\n",
    "                    error_positions_doc.append(position)\n",
    "                    # find where the right word is in the candidates list (same as detection list)\n",
    "                    try: \n",
    "                        position_right_token = candidates.index(GT_text[i])\n",
    "                    except ValueError: \n",
    "                        position_right_token = topn_detection\n",
    "                    print('position_right_token:', position_right_token)\n",
    "                    right_token_positions_doc.append(position_right_token)\n",
    "                    # try two correction methods\n",
    "                    # first calculate the normalized LDs:\n",
    "                    LD = np.array([fuzz.ratio(OCR_text[i], word)/100 for word in candidates])\n",
    "                    # try sorting method\n",
    "                    correction = correct_sorted(candidates, cosines, LD)\n",
    "                    if correction == GT_text[i]:\n",
    "                        rights_correct_sorted += 1\n",
    "                    elif correction != GT_text[i]:\n",
    "                        wrongs_correct_sorted += 1\n",
    "                    print('OCR:', OCR_text[i])\n",
    "                    print('correction sorted:', correction)\n",
    "                    # try again the sorting methods, but without stopwords\n",
    "                    candidates_nostopwords, cosine_nostopwords, LD_nostopwords = remove_stopwords(candidates, cosines, LD)\n",
    "                    correction = correct_sorted(candidates_nostopwords, cosine_nostopwords, LD_nostopwords)\n",
    "                    if correction == GT_text[i]:\n",
    "                        rights_correct_sorted_nosw += 1\n",
    "                    elif correction != GT_text[i]:\n",
    "                        wrongs_correct_sorted_nosw += 1\n",
    "                    # try score calculation method\n",
    "                    correction = correct_calculated(candidates, cosines, LD)\n",
    "                    if correction == GT_text[i]:\n",
    "                        rights_correct_calculated += 1\n",
    "                    elif correction != GT_text[i]:\n",
    "                        wrongs_correct_calculated += 1\n",
    "                    print('OCR:', OCR_text[i])\n",
    "                    print('correction score:', correction)\n",
    "                elif error == False:\n",
    "                    # where the non error is in the detection list\n",
    "                    non_error_positions_doc.append(position)\n",
    "                # calculate positions correction task\n",
    "                if error == True:\n",
    "                    try:\n",
    "                        right_token_position = candidates.index(GT_text[i])\n",
    "                        right_token_positions_doc.append(right_token_position)\n",
    "                    except ValueError:\n",
    "                        right_token_positions_doc.append(topn_detection)\n",
    "                    right_token_positions.append(right_token_positions_doc)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "    #return error_positions, non_error_positions\n",
    "    error_positions.append(error_positions_doc)\n",
    "    non_error_positions.append(non_error_positions_doc)\n",
    "    right_token_positions.append(right_token_positions_doc)\n",
    "    \n",
    "    # add performance to the list\n",
    "    rights_correct_sorted_list.append(rights_correct_sorted)\n",
    "    wrongs_correct_sorted_list.append(wrongs_correct_sorted)\n",
    "    rights_correct_sorted_nosw_list.append(rights_correct_sorted_nosw)\n",
    "    wrongs_correct_sorted_nosw_list.append(wrongs_correct_sorted_nosw)\n",
    "    rights_correct_calculated_list.append(rights_correct_calculated)\n",
    "    wrongs_correct_calculated_list.append(wrongs_correct_calculated)\n",
    "    \n",
    "    return identifier, error_positions, non_error_positions, right_token_positions_doc, rights_correct_sorted_list, wrongs_correct_sorted_list, rights_correct_sorted_nosw_list, wrongs_correct_sorted_nosw_list, rights_correct_calculated_list, wrongs_correct_calculated_list   \n",
    "    \n",
    "#for index, row in df.iterrows():\n",
    "#    detection_word2vec(row)\n",
    "# df.loc[70]\n",
    "fake_test_list_GT_aligned = \"\"\"Een koekenpan of kortweg pan is een platte pan met een lang handvat.\n",
    "De pan ontleent zijn naam aan het feit dat in zo'n pan pannenkoeken worden gebakken. Ook ander voedsel, zoals vlees, wordt in een koekenpan gebraden\"\"\"\n",
    "fake_test_list_OCR_aligned = \"\"\"Een hoekenpan of kortweg pan is een platte pan met een hang handvat.\n",
    "De pan ontleent zijn naam haan het feit dat in zo'n pan pannenkoeken horden gebakken. Ook ander voedsel, zoals vlees, word in een hoekenpan gebraden\"\"\"\n",
    "fake_test_list_GT_aligned = fake_test_list_GT_aligned.split('.')\n",
    "fake_test_list_OCR_aligned = fake_test_list_OCR_aligned.split('.')\n",
    "fake_test_list_GT_aligned = [x.split(' ') for x in fake_test_list_GT_aligned]\n",
    "fake_test_list_OCR_aligned = [x.split(' ') for x in fake_test_list_OCR_aligned]\n",
    "d = {'identifier': ['111'], 'aligned_OCR_sentences': [str(fake_test_list_OCR_aligned)], 'aligned_GT_sentences': [str(fake_test_list_GT_aligned)], 'set': ['val']}\n",
    "df_probeer = pd.DataFrame(data=d)\n",
    "\n",
    "for index, row in df.loc[df['set'].isin(['val'])].iterrows():\n",
    "    if (index == 70) or (index == 72):\n",
    "        print('index:', index)\n",
    "        identifier, error_positions, non_error_positions, right_token_positions_doc, rights_correct_sorted_list, wrongs_correct_sorted_list, rights_correct_sorted_nosw_list, wrongs_correct_sorted_nosw_list, rights_correct_calculated_list, wrongs_correct_calculated_list = detection_and_correction_word2vec(row, word2vec_model, ocr_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af82bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rights_correct_sorted_list)\n",
    "print(wrongs_correct_sorted_list)\n",
    "print(rights_correct_sorted_nosw_list)\n",
    "print(wrongs_correct_sorted_nosw_list)\n",
    "print(rights_correct_calculated_list)\n",
    "print(wrongs_correct_calculated_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "82984e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 1000, 1000, 1000, 1000, 1000]\n",
      "[1000, 1000, 1000, 1000, 1000, 1000, 7, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 60, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
      "errors: 1000\n",
      "non errors: 919.4583333333334\n",
      "right token in prediction: 911.3333333333334\n",
      "   rights_correct_sorted_list  wrongs_correct_sorted_list  \\\n",
      "0                           1                           5   \n",
      "\n",
      "   rights_correct_sorted_nosw_list  wrongs_correct_sorted_nosw_list  \\\n",
      "0                                1                                5   \n",
      "\n",
      "   rights_correct_calculated_list  wrongs_correct_calculated_list  \n",
      "0                               0                               6  \n"
     ]
    }
   ],
   "source": [
    "# analysis of performance on validation set word2vec\n",
    "#d = {'error_positions in detection': error_positions, 'non_error_positions in detection': non_error_positions}\n",
    "#validation_df_positions = pd.DataFrame(data = d)\n",
    "d = {'rights_correct_sorted_list': rights_correct_sorted_list, 'wrongs_correct_sorted_list': wrongs_correct_sorted_list, \\\n",
    "    'rights_correct_sorted_nosw_list': rights_correct_sorted_nosw_list, 'wrongs_correct_sorted_nosw_list': wrongs_correct_sorted_nosw_list, \\\n",
    "    'rights_correct_calculated_list': rights_correct_calculated_list, 'wrongs_correct_calculated_list': wrongs_correct_calculated_list}\n",
    "validation_df_correction_methods_word2vec = pd.DataFrame(data = d)\n",
    "\n",
    "\n",
    "print(error_positions)\n",
    "print(non_error_positions)\n",
    "#print(error_positions)\n",
    "error_positions = list_merger(error_positions)\n",
    "non_error_positions = list_merger(non_error_positions)\n",
    "right_token_positions = list_merger(right_token_positions)\n",
    "print('errors:', s.mean(error_positions))\n",
    "print('non errors:', s.mean(non_error_positions))\n",
    "print('right token in prediction:', s.mean(right_token_positions))\n",
    "print(validation_df_correction_methods_word2vec.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b3a92c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'identifier', 'gt text', 'ocr text', 'CER', 'WER',\n",
       "       'WER (order independent)', 'dictionary lookup gt',\n",
       "       'dictionary lookup ocr', 'jaccard_coefficient',\n",
       "       'levenshtein_dist_normalized', 'source', 'word count gt',\n",
       "       'word count ocr', 'year', 'century', 'old index', 'gt text org',\n",
       "       'ocr text org', 'set', 'gt sentences matched', 'ocr sentences matched',\n",
       "       'CER matched sentences', 'WER matched sentences',\n",
       "       'avg sentence length gt (fuzzy matched)_x',\n",
       "       'avg sentence length ocr (fuzzy matched)_x',\n",
       "       'max sentence length gt (fuzzy matched)_x',\n",
       "       'max sentence length ocr (fuzzy matched)_x',\n",
       "       'sentences gt (fuzzy matched)_x', 'sentences ocr (fuzzy matched)_x',\n",
       "       'word count gt (fuzzy matched)_x', 'word count ocr (fuzzy matched)_x',\n",
       "       'longest_streak', 'avg_longest_streaks', 'avg_total_missing_words',\n",
       "       'avg_perc_missing_words ', 'gt sentences matchated',\n",
       "       'avg sentence length gt (fuzzy matched)_y',\n",
       "       'avg sentence length ocr (fuzzy matched)_y',\n",
       "       'max sentence length gt (fuzzy matched)_y',\n",
       "       'max sentence length ocr (fuzzy matched)_y',\n",
       "       'sentences gt (fuzzy matched)_y', 'sentences ocr (fuzzy matched)_y',\n",
       "       'word count gt (fuzzy matched)_y', 'word count ocr (fuzzy matched)_y',\n",
       "       'aligned_GT_sentences', 'aligned_OCR_sentences', 'good_alignments',\n",
       "       'bad_alignments', 'gt for training'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "daf54eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val\n",
      "OCR: [['Een', 'hoekenpan', 'of', 'kortweg', 'pan', 'is', 'een', 'platte', 'pan', 'met', 'een', 'hang', 'handvat'], ['\\nDe', 'pan', 'ontleent', 'zijn', 'naam', 'haan', 'het', 'feit', 'dat', 'in', \"zo'n\", 'pan', 'pannenkoeken', 'horden', 'gebakken'], ['', 'Ook', 'ander', 'voedsel,', 'zoals', 'vlees,', 'word', 'in', 'een', 'hoekenpan', 'gebraden']]\n",
      "GT: [['Een', 'koekenpan', 'of', 'kortweg', 'pan', 'is', 'een', 'platte', 'pan', 'met', 'een', 'lang', 'handvat'], ['\\nDe', 'pan', 'ontleent', 'zijn', 'naam', 'aan', 'het', 'feit', 'dat', 'in', \"zo'n\", 'pan', 'pannenkoeken', 'worden', 'gebakken'], ['', 'Ook', 'ander', 'voedsel,', 'zoals', 'vlees,', 'wordt', 'in', 'een', 'koekenpan', 'gebraden']]\n",
      "[['Een', 'hoekenpan', 'of', 'kortweg', 'pan', 'is', 'een', 'platte', 'pan', 'met', 'een', 'hang', 'handvat'], ['\\nDe', 'pan', 'ontleent', 'zijn', 'naam', 'haan', 'het', 'feit', 'dat', 'in', \"zo'n\", 'pan', 'pannenkoeken', 'horden', 'gebakken'], ['', 'Ook', 'ander', 'voedsel,', 'zoals', 'vlees,', 'word', 'in', 'een', 'hoekenpan', 'gebraden']]\n",
      "[['Een', 'hoekenpan', 'of', 'kortweg', 'pan', 'is', 'een', 'platte', 'pan', 'met', 'een', 'hang', 'handvat'], ['\\nDe', 'pan', 'ontleent', 'zijn', 'naam', 'haan', 'het', 'feit', 'dat', 'in', \"zo'n\", 'pan', 'pannenkoeken', 'horden', 'gebakken'], ['', 'Ook', 'ander', 'voedsel,', 'zoals', 'vlees,', 'word', 'in', 'een', 'hoekenpan', 'gebraden']]\n",
      "[MASK] hoekenpan of kortweg pan is een platte pan met een hang handvat\n",
      "Een [MASK] of kortweg pan is een platte pan met een hang handvat\n",
      "position_right_token: 156\n",
      "OCR: hoekenpan\n",
      "correction sorted: koekenpan\n",
      "OCR: hoekenpan\n",
      "correction score: UNK\n",
      "Een hoekenpan of [MASK] pan is een platte pan met een hang handvat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nvanthof/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Een hoekenpan of kortweg [MASK] is een platte pan met een hang handvat\n",
      "Een hoekenpan of kortweg pan is [MASK] platte pan met een hang handvat\n",
      "Een hoekenpan of kortweg pan is een [MASK] pan met een hang handvat\n",
      "Een hoekenpan of kortweg pan is een platte [MASK] met een hang handvat\n",
      "Een hoekenpan of kortweg pan is een platte pan [MASK] een hang handvat\n",
      "Een hoekenpan of kortweg pan is een platte pan met [MASK] hang handvat\n",
      "Een hoekenpan of kortweg pan is een platte pan met een [MASK] handvat\n",
      "position_right_token: 123\n",
      "OCR: hang\n",
      "correction sorted: handig\n",
      "OCR: hang\n",
      "correction score: houten\n",
      "Een hoekenpan of kortweg pan is een platte pan met een hang [MASK]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nvanthof/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "De [MASK] ontleent zijn naam haan het feit dat in zo'n pan pannenkoeken horden gebakken\n",
      "\n",
      "De pan [MASK] zijn naam haan het feit dat in zo'n pan pannenkoeken horden gebakken\n",
      "\n",
      "De pan ontleent [MASK] naam haan het feit dat in zo'n pan pannenkoeken horden gebakken\n",
      "\n",
      "De pan ontleent zijn [MASK] haan het feit dat in zo'n pan pannenkoeken horden gebakken\n",
      "\n",
      "De pan ontleent zijn naam [MASK] het feit dat in zo'n pan pannenkoeken horden gebakken\n",
      "position_right_token: 0\n",
      "OCR: haan\n",
      "correction sorted: aan\n",
      "OCR: haan\n",
      "correction score: aan\n",
      "\n",
      "De pan ontleent zijn naam haan [MASK] feit dat in zo'n pan pannenkoeken horden gebakken\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nvanthof/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "De pan ontleent zijn naam haan het [MASK] dat in zo'n pan pannenkoeken horden gebakken\n",
      "\n",
      "De pan ontleent zijn naam haan het feit [MASK] in zo'n pan pannenkoeken horden gebakken\n",
      "\n",
      "De pan ontleent zijn naam haan het feit dat in zo'n [MASK] pannenkoeken horden gebakken\n",
      "\n",
      "De pan ontleent zijn naam haan het feit dat in zo'n pan [MASK] horden gebakken\n",
      "\n",
      "De pan ontleent zijn naam haan het feit dat in zo'n pan pannenkoeken [MASK] gebakken\n",
      "position_right_token: 0\n",
      "OCR: horden\n",
      "correction sorted: horen\n",
      "OCR: horden\n",
      "correction score: worden\n",
      "\n",
      "De pan ontleent zijn naam haan het feit dat in zo'n pan pannenkoeken horden [MASK]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nvanthof/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [MASK] ander voedsel, zoals vlees, word in een hoekenpan gebraden\n",
      " Ook [MASK] voedsel, zoals vlees, word in een hoekenpan gebraden\n",
      " Ook ander voedsel, [MASK] vlees, word in een hoekenpan gebraden\n",
      " Ook ander voedsel, zoals vlees, [MASK] in een hoekenpan gebraden\n",
      "position_right_token: 0\n",
      "OCR: word\n",
      "correction sorted: word\n",
      "OCR: word\n",
      "correction score: word\n",
      " Ook ander voedsel, zoals vlees, word in [MASK] hoekenpan gebraden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nvanthof/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/nvanthof/.local/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ook ander voedsel, zoals vlees, word in een [MASK] gebraden\n",
      "position_right_token: 246\n",
      "OCR: hoekenpan\n",
      "correction sorted: koekenpan\n",
      "OCR: hoekenpan\n",
      "correction score: restaurant\n",
      " Ook ander voedsel, zoals vlees, word in een hoekenpan [MASK]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nvanthof/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#detection and correction validation BERTje\n",
    "error_positions = [] # the position of a word when it is an error in predictions\n",
    "non_error_positions = [] # the position of a word when it is not an error in predictions\n",
    "right_token_positions = []\n",
    "\n",
    "rights_correct_sorted_list = []\n",
    "wrongs_correct_sorted_list = []\n",
    "rights_correct_sorted_nosw_list = []\n",
    "wrongs_correct_sorted_nosw_list = []\n",
    "rights_correct_calculated_list = []\n",
    "wrongs_correct_calculated_list = []\n",
    "\n",
    "def detection_and_correction_BERTje(row, BERT_model, tokenizer, ocr_names, topn_detection=1000):\n",
    "    if row['set'] != 'val':\n",
    "        return np.nan\n",
    "    else:\n",
    "        identifier = row['identifier']\n",
    "        print('Val')\n",
    "        OCR_text = row['aligned_OCR_sentences']\n",
    "        GT_text = row['aligned_GT_sentences']\n",
    "        print('OCR:', OCR_text)\n",
    "        print('GT:', GT_text)\n",
    "        OCR_text = ast.literal_eval(OCR_text)\n",
    "        GT_text = ast.literal_eval(GT_text)\n",
    "        print(OCR_text)\n",
    "        \n",
    "        # keep track of positions in candidates\n",
    "        error_positions_doc = []\n",
    "        non_error_positions_doc = []\n",
    "        right_token_positions_doc = []\n",
    "        \n",
    "        # keep track of performance\n",
    "        rights_correct_sorted = 0\n",
    "        wrongs_correct_sorted = 0\n",
    "        rights_correct_sorted_nosw = 0\n",
    "        wrongs_correct_sorted_nosw = 0\n",
    "        rights_correct_calculated = 0\n",
    "        wrongs_correct_calculated = 0\n",
    "        \n",
    "        print(OCR_text)\n",
    "        \n",
    "        for j in range(len(OCR_text)):\n",
    "            for i in range(len(OCR_text[j])):\n",
    "                if (OCR_text[j][i] in ocr_names) or (OCR_text[j][i].isalpha() == False) or (len(OCR_text[j][i]) <= 2):\n",
    "                    continue\n",
    "                error = True\n",
    "                if OCR_text[j][i] == GT_text[j][i]:\n",
    "                    error = False\n",
    "                candidates = []\n",
    "                probabilities = []\n",
    "                # calculate positions detection task\n",
    "                sentence = copy.deepcopy(OCR_text[j])\n",
    "                sentence[i] = '[MASK]'\n",
    "                sentence = ' '.join(sentence)\n",
    "                print(sentence)\n",
    "                pipe = pipeline('fill-mask', model=BERT_model, tokenizer = tokenizer, top_k=topn_detection)\n",
    "                for res in pipe(sentence):\n",
    "                    candidates.append(res['token_str'].replace(' ', ''))\n",
    "                    probabilities.append(res['score'])\n",
    "                # remove punctuation except for hyphen from candidates\n",
    "                candidates = [re.sub(r'[^\\w\\d\\s\\-]+', '', x) for x in candidates]\n",
    "                try:\n",
    "                    position = candidates.index(OCR_text[j][i])\n",
    "                except ValueError:\n",
    "                    position = topn_detection\n",
    "                if error == True:\n",
    "                    # where the error is in the detection list\n",
    "                    error_positions_doc.append(position)\n",
    "                    # find where the right word is in the candidates list (same as detection list)\n",
    "                    try: \n",
    "                        position_right_token = candidates.index(GT_text[j][i])\n",
    "                    except ValueError: \n",
    "                        position_right_token = topn_detection\n",
    "                    print('position_right_token:', position_right_token)\n",
    "                    right_token_positions_doc.append(position_right_token)\n",
    "                    # try two correction methods\n",
    "                    # first calculate the normalized LDs:\n",
    "                    LD = np.array([fuzz.ratio(OCR_text[j][i], word)/100 for word in candidates])\n",
    "                    # try sorting method\n",
    "                    correction = correct_sorted(candidates, probabilities, LD)\n",
    "                    if correction == GT_text[j][i]:\n",
    "                        rights_correct_sorted += 1\n",
    "                    elif correction != GT_text[j][i]:\n",
    "                        wrongs_correct_sorted += 1\n",
    "                    print('OCR:', OCR_text[j][i])\n",
    "                    print('correction sorted:', correction)\n",
    "                    # try again the sorting methods, but without stopwords\n",
    "                    candidates_nostopwords, probabilities_nostopwords, LD_nostopwords = remove_stopwords(candidates, probabilities, LD)\n",
    "                    correction = correct_sorted(candidates_nostopwords, probabilities_nostopwords, LD_nostopwords)\n",
    "                    if correction == GT_text[j][i]:\n",
    "                        rights_correct_sorted_nosw += 1\n",
    "                    elif correction != GT_text[j][i]:\n",
    "                        wrongs_correct_sorted_nosw += 1\n",
    "                    # try score calculation method\n",
    "                    correction = correct_calculated(candidates, probabilities, LD)\n",
    "                    if correction == GT_text[j][i]:\n",
    "                        rights_correct_calculated += 1\n",
    "                    elif correction != GT_text[j][i]:\n",
    "                        wrongs_correct_calculated += 1\n",
    "                    print('OCR:', OCR_text[j][i])\n",
    "                    print('correction score:', correction)\n",
    "                elif error == False:\n",
    "                    # where the non error is in the detection list\n",
    "                    non_error_positions_doc.append(position)\n",
    "                # calculate positions correction task\n",
    "                if error == True:\n",
    "                    try:\n",
    "                        right_token_position = candidates.index(GT_text[j][i])\n",
    "                        right_token_positions_doc.append(right_token_position)\n",
    "                    except ValueError:\n",
    "                        right_token_positions_doc.append(topn_detection)\n",
    "                    right_token_positions.append(right_token_positions_doc)\n",
    "            \n",
    "            \n",
    "    #return error_positions, non_error_positions\n",
    "    error_positions.append(error_positions_doc)\n",
    "    non_error_positions.append(non_error_positions_doc)\n",
    "    right_token_positions.append(right_token_positions_doc)\n",
    "    \n",
    "    # add performance to the list\n",
    "    rights_correct_sorted_list.append(rights_correct_sorted)\n",
    "    wrongs_correct_sorted_list.append(wrongs_correct_sorted)\n",
    "    rights_correct_sorted_nosw_list.append(rights_correct_sorted_nosw)\n",
    "    wrongs_correct_sorted_nosw_list.append(wrongs_correct_sorted_nosw)\n",
    "    rights_correct_calculated_list.append(rights_correct_calculated)\n",
    "    wrongs_correct_calculated_list.append(wrongs_correct_calculated)\n",
    "    \n",
    "    return identifier, error_positions, non_error_positions, right_token_positions_doc, rights_correct_sorted_list, wrongs_correct_sorted_list, rights_correct_sorted_nosw_list, wrongs_correct_sorted_nosw_list, rights_correct_calculated_list, wrongs_correct_calculated_list   \n",
    "    \n",
    "#for index, row in df.iterrows():\n",
    "#    detection_word2vec(row)\n",
    "# df.loc[70]\n",
    "fake_test_list_GT_aligned = \"\"\"Een koekenpan of kortweg pan is een platte pan met een lang handvat.\n",
    "De pan ontleent zijn naam aan het feit dat in zo'n pan pannenkoeken worden gebakken. Ook ander voedsel, zoals vlees, wordt in een koekenpan gebraden\"\"\"\n",
    "fake_test_list_OCR_aligned = \"\"\"Een hoekenpan of kortweg pan is een platte pan met een hang handvat.\n",
    "De pan ontleent zijn naam haan het feit dat in zo'n pan pannenkoeken horden gebakken. Ook ander voedsel, zoals vlees, word in een hoekenpan gebraden\"\"\"\n",
    "fake_test_list_GT_aligned = fake_test_list_GT_aligned.split('.')\n",
    "fake_test_list_OCR_aligned = fake_test_list_OCR_aligned.split('.')\n",
    "fake_test_list_GT_aligned = [x.split(' ') for x in fake_test_list_GT_aligned]\n",
    "fake_test_list_OCR_aligned = [x.split(' ') for x in fake_test_list_OCR_aligned]\n",
    "d = {'identifier': ['111'], 'aligned_OCR_sentences': [str(fake_test_list_OCR_aligned)], 'aligned_GT_sentences': [str(fake_test_list_GT_aligned)], 'set': ['val']}\n",
    "df_probeer = pd.DataFrame(data=d)\n",
    "identifier, error_positions, non_error_positions, right_token_positions_doc, rights_correct_sorted_list, wrongs_correct_sorted_list, rights_correct_sorted_nosw_list, wrongs_correct_sorted_nosw_list, rights_correct_calculated_list, wrongs_correct_calculated_list = detection_and_correction_BERTje(df_probeer.loc[0], BERT_model, tokenizer, ocr_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "81765a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[156, 156, 123, 123, 0, 0, 0, 0, 0, 0, 246, 246], [156, 156, 123, 123, 0, 0, 0, 0, 0, 0, 246, 246], [156, 156, 123, 123, 0, 0, 0, 0, 0, 0, 246, 246], [156, 156, 123, 123, 0, 0, 0, 0, 0, 0, 246, 246], [156, 156, 123, 123, 0, 0, 0, 0, 0, 0, 246, 246], [156, 156, 123, 123, 0, 0, 0, 0, 0, 0, 246, 246], [156, 156, 123, 123, 0, 0, 0, 0, 0, 0, 246, 246]]\n",
      "errors: 851.8333333333334\n",
      "non errors: 180.66666666666666\n",
      "right token in prediction: 87.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rights_correct_sorted_list</th>\n",
       "      <th>wrongs_correct_sorted_list</th>\n",
       "      <th>rights_correct_sorted_nosw_list</th>\n",
       "      <th>wrongs_correct_sorted_nosw_list</th>\n",
       "      <th>rights_correct_calculated_list</th>\n",
       "      <th>wrongs_correct_calculated_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rights_correct_sorted_list  wrongs_correct_sorted_list  \\\n",
       "0                           3                           3   \n",
       "\n",
       "   rights_correct_sorted_nosw_list  wrongs_correct_sorted_nosw_list  \\\n",
       "0                                2                                4   \n",
       "\n",
       "   rights_correct_calculated_list  wrongs_correct_calculated_list  \n",
       "0                               2                               4  "
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analysis of performance on validation set BERTje\n",
    "#d = {'error_positions in detection': error_positions, 'non_error_positions in detection': non_error_positions}\n",
    "#validation_df_positions = pd.DataFrame(data = d)\n",
    "d = {'rights_correct_sorted_list': rights_correct_sorted_list, 'wrongs_correct_sorted_list': wrongs_correct_sorted_list, \\\n",
    "    'rights_correct_sorted_nosw_list': rights_correct_sorted_nosw_list, 'wrongs_correct_sorted_nosw_list': wrongs_correct_sorted_nosw_list, \\\n",
    "    'rights_correct_calculated_list': rights_correct_calculated_list, 'wrongs_correct_calculated_list': wrongs_correct_calculated_list}\n",
    "validation_df_correction_methods_BERTje = pd.DataFrame(data = d)\n",
    "\n",
    "print(right_token_positions)\n",
    "#print(error_positions)\n",
    "error_positions = list_merger(error_positions)\n",
    "non_error_positions = list_merger(non_error_positions)\n",
    "right_token_positions = list_merger(right_token_positions)\n",
    "print('errors:', s.mean(error_positions))\n",
    "print('non errors:', s.mean(non_error_positions))\n",
    "print('right token in prediction:', s.mean(right_token_positions))\n",
    "validation_df_correction_methods_BERTje.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb02a319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[-2. -1.  0.  1.  2.]\n",
      "0\n",
      "Dit1\n",
      "context: ['is2', 'een3']\n",
      "1\n",
      "is2\n",
      "context: ['Dit1', 'een3', 'zin4']\n",
      "2\n",
      "een3\n",
      "context: ['Dit1', 'is2', 'zin4', 'die5']\n",
      "3\n",
      "zin4\n",
      "context: ['is2', 'een3', 'die5', 'lang6']\n",
      "4\n",
      "die5\n",
      "context: ['een3', 'zin4', 'lang6', 'genoeg7']\n",
      "5\n",
      "lang6\n",
      "context: ['zin4', 'die5', 'genoeg7', 'is8']\n",
      "6\n",
      "genoeg7\n",
      "context: ['die5', 'lang6', 'is8']\n",
      "7\n",
      "is8\n",
      "context: ['lang6', 'genoeg7']\n"
     ]
    }
   ],
   "source": [
    "text = \"Dit1 is2 een3 zin4 die5 lang6 genoeg7 is8\"\n",
    "text = text.split(' ')\n",
    "\n",
    "window = 5\n",
    "window_range = list(range(0,window))\n",
    "print(window_range)\n",
    "window_range = np.array(window_range) - ((window - 1) / 2)\n",
    "print(window_range)\n",
    "\n",
    "context = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    print(i)\n",
    "    print(text[i])\n",
    "    context = []\n",
    "    for j in window_range:\n",
    "        if (i+j >= 0) and (i+j < len(text)) and j != 0:\n",
    "            context.append(text[i+int(j)])\n",
    "        else:\n",
    "            pass\n",
    "    print('context:', context)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7a61a40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('den', 7.57105e-07),\n",
       " ('ende', 7.248402e-07),\n",
       " ('de', 7.188712e-07),\n",
       " ('noch', 7.1387456e-07),\n",
       " ('is', 7.0737497e-07),\n",
       " ('hebben', 7.070108e-07),\n",
       " ('vyand', 7.065082e-07),\n",
       " ('eenige', 7.0626095e-07),\n",
       " ('met', 7.061281e-07),\n",
       " ('wt', 7.042607e-07)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.predict_output_word(['Een', 'of', 'kortweg'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "20f0204c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('karton', 0.8278681039810181),\n",
       " ('inkt', 0.8026106357574463),\n",
       " ('rubberdoek', 0.7930652499198914),\n",
       " ('krantenpapier', 0.792934775352478),\n",
       " ('perkament', 0.7911171913146973),\n",
       " ('vloeipapier', 0.7774406671524048),\n",
       " ('vel', 0.7763333320617676),\n",
       " ('handgeschept', 0.771336555480957),\n",
       " ('potlood', 0.760221540927887),\n",
       " ('bedrukken', 0.7578753232955933)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar('papier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "28dd2d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ik vind jou een meisje.\n",
      "{'sequence': 'Ik vind jou een meisje.', 'score': 0.06847196072340012, 'token': 15499, 'token_str': 'm e i s j e'}\n",
      "Ik vind jou een vrouw.\n",
      "{'sequence': 'Ik vind jou een vrouw.', 'score': 0.03620399907231331, 'token': 22098, 'token_str': 'v r o u w'}\n",
      "Ik vind jou een vriendin.\n",
      "{'sequence': 'Ik vind jou een vriendin.', 'score': 0.03159778192639351, 'token': 22057, 'token_str': 'v r i e n d i n'}\n",
      "Ik vind jou een schatje.\n",
      "{'sequence': 'Ik vind jou een schatje.', 'score': 0.030144749209284782, 'token': 18609, 'token_str': 's c h a t j e'}\n",
      "Ik vind jou een vriend.\n",
      "{'sequence': 'Ik vind jou een vriend.', 'score': 0.02135034091770649, 'token': 22053, 'token_str': 'v r i e n d'}\n",
      "Ik vind jou een klootzak.\n",
      "{'sequence': 'Ik vind jou een klootzak.', 'score': 0.017202582210302353, 'token': 14376, 'token_str': 'k l o o t z a k'}\n",
      "Ik vind jou een trut.\n",
      "{'sequence': 'Ik vind jou een trut.', 'score': 0.015790071338415146, 'token': 20399, 'token_str': 't r u t'}\n",
      "Ik vind jou een jongen.\n",
      "{'sequence': 'Ik vind jou een jongen.', 'score': 0.014341817237436771, 'token': 13968, 'token_str': 'j o n g e n'}\n",
      "Ik vind jou een schat.\n",
      "{'sequence': 'Ik vind jou een schat.', 'score': 0.014271577820181847, 'token': 18608, 'token_str': 's c h a t'}\n",
      "Ik vind jou een mens.\n",
      "{'sequence': 'Ik vind jou een mens.', 'score': 0.01069585233926773, 'token': 15528, 'token_str': 'm e n s'}\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline('fill-mask', model=BERT_model, tokenizer = tokenizer, top_k=10)\n",
    "for res in pipe('Ik vind jou een [MASK].'):\n",
    "    print(res['sequence'])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2f5d337f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweeling\n",
      "0.03529518470168114\n",
      "jongen\n",
      "0.021275663748383522\n",
      "Kameleon\n",
      "0.016648495569825172\n",
      "[UNK]\n",
      "0.01608702912926674\n",
      "dwerg\n",
      "0.010880783200263977\n",
      "[UNK]\n",
      "0.019976049661636353\n",
      "q\n",
      "0.011574698612093925\n",
      "##Ô\n",
      "0.011502194218337536\n",
      "##ô\n",
      "0.011489509604871273\n",
      "x\n",
      "0.011456236243247986\n"
     ]
    }
   ],
   "source": [
    "old_sentence = \"Die [MASK] hadde viele avontuere ghemaect.\"\n",
    "new_sentence = \"De [MASK] had vele avonturen gemaakt.\"\n",
    "pipe = pipeline('fill-mask', model=BERT_model, tokenizer = tokenizer, top_k=5)\n",
    "for res in pipe(new_sentence):\n",
    "    print(res['token_str'].replace(' ', ''))\n",
    "    print(res['score'])\n",
    "    pipe = pipeline('fill-mask', model=BERT_model, tokenizer = tokenizer, top_k=5)\n",
    "for res in pipe(old_sentence):\n",
    "    print(res['token_str'].replace(' ', ''))\n",
    "    print(res['score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
