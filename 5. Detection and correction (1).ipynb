{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2d5ea7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle\n",
    "import ast\n",
    "import statistics as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d138fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec model\n",
    "#w2v_model = ...\n",
    "# load BERT model\n",
    "#BERT_model = ...\n",
    "# load dataframe\n",
    "#df = ...\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "BERT_model = BertForMaskedLM.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "#w2v_model.intersect_word2vec_format(r\"combined-160.txt\", binary=False, lockf=1.0)\n",
    "# https://github.com/clips/dutchembeddings\n",
    "\n",
    "df = pd.read_csv('preprocessed_df100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f3e845f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvanthof/.local/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "def finetune_word2vec(train, window=5):\n",
    "    sentences = train.split('.')\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sentences = [tokenizer.tokenize(i) for i in sentences]\n",
    "    total_examples = len(sentences)\n",
    "    \n",
    "    model_w2v = Word2Vec(size=160, min_count=1, window=window)\n",
    "    model_w2v.build_vocab(sentences)\n",
    "    total_examples = model_w2v.corpus_count\n",
    "    model = KeyedVectors.load_word2vec_format(r\"combined-160.txt\", binary=False)\n",
    "    model_w2v.build_vocab([list(model.vocab.keys())], update=True)\n",
    "    model_w2v.intersect_word2vec_format(r\"combined-160.txt\", binary=False, lockf=1.0)\n",
    "    model_w2v.train(sentences, total_examples=total_examples, epochs=model_w2v.iter)\n",
    "    return model_w2v\n",
    "\n",
    "train = df['gt text'][0]\n",
    "word2vec_model = finetune_word2vec(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de5d676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c259f170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "df.columns\n",
    "print(df.index.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83df3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters for validation\n",
    "#topn_detection = 1000\n",
    "#topn_correction = 1000\n",
    "#window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "46b20280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_merger(lists):\n",
    "    new_list = []\n",
    "    for elem in lists:\n",
    "        new_list = new_list + elem\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4fe6a8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: ['engelant', 'ampc']\n",
      "ELEM: ['engelant', 'ampc']\n",
      "ELEM: ['dublin', 'den', '24', 'maert']\n",
      "ELEM: ['tegens', 'de', 'charters', 'van', 'dese', 'stadt']\n",
      "ELEM: []\n",
      "['engelant', 'ampc', 'dublin', 'den', '24', 'maert', 'tegens', 'de', 'charters', 'van', 'dese', 'stadt']\n"
     ]
    }
   ],
   "source": [
    "#lijst = [['aa','bb','cc'],['dd','ee','ff'],[7,8,9]]\n",
    "lijst = [['engelant', 'ampc'], ['dublin', 'den', '24', 'maert'], ['tegens', 'de', 'charters', 'van', 'dese', 'stadt'], []]\n",
    "lijst = list_merger(lijst)\n",
    "print(lijst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "362ab8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[['engelant', 'ampc'], ['dublin', 'den', '24', 'maert'], ['tegens', 'de', 'charters', 'van', 'dese', 'stadt'], []]\n",
      "engelant\n"
     ]
    }
   ],
   "source": [
    "string = \"[['engelant', 'ampc'], ['dublin', 'den', '24', 'maert'], ['tegens', 'de', 'charters', 'van', 'dese', 'stadt'], []]\"\n",
    "lijst = ast.literal_eval(string)\n",
    "print(type(lijst))\n",
    "print(lijst)\n",
    "print(lijst[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ad09d631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val\n",
      "[['Een', 'hoekenpan', 'of', 'hortweg', 'pan', 'is', 'een', 'hlatte', 'pan', 'met', 'een', 'hang', 'handvat'], ['\\nDe', 'pan', 'ontleent', 'zijn', 'naam', 'haan', 'het', 'heit', 'dat', 'in', \"ho'n\", 'pan', 'pannenkoeken', 'horden', 'gebakken'], ['', 'Ook', 'hnder', 'voedsel,', 'zoals', 'vlees,', 'word', 'in', 'een', 'koekenpan', 'gebraden']]\n",
      "['Een', 'hoekenpan', 'of', 'hortweg', 'pan', 'is', 'een', 'hlatte', 'pan', 'met']\n",
      "[[1000, 1000, 1000]]\n",
      "[[1000, 1000, 1000, 4, 1000, 1000, 5]]\n",
      "errors: 1000\n",
      "non errors: 715.5714285714286\n"
     ]
    }
   ],
   "source": [
    "#detection validation\n",
    "\n",
    "detection_df = pd.DataFrame()\n",
    "error_positions = [] # the position of a word when it is an error in predictions\n",
    "non_error_positions = [] # the position of a word when it is not an error in predictions\n",
    "\n",
    "# detection word2vec\n",
    "def detection_word2vec(row, w2v_model, window=5, topn_detection=1000):\n",
    "    if row['set'] != 'val':\n",
    "        return np.nan\n",
    "    else:\n",
    "        print('Val')\n",
    "        OCR_text = row['aligned_OCR_sentences']\n",
    "        GT_text = row['aligned_GT_sentences']\n",
    "        print(OCR_text)\n",
    "        OCR_text = ast.literal_eval(OCR_text)\n",
    "        GT_text = ast.literal_eval(GT_text)\n",
    "        OCR_text = list_merger(OCR_text)\n",
    "        GT_text = list_merger(GT_text)\n",
    "        OCR_text = OCR_text[:10]\n",
    "        print(OCR_text)\n",
    "        window_range = list(range(0,window))\n",
    "        window_range = np.array(window_range) - ((window - 1) / 2)\n",
    "        error_positions_doc = []\n",
    "        non_error_positions_doc = []\n",
    "        for i in range(len(OCR_text)):\n",
    "            error = True\n",
    "            if OCR_text[i] == GT_text[i]:\n",
    "                error = False\n",
    "            context = []\n",
    "            for j in window_range:\n",
    "                if (i+j >= 0) and (i+j < len(OCR_text)) and j != 0:\n",
    "                    context.append(OCR_text[i+int(j)])\n",
    "                else:\n",
    "                    pass\n",
    "            predictions = []\n",
    "            for prediction in w2v_model.predict_output_word(context, topn=topn_detection):\n",
    "                predictions.append(prediction[0])\n",
    "            try:\n",
    "                position = predictions.index(OCR_text[i])\n",
    "            except:\n",
    "                position = topn_detection\n",
    "            if error == True:\n",
    "                error_positions_doc.append(position)\n",
    "            elif error == False:\n",
    "                non_error_positions_doc.append(position)\n",
    "    #return error_positions, non_error_positions\n",
    "    error_positions.append(error_positions_doc)\n",
    "    non_error_positions.append(non_error_positions_doc)\n",
    "    \n",
    "    \n",
    "    \n",
    "#for index, row in df.iterrows():\n",
    "#    detection_word2vec(row)\n",
    "# df.loc[70]\n",
    "fake_test_list_GT_aligned = \"\"\"Een koekenpan of kortweg pan is een platte pan met een lang handvat.\n",
    "De pan ontleent zijn naam aan het feit dat in zo'n pan pannenkoeken worden gebakken. Ook ander voedsel, zoals vlees, wordt in een koekenpan gebraden\"\"\"\n",
    "fake_test_list_OCR_aligned = \"\"\"Een hoekenpan of hortweg pan is een hlatte pan met een hang handvat.\n",
    "De pan ontleent zijn naam haan het heit dat in ho'n pan pannenkoeken horden gebakken. Ook hnder voedsel, zoals vlees, word in een koekenpan gebraden\"\"\"\n",
    "fake_test_list_GT_aligned = fake_test_list_GT_aligned.split('.')\n",
    "fake_test_list_OCR_aligned = fake_test_list_OCR_aligned.split('.')\n",
    "fake_test_list_GT_aligned = [x.split(' ') for x in fake_test_list_GT_aligned]\n",
    "fake_test_list_OCR_aligned = [x.split(' ') for x in fake_test_list_OCR_aligned]\n",
    "d = {'aligned_OCR_sentences': [str(fake_test_list_OCR_aligned)], 'aligned_GT_sentences': [str(fake_test_list_GT_aligned)], 'set': ['val']}\n",
    "df_probeer = pd.DataFrame(data=d)\n",
    "detection_word2vec(df_probeer.loc[0], word2vec_model)\n",
    "print(error_positions)\n",
    "print(non_error_positions)\n",
    "error_positions = list_merger(error_positions)\n",
    "non_error_positions = list_merger(non_error_positions)\n",
    "print('errors:', s.mean(error_positions))\n",
    "print('non errors:', s.mean(non_error_positions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fc28a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[-2. -1.  0.  1.  2.]\n",
      "0\n",
      "Dit1\n",
      "context: ['is2', 'een3']\n",
      "1\n",
      "is2\n",
      "context: ['Dit1', 'een3', 'zin4']\n",
      "2\n",
      "een3\n",
      "context: ['Dit1', 'is2', 'zin4', 'die5']\n",
      "3\n",
      "zin4\n",
      "context: ['is2', 'een3', 'die5', 'lang6']\n",
      "4\n",
      "die5\n",
      "context: ['een3', 'zin4', 'lang6', 'genoeg7']\n",
      "5\n",
      "lang6\n",
      "context: ['zin4', 'die5', 'genoeg7', 'is8']\n",
      "6\n",
      "genoeg7\n",
      "context: ['die5', 'lang6', 'is8']\n",
      "7\n",
      "is8\n",
      "context: ['lang6', 'genoeg7']\n"
     ]
    }
   ],
   "source": [
    "text = \"Dit1 is2 een3 zin4 die5 lang6 genoeg7 is8\"\n",
    "text = text.split(' ')\n",
    "\n",
    "window = 5\n",
    "window_range = list(range(0,window))\n",
    "print(window_range)\n",
    "window_range = np.array(window_range) - ((window - 1) / 2)\n",
    "print(window_range)\n",
    "\n",
    "context = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    print(i)\n",
    "    print(text[i])\n",
    "    context = []\n",
    "    for j in window_range:\n",
    "        if (i+j >= 0) and (i+j < len(text)) and j != 0:\n",
    "            context.append(text[i+int(j)])\n",
    "        else:\n",
    "            pass\n",
    "    print('context:', context)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "332e9928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('den', 7.57105e-07),\n",
       " ('ende', 7.248402e-07),\n",
       " ('de', 7.188712e-07),\n",
       " ('noch', 7.1387456e-07),\n",
       " ('is', 7.0737497e-07),\n",
       " ('hebben', 7.070108e-07),\n",
       " ('vyand', 7.065082e-07),\n",
       " ('eenige', 7.0626095e-07),\n",
       " ('met', 7.061281e-07),\n",
       " ('wt', 7.042607e-07)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.predict_output_word(['Een', 'of', 'kortweg'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1f0047ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('karton', 0.8278681039810181),\n",
       " ('inkt', 0.8026106357574463),\n",
       " ('rubberdoek', 0.7930652499198914),\n",
       " ('krantenpapier', 0.792934775352478),\n",
       " ('perkament', 0.7911171913146973),\n",
       " ('vloeipapier', 0.7774406671524048),\n",
       " ('vel', 0.7763333320617676),\n",
       " ('handgeschept', 0.771336555480957),\n",
       " ('potlood', 0.760221540927887),\n",
       " ('bedrukken', 0.7578753232955933)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar('papier')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
