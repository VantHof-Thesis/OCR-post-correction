{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c6be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db018e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle\n",
    "import ast\n",
    "import statistics as s\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "import copy\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb65e864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c942d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec model\n",
    "word2vec_model = Word2Vec.load(\"word2vec_finetuned.model\")\n",
    "# load BERT model\n",
    "#BERT_model = torch.load('BERT_finetuned.pt')\n",
    "# load dataframe\n",
    "df = pd.read_csv('preprocessed_df.csv')\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "#BERT_model = BertForMaskedLM.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "#w2v_model.intersect_word2vec_format(r\"combined-160.txt\", binary=False, lockf=1.0)\n",
    "# https://github.com/clips/dutchembeddings\n",
    "word2vec_model = Word2Vec.load(\"word2vec_finetuned.model\")\n",
    "\n",
    "# skiplist (words that should not be corrected: names)\n",
    "with open(\"ocr_names.txt\", \"rb\") as fp:   # Unpickling\n",
    "    ocr_names = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e377c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4977c926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617fada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7fbc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "'pannenkoeken' in ocr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a37c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters for validation\n",
    "#topn_detection = 1000\n",
    "#topn_correction = 1000\n",
    "#window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649fed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_merger(lists):\n",
    "    new_list = []\n",
    "    for elem in lists:\n",
    "        new_list = new_list + elem\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ac99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_numbers(token):\n",
    "    numbers = any(char.isdigit() for char in token)\n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b81e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sorted(candidates, sim_or_probs, LD): # sorts first by LD, then by similarity/probability\n",
    "    paired_sorted = sorted(zip(LD,sim_or_probs,candidates),key = lambda x: (x[0],x[1]), reverse=True)\n",
    "    LD,sim_or_probs,candidates = zip(*paired_sorted)\n",
    "    correction = candidates[0]\n",
    "    return correction\n",
    "    \n",
    "def correct_calculated(candidates, sim_or_probs, LD): # calculates a score from LD and normalised similarity/probability\n",
    "    inv_LD = 1 - LD\n",
    "    sim_or_probs = np.array(sim_or_probs)\n",
    "    sim_or_probs = np.interp(sim_or_probs, (sim_or_probs.min(), sim_or_probs.max()), (0, 1)).tolist()\n",
    "    score = sim_or_probs / inv_LD\n",
    "    zipped_pairs = zip(score.tolist(), candidates)\n",
    "    sorted_by_score = [x for _, x in sorted(zipped_pairs, reverse=True)]\n",
    "    correction = sorted_by_score[0]\n",
    "    return correction\n",
    "\n",
    "def remove_stopwords(candidates, cosine, LD):\n",
    "    stop_words = set(stopwords.words('dutch'))\n",
    "    candidates_nostopwords = []\n",
    "    cosine_nostopwords = []\n",
    "    LD_nostopwords = []\n",
    "    for i in range(len(candidates)):\n",
    "        if candidates[i] not in stop_words:\n",
    "            candidates_nostopwords.append(candidates[i])\n",
    "            cosine_nostopwords.append(cosine[i])\n",
    "            LD_nostopwords.append(LD[i])\n",
    "    LD_nostopwords = np.array(LD_nostopwords)\n",
    "    return candidates_nostopwords, cosine_nostopwords, LD_nostopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab264ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#detection and correction validation word2vec\n",
    "error_positions = [] # the position of a word when it is an error in predictions\n",
    "non_error_positions = [] # the position of a word when it is not an error in predictions\n",
    "right_token_positions = []\n",
    "\n",
    "rights_correct_sorted_list = []\n",
    "wrongs_correct_sorted_list = []\n",
    "rights_correct_sorted_nosw_list = []\n",
    "wrongs_correct_sorted_nosw_list = []\n",
    "rights_correct_calculated_list = []\n",
    "wrongs_correct_calculated_list = []\n",
    "\n",
    "def detection_and_correction_word2vec(row, w2v_model, ocr_names, window=5, topn_detection=500):\n",
    "    if row['set'] != 'val':\n",
    "        return np.nan\n",
    "    else:\n",
    "        identifier = row['identifier']\n",
    "        OCR_text = row['aligned_OCR_sentences']\n",
    "        GT_text = row['aligned_GT_sentences']\n",
    "        OCR_text = ast.literal_eval(OCR_text)\n",
    "        GT_text = ast.literal_eval(GT_text)\n",
    "        OCR_text = list_merger(OCR_text)\n",
    "        GT_text = list_merger(GT_text)\n",
    "        window_range = list(range(0,window))\n",
    "        window_range = np.array(window_range) - ((window - 1) / 2)\n",
    "        \n",
    "        #print(OCR_text)\n",
    "        \n",
    "        # keep track of positions in candidates\n",
    "        error_positions_doc = []\n",
    "        non_error_positions_doc = []\n",
    "        right_token_positions_doc = []\n",
    "        \n",
    "        # keep track of performance\n",
    "        rights_correct_sorted = 0\n",
    "        wrongs_correct_sorted = 0\n",
    "        rights_correct_sorted_nosw = 0\n",
    "        wrongs_correct_sorted_nosw = 0\n",
    "        rights_correct_calculated = 0\n",
    "        wrongs_correct_calculated = 0\n",
    "        \n",
    "        for i in range(len(OCR_text)):\n",
    "            #print(OCR_text[i])\n",
    "            try:\n",
    "                if (OCR_text[i] in ocr_names) or (check_numbers(OCR_text[i]) == True) or (len(OCR_text[i]) <= 2) or (GT_text[i] == 'REMOVED'):\n",
    "                    continue\n",
    "                error = True\n",
    "                if OCR_text[i] == GT_text[i]:\n",
    "                    error = False\n",
    "                context = []\n",
    "                for j in window_range:\n",
    "                    if (i+j >= 0) and (i+j < len(OCR_text)) and j != 0:\n",
    "                        #context.append(OCR_text[i+int(j)])\n",
    "                        if any(str.isdigit(c) for c in  OCR_text[i+int(j)]) == True:\n",
    "                            context.append('%NUMBER%')\n",
    "                        elif OCR_text[i+int(j)] in ocr_names:\n",
    "                            context.append('%NNP%')\n",
    "                        else:\n",
    "                            context.append(OCR_text[i+int(j)])\n",
    "                        #GT_context.append(GT_text[i+int(j)])\n",
    "                candidates = []\n",
    "                cosines = []\n",
    "                # calculate positions detection task\n",
    "                try:\n",
    "                    for prediction in w2v_model.predict_output_word(context, topn=topn_detection):\n",
    "                        candidates.append(prediction[0])\n",
    "                        cosines.append(prediction[1]) \n",
    "                    # remove punctuation except for hyphen from candidates\n",
    "                    candidates = [re.sub(r'[^\\w\\d\\s\\-]+', '', x) for x in candidates]\n",
    "                    candidates = [x.lower() for x in candidates]\n",
    "                    try:\n",
    "                        position = candidates.index(OCR_text[i])\n",
    "                    except ValueError:\n",
    "                        position = topn_detection\n",
    "                    #print('error?', error)\n",
    "                    if error == True:\n",
    "                        #print('correction needed:', row['identifier'])\n",
    "                        # where the error is in the detection list\n",
    "                        error_positions_doc.append(position)\n",
    "                        # find where the right word is in the candidates list (same as detection list)\n",
    "                        try: \n",
    "                            position_right_token = candidates.index(GT_text[i])\n",
    "                        except ValueError: \n",
    "                            position_right_token = topn_detection\n",
    "                        right_token_positions_doc.append(position_right_token)\n",
    "                        # try two correction methods\n",
    "                        # first calculate the normalized LDs:\n",
    "                        LD = np.array([fuzz.ratio(OCR_text[i], word)/100 for word in candidates])\n",
    "                        # try sorting method\n",
    "                        correction = correct_sorted(candidates, cosines, LD)\n",
    "                        if correction == GT_text[i]:\n",
    "                            rights_correct_sorted += 1\n",
    "                        elif correction != GT_text[i]:\n",
    "                            wrongs_correct_sorted += 1\n",
    "                        # try again the sorting methods, but without stopwords\n",
    "                        candidates_nostopwords, cosine_nostopwords, LD_nostopwords = remove_stopwords(candidates, cosines, LD)\n",
    "                        correction = correct_sorted(candidates_nostopwords, cosine_nostopwords, LD_nostopwords)\n",
    "                        if correction == GT_text[i]:\n",
    "                            rights_correct_sorted_nosw += 1\n",
    "                        elif correction != GT_text[i]:\n",
    "                            wrongs_correct_sorted_nosw += 1\n",
    "                        # try score calculation method\n",
    "                        correction = correct_calculated(candidates, cosines, LD)\n",
    "                        if correction == GT_text[i]:\n",
    "                            rights_correct_calculated += 1\n",
    "                        elif correction != GT_text[i]:\n",
    "                            wrongs_correct_calculated += 1\n",
    "                    elif error == False:\n",
    "                        # where the non error is in the detection list\n",
    "                        non_error_positions_doc.append(position)\n",
    "                    # calculate positions correction task\n",
    "                    if error == True:\n",
    "                        try:\n",
    "                            right_token_position = candidates.index(GT_text[i])\n",
    "                            right_token_positions_doc.append(right_token_position)\n",
    "                        except ValueError:\n",
    "                            right_token_positions_doc.append(topn_detection)\n",
    "                        right_token_positions.append(right_token_positions_doc)\n",
    "                except:\n",
    "                    pass\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "    #return error_positions, non_error_positions\n",
    "    error_positions.append(error_positions_doc)\n",
    "    non_error_positions.append(non_error_positions_doc)\n",
    "    right_token_positions.append(right_token_positions_doc)\n",
    "    \n",
    "    # add performance to the list\n",
    "    rights_correct_sorted_list.append(rights_correct_sorted)\n",
    "    wrongs_correct_sorted_list.append(wrongs_correct_sorted)\n",
    "    rights_correct_sorted_nosw_list.append(rights_correct_sorted_nosw)\n",
    "    wrongs_correct_sorted_nosw_list.append(wrongs_correct_sorted_nosw)\n",
    "    rights_correct_calculated_list.append(rights_correct_calculated)\n",
    "    wrongs_correct_calculated_list.append(wrongs_correct_calculated)\n",
    "    \n",
    "    #return identifier, error_positions, non_error_positions, right_token_positions, rights_correct_sorted_list, wrongs_correct_sorted_list, rights_correct_sorted_nosw_list, wrongs_correct_sorted_nosw_list, rights_correct_calculated_list, wrongs_correct_calculated_list   \n",
    "    \n",
    "#for index, row in df.iterrows():\n",
    "#    detection_word2vec(row)\n",
    "# df.loc[70]\n",
    "fake_test_list_GT_aligned = \"\"\"Een koekenpan of kortweg pan is een platte pan met een lang handvat.\n",
    "De pan ontleent zijn naam aan het feit dat in zo'n pan pannenkoeken worden gebakken. Ook ander voedsel, zoals vlees, wordt in een koekenpan gebraden\"\"\"\n",
    "fake_test_list_OCR_aligned = \"\"\"Een hoekenpan of kortweg pan is een platte pan met een hang handvat.\n",
    "De pan ontleent zijn naam haan het feit dat in zo'n pan pannenkoeken horden gebakken. Ook ander voedsel, zoals vlees, word in een hoekenpan gebraden\"\"\"\n",
    "fake_test_list_GT_aligned = fake_test_list_GT_aligned.split('.')\n",
    "fake_test_list_OCR_aligned = fake_test_list_OCR_aligned.split('.')\n",
    "fake_test_list_GT_aligned = [x.split(' ') for x in fake_test_list_GT_aligned]\n",
    "fake_test_list_OCR_aligned = [x.split(' ') for x in fake_test_list_OCR_aligned]\n",
    "d = {'identifier': ['111'], 'aligned_OCR_sentences': [str(fake_test_list_OCR_aligned)], 'aligned_GT_sentences': [str(fake_test_list_GT_aligned)], 'set': ['val']}\n",
    "df_probeer = pd.DataFrame(data=d)\n",
    "\n",
    "#identifier, w2v_error_positions, w2v_non_error_positions, w2v_right_token_positions, rights_correct_sorted_list, wrongs_correct_sorted_list, rights_correct_sorted_nosw_list, wrongs_correct_sorted_nosw_list, rights_correct_calculated_list, wrongs_correct_calculated_list = detection_and_correction_word2vec(df_probeer.loc[0], word2vec_model, ocr_names)\n",
    "\n",
    "print('start')\n",
    "for index, row in df.loc[df['set'].isin(['val'])].iterrows():\n",
    "    if index%1000 == 0:\n",
    "        print(index)\n",
    "    detection_and_correction_word2vec(row, word2vec_model, ocr_names)\n",
    "#detection_and_correction_word2vec(df.loc[6], word2vec_model, ocr_names)\n",
    "#detection_and_correction_word2vec(df.loc[7], word2vec_model, ocr_names)\n",
    "#detection_and_correction_word2vec(df.loc[37], word2vec_model, ocr_names)\n",
    "\n",
    "#detection_and_correction_word2vec(df_probeer.loc[0], word2vec_model, ocr_names)\n",
    "\n",
    "#for index, row in df.loc[df['set'].isin(['val'])].iterrows():\n",
    "#    if (index == 70) or (index == 72):\n",
    "#        print('index:', index)\n",
    "#        identifier, w2v_error_positions, w2v_non_error_positions, w2v_right_token_positions, rights_correct_sorted_list, wrongs_correct_sorted_list, rights_correct_sorted_nosw_list, wrongs_correct_sorted_nosw_list, rights_correct_calculated_list, wrongs_correct_calculated_list = detection_and_correction_word2vec(row, word2vec_model, ocr_names)\n",
    "#    else:\n",
    "#        print('passed')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae51b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec247e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(rights_correct_sorted_list)\n",
    "#print(wrongs_correct_sorted_list)\n",
    "#print(rights_correct_sorted_nosw_list)\n",
    "#print(wrongs_correct_sorted_nosw_list)\n",
    "#print(rights_correct_calculated_list)\n",
    "#print(wrongs_correct_calculated_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1026c4a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# analysis of performance on validation set word2vec\n",
    "#d = {'error_positions in detection': error_positions, 'non_error_positions in detection': non_error_positions}\n",
    "#validation_df_positions = pd.DataFrame(data = d)\n",
    "d = {'rights_correct_sorted_list': rights_correct_sorted_list, 'wrongs_correct_sorted_list': wrongs_correct_sorted_list, \\\n",
    "    'rights_correct_sorted_nosw_list': rights_correct_sorted_nosw_list, 'wrongs_correct_sorted_nosw_list': wrongs_correct_sorted_nosw_list, \\\n",
    "    'rights_correct_calculated_list': rights_correct_calculated_list, 'wrongs_correct_calculated_list': wrongs_correct_calculated_list}\n",
    "validation_df_correction_methods_word2vec = pd.DataFrame(data = d)\n",
    "\n",
    "\n",
    "#print(error_positions)\n",
    "w2v_error_positions = list_merger(error_positions)\n",
    "w2v_non_error_positions = list_merger(non_error_positions)\n",
    "w2v_right_token_positions = list_merger(right_token_positions)\n",
    "#print(w2v_error_positions)\n",
    "#print(w2v_non_error_positions)\n",
    "\n",
    "for method in ['correct_sorted', 'correct_sorted_nosw', 'correct_calculated']:\n",
    "    rights = np.array(validation_df_correction_methods_word2vec[f'rights_{method}_list'])\n",
    "    wrongs = np.array(validation_df_correction_methods_word2vec[f'wrongs_{method}_list'])\n",
    "    validation_df_correction_methods_word2vec[f'accuracy {method}'] = list(rights/(rights + wrongs))\n",
    "\n",
    "#print('errors:', s.mean(w2v_error_positions))\n",
    "#print('non errors:', s.mean(w2v_non_error_positions))\n",
    "#print('right token in prediction:', s.mean(w2v_right_token_positions))\n",
    "\n",
    "#print(validation_df_correction_methods_word2vec.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20934987",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_columns = (validation_df_correction_methods_word2vec.filter(regex='accuracy').columns).tolist()\n",
    "print('Mean accuracies for word2vec:')\n",
    "print(validation_df_correction_methods_word2vec[acc_columns].mean())\n",
    "print('Accuracies standard deviation for word2vec:')\n",
    "print(validation_df_correction_methods_word2vec[acc_columns].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7adb5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc(my_diction):  \n",
    "    total = 0 \n",
    "    my_diction = Counter(my_diction)\n",
    "    for i in my_diction:  \n",
    "        total = total + my_diction[i]  \n",
    "    for j in my_diction:  \n",
    "        my_diction[j] = (float)(my_diction[j])/total  \n",
    "    return my_diction     \n",
    "\n",
    "def process_list(my_list):\n",
    "    #my_list = list(filter(lambda x: x != 500, my_list))\n",
    "    my_list = [round(x/10)*10 for x in my_list]\n",
    "    return my_list\n",
    "                   \n",
    "                   \n",
    "        \n",
    "nep = w2v_non_error_positions\n",
    "ep = w2v_error_positions\n",
    "nep = process_list(nep)\n",
    "ep = process_list(ep)\n",
    "nep = perc(nep)\n",
    "ep = perc(ep)\n",
    "#print('nep:', nep)\n",
    "#print('ep:', ep)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.bar(nep.keys(), nep.values(), alpha = 0.75, label = 'positions of erroneous tokens', color = 'orange')\n",
    "plt.bar(ep.keys(), ep.values(), alpha = 0.75, label = 'positions of accurate tokens', color = 'green')\n",
    "plt.legend()\n",
    "plt.title('detection positions word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92140d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.hist(w2v_non_error_positions, alpha = 0.75, label = 'positions of accurate tokens')\n",
    "plt.hist(w2v_error_positions, alpha = 0.75, label = 'positions of erroneous tokens')\n",
    "plt.legend()\n",
    "plt.title('detection positions word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c852fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('95th percentile GT tokens in candidate list word2vec:', np.percentile(np.array(w2v_right_token_positions), 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c767a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f83ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c061c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1cbbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54eed4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453579a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8c716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save important information\n",
    "# BERT: error_positions, non_error_positions, right_token_positions\n",
    "#BERT_positions = [BERT_error_positions, BERT_non_error_positions, BERT_right_token_positions]\n",
    "#with open('BERT_positions.txt', 'wb') as f:\n",
    "#    pickle.dump(BERT_positions, f)\n",
    "# word2vec: error_positions, non_error_positions, right_token_positions\n",
    "w2v_positions = [w2v_error_positions, w2v_non_error_positions, w2v_right_token_positions]\n",
    "with open('word2vec_positions.txt', 'wb') as f:\n",
    "    pickle.dump(w2v_positions, f)\n",
    "# BERT validation df\n",
    "#validation_df_correction_methods_BERTje.to_csv('validation_BERT.csv')\n",
    "# word2vec validation df\n",
    "validation_df_correction_methods_word2vec.to_csv('validation_word2vec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd3868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cce06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796bec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old_sentence = \"Die [MASK] hadde viele avontuere ghemaect.\"\n",
    "#new_sentence = \"De [MASK] had vele avonturen gemaakt.\"\n",
    "#pipe = pipeline('fill-mask', model=BERT_model, tokenizer = tokenizer, top_k=5)\n",
    "#for res in pipe(new_sentence):\n",
    "#    print(res['token_str'].replace(' ', ''))\n",
    "#    print(res['score'])\n",
    "#    pipe = pipeline('fill-mask', model=BERT_model, tokenizer = tokenizer, top_k=5)\n",
    "#for res in pipe(old_sentence):\n",
    "#    print(res['token_str'].replace(' ', ''))\n",
    "#    print(res['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d84b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
