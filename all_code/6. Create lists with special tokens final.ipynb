{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bee2cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle\n",
    "import ast\n",
    "import statistics as s\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "import copy\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3004917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "#BERT_model = BertForMaskedLM.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(\"combined-320.txt\")\n",
    "# https://github.com/clips/dutchembeddings\n",
    "\n",
    "#df = pd.read_csv('preprocessed_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9ead7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org vocab word2vec count: 1442950\n",
      "org vocab BERT count: 30073\n",
      "finetuned unique words: 131823\n",
      "pretrained and finetuned vocab combined w2v: 1510565\n",
      "pretrained and finetuned vocab combined BERT: 151245\n"
     ]
    }
   ],
   "source": [
    "# list of unique words w2v model original\n",
    "vocab_word2vec = list(word2vec_model.vocab.keys())\n",
    "vocab_word2vec = list(set(vocab_word2vec))\n",
    "print('org vocab word2vec count:', len(vocab_word2vec))\n",
    "# list of unique words BERT model original\n",
    "vocab_BERT = list(tokenizer.vocab.keys())\n",
    "vocab_BERT = list(set(vocab_BERT))\n",
    "print('org vocab BERT count:', len(vocab_BERT))\n",
    "# list of unique words finetuned on\n",
    "with open('gtfortraining.txt', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "train = list(set(train))\n",
    "print('finetuned unique words:', len(train))\n",
    "# merge to create new vocabularies\n",
    "vocab_word2vec = vocab_word2vec + train\n",
    "vocab_word2vec = list(set(vocab_word2vec))\n",
    "print('pretrained and finetuned vocab combined w2v:', len(vocab_word2vec))\n",
    "vocab_BERT = vocab_BERT + train\n",
    "vocab_BERT = list(set(vocab_BERT))\n",
    "print('pretrained and finetuned vocab combined BERT:', len(vocab_BERT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb384b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe2bcd26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d87c47f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30073\n",
      "1442950\n",
      "67231\n",
      "1442950\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aca7fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asphaltbereider', 'Luitenant-Colonel', 'koortsachtigheid', 'verdaadighschrift', 'bewooneren', 'Schaker', 'gemakkajemd', 'Paisiblement', 'verstocket', 'doorsteekend', 'offerpenningen', 'zuijelijken', 'kluchtjes', 'tijdboeken', 'stekelhage', 'voorzangster', 'November', 'steppenwolf', 'Vallot', 'kugler', 'azimuthcirkels', 'oorsaecks', 'Musique', 'sla-bek', 'Abendlied', 'uitbengelen', 'irréductible', \"p'ruyck\", 'Geertruyd', 'tsire', 'gumins', 'Hingman', 'adiutorio', 'opper-bestuur', 'onwedersprekelijke', 'eyser', 'distraheren', 'wormlyders', 'twantelt', 'wegh-gesellen', 'kettingseizingen', 'zaadvochtachtig', 'scheefgeloopen', 'kegellaag', '837', \"t'vierige\", 'zengere', 'reeckenaers', 'interficiendi', 'apologia', 'uitwaazemen', 'patroonen', 'Westph', 'ontzettendste', 'veritable', 'accusationem', 'Funktionen', 'standsgelegenheid', 'zooghe', 'beringhe', 'wond-doorn', 'geloovich', 'Beschimpte', 'uytgespreydt', 'Costelijck', 'waareren', 'damassé', 'poigneerdeert', 'tooneelkapper', 'Apollodorus', 'ghelover', 'retrosyn', 'Hoogmoedige', 'Rougon', '4canten', 'mergen-tael', 'religieuslyk', 'scheurziek', 'wederkusje', 'endelinghe', 'Purm', 'rottingknop', 'voorjaarshout', 'op-winden', 'grotelyks', '9e-10e', 'waillaerds', 'strafbriefje', 'voortoogt', 'redacteur-binnenland', 'weeghen', 'Beschouwende', 'Secreet', '2675', 'brullocht', 'verbindingskring', 'verzoetene', '5Alsoo', 'antierd', 'wezenheit']\n",
      "656457\n",
      "413284\n"
     ]
    }
   ],
   "source": [
    "# create list of historical expressions\n",
    "with open('dictionaryBasedDutchDictionary_1.0.type_frequency.txt', 'r', encoding=\"utf8\") as f: # part 1 historical expressions\n",
    "    words = f.readlines()\n",
    "hist_words1 = [word[0 : word.index(\"\\t\")] for word in words]\n",
    "#print(woorden)\n",
    "\n",
    "with open('dutchCorpusBasedDictionary_1.1.tf.txt', 'r', encoding=\"utf8\") as f: # part 2 historical expressions\n",
    "    words = f.readlines()\n",
    "hist_words2 = [word[0 : word.index(\"\\t\")] for word in words]\n",
    "#print(woorden)\n",
    "\n",
    "expressions = list(set(hist_words1).union(set(hist_words2))) # all historical expressions\n",
    "\n",
    "with open('wordlist.txt', 'r', encoding=\"utf8\") as f: # modern_vocab\n",
    "    words = f.read().split('\\n')\n",
    "    #words3 = [word[0 : word.index(\"\\n\")] for word in words]\n",
    "    modern_vocab = words\n",
    "    \n",
    "# historical expressions is everything in hist_expression that is not in modern_vocab\n",
    "hist_expressions = list(set(expressions).difference(set(modern_vocab)))\n",
    "print(hist_expressions[:100])\n",
    "print(len(hist_expressions))\n",
    "print(len(modern_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fda952f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print('aen' in hist_expressions)\n",
    "print('aan' in modern_vocab)\n",
    "print('aan' in hist_expressions)\n",
    "print('aen' in modern_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56a5bcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['horen', 'vuil', 'vizier', 'militant', 'naturel', 'primetime', 'koperdraad', 'tel', 'zondagmiddag', 'plastiek']\n"
     ]
    }
   ],
   "source": [
    "# create list with homonyms\n",
    "with open('homoniemen.txt', 'r', encoding=\"utf8\") as f: # part 1 historical expressions\n",
    "    homonyms = f.readlines()\n",
    "homonyms = [word[0 : word.index(\":\")] for word in homonyms]\n",
    "print(homonyms[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a13ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list with infrequent expressions\n",
    "# = domain expressions etc. Not in original vocab, but a few times in the new training set.\n",
    "#model_vocabs = list(tokenizer.vocab.keys()) + list(model.vocab.keys())\n",
    "# find infrequent expressions\n",
    "# merge all GT text to list of words\n",
    "# count unique words\n",
    "import re\n",
    "from collections import Counter\n",
    "occurrences = Counter(word for word in train)\n",
    "occurrences_counts = Counter(occ for occ in occurrences.values())\n",
    "infrequent_expressions = [key for key , value in occurrences.items() if value == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a70208c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(tokens_list):\n",
    "    tokens_list = [re.sub(r'[^\\w\\d\\s\\-]+', '', x) for x in tokens_list]\n",
    "    tokens_list = [x.lower() for x in tokens_list]\n",
    "    tokens_list = [re.sub(r'[^a-z]+', '', x) for x in tokens_list]\n",
    "    tokens_list = list(filter(None, tokens_list))\n",
    "    return tokens_list\n",
    "\n",
    "# make sure all lists are in lowercase and without punctuation except for hyphens\n",
    "# homonyms\n",
    "homonyms = process(homonyms)\n",
    "# BERTs vocab:\n",
    "vocab_BERT = process(vocab_BERT)\n",
    "# word2vecs vocab:\n",
    "vocab_word2vec = process(vocab_word2vec)\n",
    "# historical expressions\n",
    "hist_expressions = process(hist_expressions)\n",
    "# the modern vocabulary\n",
    "modern_vocab = process(modern_vocab)\n",
    "# infrequent expressions\n",
    "infrequent_expressions = process(infrequent_expressions)\n",
    "# the dictionary for the baseline model\n",
    "dictionary = process(modern_vocab + hist_expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c35a851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save al lists and later unpack:\n",
    "#all_lists_tokens = [homonyms, vocab_BERT, vocab_word2vec, hist_expressions, modern_vocab, infrequent_expressions, dictionary]\n",
    "#all_lists_tokens = [homonyms, hist_expressions, modern_vocab, infrequent_expressions, dictionary]\n",
    "\n",
    "with open('homonyms.txt', 'wb') as f:\n",
    "    pickle.dump(homonyms, f)\n",
    "with open('vocab_BERT.txt', 'wb') as f:\n",
    "    pickle.dump(vocab_BERT, f)\n",
    "with open('vocab_word2vec.txt', 'wb') as f:\n",
    "    pickle.dump(vocab_word2vec, f)\n",
    "with open('hist_expressions.txt', 'wb') as f:\n",
    "    pickle.dump(hist_expressions, f)\n",
    "with open('infrequent_expressions.txt', 'wb') as f:\n",
    "    pickle.dump(infrequent_expressions, f)\n",
    "with open('dictionary.txt', 'wb') as f:\n",
    "    pickle.dump(dictionary, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#with open('all_lists_tokens.txt', 'wb') as f:\n",
    "#    pickle.dump(all_lists_tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efe5b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa460019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
