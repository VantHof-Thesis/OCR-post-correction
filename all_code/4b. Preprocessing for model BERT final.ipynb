{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import transformers\n",
    "#print(transformers.__version__)\n",
    "#print(torch.__version__)\n",
    "#print(gensim.__version__)\n",
    "# docs needed = preprocessed_df, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/arch/Debian10/EB_production/2020/software/IPython/7.13.0-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3062: DtypeWarning: Columns (20,21,32,33) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('preprocessed_df.csv')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'identifier', 'gt text', 'ocr text', 'CER', 'WER',\n",
      "       'WER (order independent)', 'dictionary lookup gt',\n",
      "       'dictionary lookup ocr', 'jaccard_coefficient',\n",
      "       'levenshtein_dist_normalized', 'source', 'word count gt',\n",
      "       'word count ocr', 'year', 'century', 'old index', 'gt text org',\n",
      "       'ocr text org', 'set', 'gt sentences matched', 'ocr sentences matched',\n",
      "       'CER matched sentences', 'WER matched sentences',\n",
      "       'avg sentence length gt (fuzzy matched)',\n",
      "       'avg sentence length ocr (fuzzy matched)',\n",
      "       'max sentence length gt (fuzzy matched)',\n",
      "       'max sentence length ocr (fuzzy matched)',\n",
      "       'sentences gt (fuzzy matched)', 'sentences ocr (fuzzy matched)',\n",
      "       'word count gt (fuzzy matched)', 'word count ocr (fuzzy matched)',\n",
      "       'aligned_GT_sentences', 'aligned_OCR_sentences', 'good_alignments',\n",
      "       'bad_alignments', 'good alignments percentage', 'longest_streak',\n",
      "       'avg_longest_streaks', 'avg_total_missing_words',\n",
      "       'avg_perc_missing_words '],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# select relevant columns\n",
    "print(df.columns)\n",
    "#df = df[['identifier', 'gt text org', 'ocr text org', 'gt text', 'ocr text', 'source', 'century', 'gt sentences matched', 'aligned_GT_sentences', 'aligned_OCR_sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>identifier</th>\n",
       "      <th>gt text</th>\n",
       "      <th>ocr text</th>\n",
       "      <th>CER</th>\n",
       "      <th>WER</th>\n",
       "      <th>WER (order independent)</th>\n",
       "      <th>dictionary lookup gt</th>\n",
       "      <th>dictionary lookup ocr</th>\n",
       "      <th>jaccard_coefficient</th>\n",
       "      <th>...</th>\n",
       "      <th>word count ocr (fuzzy matched)</th>\n",
       "      <th>aligned_GT_sentences</th>\n",
       "      <th>aligned_OCR_sentences</th>\n",
       "      <th>good_alignments</th>\n",
       "      <th>bad_alignments</th>\n",
       "      <th>good alignments percentage</th>\n",
       "      <th>longest_streak</th>\n",
       "      <th>avg_longest_streaks</th>\n",
       "      <th>avg_total_missing_words</th>\n",
       "      <th>avg_perc_missing_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38797</th>\n",
       "      <td>6296</td>\n",
       "      <td>ddd.010680710.mpeg21.a0006</td>\n",
       "      <td>vyt venetien den 7 dito</td>\n",
       "      <td>vyt venetien den 7 dito</td>\n",
       "      <td>34.42</td>\n",
       "      <td>81.25</td>\n",
       "      <td>78.75</td>\n",
       "      <td>92.65</td>\n",
       "      <td>48.61</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[['vyt', 'venetien', 'den', '7', 'dito']]</td>\n",
       "      <td>[['vyt', 'venetien', 'den', '7', 'dito']]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                  identifier                  gt text  \\\n",
       "38797        6296  ddd.010680710.mpeg21.a0006  vyt venetien den 7 dito   \n",
       "\n",
       "                      ocr text    CER    WER  WER (order independent)  \\\n",
       "38797  vyt venetien den 7 dito  34.42  81.25                    78.75   \n",
       "\n",
       "       dictionary lookup gt  dictionary lookup ocr  jaccard_coefficient  ...  \\\n",
       "38797                 92.65                  48.61             0.126984  ...   \n",
       "\n",
       "       word count ocr (fuzzy matched)  \\\n",
       "38797                             4.0   \n",
       "\n",
       "                            aligned_GT_sentences  \\\n",
       "38797  [['vyt', 'venetien', 'den', '7', 'dito']]   \n",
       "\n",
       "                           aligned_OCR_sentences  good_alignments  \\\n",
       "38797  [['vyt', 'venetien', 'den', '7', 'dito']]              1.0   \n",
       "\n",
       "      bad_alignments good alignments percentage  longest_streak  \\\n",
       "38797            0.0                        1.0             0.0   \n",
       "\n",
       "      avg_longest_streaks avg_total_missing_words avg_perc_missing_words   \n",
       "38797                 0.0                     0.0                     0.0  \n",
       "\n",
       "[1 rows x 41 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess for models\n",
    "def replace_num(text, character):\n",
    "    try:\n",
    "        numbers = re.findall(r'\\d+', text) \n",
    "        res = list(map(int, numbers))\n",
    "        for numText in res:\n",
    "            text = text.replace(str(numText), character)\n",
    "    except:\n",
    "        text = ''\n",
    "    return text\n",
    "\n",
    "def replace_proper_nouns(text, character, names): \n",
    "    try:\n",
    "        res = list(map(str, names))\n",
    "        for nounText in res:\n",
    "            text = text.replace(str(nounText), character)\n",
    "    except:\n",
    "        text = ''\n",
    "    return text\n",
    "    \n",
    "with open(\"gt_names.txt\", \"rb\") as fp:   # Unpickling\n",
    "    gt_names_org = pickle.load(fp)\n",
    "    gt_names = []\n",
    "    for name in gt_names_org:\n",
    "        if len(name) >= 5:\n",
    "            gt_names.append(name)\n",
    "\n",
    "\n",
    "df['gt for training'] = df['gt text']\n",
    "# replace numbers\n",
    "df['gt for training'] = df['gt for training'].apply(lambda x: replace_num(x, '%NUMBER%'))\n",
    "# replace proper nouns\n",
    "df['gt for training'] = df['gt for training'].apply(lambda x: replace_proper_nouns(x, \"%NNP%\", gt_names))\n",
    "\n",
    "train = df[df['set']=='train']\n",
    "train = '.'.join(list(train['gt for training']))\n",
    "train_list = (train.replace('.', '')).split(' ')\n",
    "with open('gtfortraining.txt', 'wb') as f:\n",
    "    pickle.dump(train_list, f)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['gt for training'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parts for cross-validation, train, validation oand test set\n",
    "#df_17th = df[df['century'] == '1600s']\n",
    "#df_18th = df[df['century'] == '1700s']\n",
    "#df_19th = df[df['century'] == '1800s']\n",
    "#df_20th = df[df['century'] == '1900s']\n",
    "\n",
    "#datasets = [df_17th, df_18th, df_19th, df_20th]\n",
    "# create a list of train, validation and test set\n",
    "\n",
    "#train = pd.DataFrame()\n",
    "#val = pd.DataFrame()\n",
    "#test = pd.DataFrame()\n",
    "#for dataset in datasets:\n",
    "#    splits = np.array_split(dataset, 5)\n",
    "#    train_sub = splits[:3]\n",
    "#    train_sub = pd.concat(train_sub)\n",
    "#    val_sub = splits[3]\n",
    "#    test_sub = splits[4]\n",
    "#    train = pd.concat([train, train_sub])\n",
    "#   val = pd.concat([val, val_sub])\n",
    "#    test = pd.concat([test, test_sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine_tuning the models\n",
    "\n",
    "word2vec_models = []\n",
    "BERT_models = []\n",
    "\n",
    "def finetune_word2vec(train, window=5):\n",
    "    sentences = train.split('.')\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sentences = [tokenizer.tokenize(i) for i in sentences]\n",
    "    total_examples = len(sentences)\n",
    "    \n",
    "    model_w2v = Word2Vec(size=160, min_count=1, window=window)\n",
    "    model_w2v.build_vocab(sentences)\n",
    "    total_examples = model_w2v.corpus_count\n",
    "    model = KeyedVectors.load_word2vec_format(r\"combined-160.txt\", binary=False)\n",
    "    model_w2v.build_vocab([list(model.vocab.keys())], update=True)\n",
    "    model_w2v.intersect_word2vec_format(r\"combined-160.txt\", binary=False, lockf=1.0)\n",
    "    model_w2v.train(sentences, total_examples=total_examples, epochs=model_w2v.iter)\n",
    "    return model_w2v\n",
    "    \n",
    "def finetune_BERT(train):\n",
    "    \n",
    "    class TrainingDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings):\n",
    "            self.encodings = encodings\n",
    "        def __getitem__(self, idx):\n",
    "            return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        def __len__(self):\n",
    "            return len(self.encodings.input_ids)\n",
    "    \n",
    "    text = train.split('.')\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "    model = BertForMaskedLM.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "    # BERT tuning code from https://github.com/jamescalam/transformers/blob/main/course/training/03_mlm_training.ipynb\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "    inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "    # create random array of floats with equal dimensions to input_ids tensor\n",
    "    rand = torch.rand(inputs.input_ids.shape)\n",
    "    # create mask array\n",
    "    mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "           (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
    "    # we take take the indices of each True value, within each individual vector.\n",
    "    selection = []\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        selection.append(\n",
    "            torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        )\n",
    "    # apply these indices to each respective row in input_ids, assigning each of the values at these indices as 103.\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        inputs.input_ids[i, selection[i]] = 103\n",
    "    # initialize data\n",
    "    dataset = TrainingDataset(inputs)\n",
    "    \n",
    "    # start training loop\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print('Cuda available?', torch.cuda.is_available())\n",
    "    # and move our model over to the selected device\n",
    "    model.to(device)\n",
    "    # activate training mode\n",
    "    model.train()\n",
    "    # initialize optimizer\n",
    "    optim = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    epochs = 2\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # setup loop with TQDM and dataloader\n",
    "        loop = tqdm(loader, leave=True)\n",
    "        for batch in loop:\n",
    "            # initialize calculated gradients (from prev step)\n",
    "            optim.zero_grad()\n",
    "            # pull all tensor batches required for training\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            #labels = batch['labels']\n",
    "            # process\n",
    "            outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "            # extract loss\n",
    "            loss = outputs.loss\n",
    "            # calculate loss for every parameter that needs grad update\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optim.step()\n",
    "            # print relevant info to progress bar\n",
    "            loop.set_description(f'Epoch {epoch}')\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERTje.\n",
      "Cuda available? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23113 [00:00<?, ?it/s]<ipython-input-10-44f777c1c276>:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0:   1%|          | 182/23113 [01:29<3:08:40,  2.03it/s, loss=0.0336]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-bf11f7bea3d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#print('Word2vec trained.')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training BERTje.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mBERT_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mfinetune_BERT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BERTje trained.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-44f777c1c276>\u001b[0m in \u001b[0;36mfinetune_BERT\u001b[0;34m(train)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m# calculate loss for every parameter that needs grad update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0;31m# update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train models\n",
    "train = df[df['set']=='train']\n",
    "train = '.'.join(list(train['gt for training']))\n",
    "#print('Training word2vec.')\n",
    "#word2vec_model = finetune_word2vec(train)\n",
    "#print('Word2vec trained.')\n",
    "print('Training BERTje.')\n",
    "BERT_model= finetune_BERT(train)\n",
    "print('BERTje trained.')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec_model.save(\"word2vec_finetuned.model\", map_location=\"cuda:0\")\n",
    "#word2vec_model = Word2Vec.load(\"word2vec_finetuned.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(BERT_model, 'BERT_finetuned.pt')\n",
    "#BERT_model = torch.load('BERT_finetuned.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
