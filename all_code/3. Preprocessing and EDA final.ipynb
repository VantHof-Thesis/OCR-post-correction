{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ddce3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import difflib\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "import subprocess\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.dammit import EncodingDetector\n",
    "import statistics as s\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58b4ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_centuries.csv')\n",
    "df['old index'] = df.index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e264811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38798\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7cbbc017",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = shuffle(df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d65588f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to try a smaller tryout dataset\n",
    "df_1600s = df.loc[df['century'] == '1600s']\n",
    "df_1700s = df.loc[df['century'] == '1700s']\n",
    "df_1800s = df.loc[df['century'] == '1800s']\n",
    "df_1900s = df.loc[df['century'] == '1900s']\n",
    "#df = pd.concat([df_1600s, df_1700s, df_1800s, df_1900s]) small set turned off (else: use .head()). Whole set used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f49c197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removal:\n",
      "Total amount of pages 1600s gt: 75242\n",
      "Total amount of pages 1600s ocr: 73322\n",
      "Total amount of pages 1700s gt: 50695\n",
      "Total amount of pages 1700s ocr: 52902\n",
      "Total amount of pages 1800s gt: 226653\n",
      "Total amount of pages 1800s ocr: 226296\n",
      "Total amount of pages 1900s gt: 25957\n",
      "Total amount of pages 1900s ocr: 25488\n",
      "Total pages gt: 378547\n",
      "Total pages ocr: 378008\n"
     ]
    }
   ],
   "source": [
    "total_pages_gt = 0\n",
    "total_pages_ocr = 0\n",
    "\n",
    "print('Before removal:')\n",
    "for century in ['1600s', '1700s', '1800s', '1900s']:\n",
    "    pages_gt = round(len(''.join(list(df.loc[df['century'] == century]['gt text'])))/1361)\n",
    "    pages_ocr = round(len(''.join(list(df.loc[df['century'] == century]['ocr text'])))/1361)\n",
    "    total_pages_gt += pages_gt\n",
    "    total_pages_ocr += pages_ocr\n",
    "    print(f'Total amount of pages {century} gt:', pages_gt)\n",
    "    print(f'Total amount of pages {century} ocr:', pages_ocr)\n",
    "\n",
    "print('Total pages gt:', total_pages_gt)\n",
    "print('Total pages ocr:', total_pages_ocr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "362818f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents whole dataset: 38798\n",
      "total pages if on average 0.5 page per document: 19399.0\n",
      "dataset: df_1600s\n",
      "documents: 34810\n",
      "percentage of total dataset: 0.8972111964534254\n",
      "assigned total pages: 4849.75\n",
      "pages per document: 0.13932059752944556\n",
      "dataset: df_1700s\n",
      "documents: 1646\n",
      "percentage of total dataset: 0.04242486726119903\n",
      "assigned total pages: 4849.75\n",
      "pages per document: 2.94638517618469\n",
      "dataset: df_1800s\n",
      "documents: 631\n",
      "percentage of total dataset: 0.016263724934274963\n",
      "assigned total pages: 4849.75\n",
      "pages per document: 7.68581616481775\n",
      "dataset: df_1900s\n",
      "documents: 1711\n",
      "percentage of total dataset: 0.04410021135110057\n",
      "assigned total pages: 4849.75\n",
      "pages per document: 2.8344535359438923\n",
      "[0.13932059752944556, 2.94638517618469, 7.68581616481775, 2.8344535359438923]\n"
     ]
    }
   ],
   "source": [
    "def namestr(obj, namespace):\n",
    "    return [name for name in namespace if namespace[name] is obj]\n",
    "\n",
    "print('documents whole dataset:', len(df))\n",
    "print('total pages if on average 0.5 page per document:', len(df)/2)\n",
    "\n",
    "pages_per_document_list = []\n",
    "\n",
    "for century in [df_1600s, df_1700s, df_1800s, df_1900s]:\n",
    "    print('dataset:', namestr(century, globals())[0])\n",
    "    print('documents:', len(century))\n",
    "    print('percentage of total dataset:', len(century) / len(df))\n",
    "    #print('assigned total pages:', (len(df)/2) * (len(century) / len(df)))\n",
    "    print('assigned total pages:', (len(df)/2) / 4)\n",
    "    pages_per_document = ((len(df)/2) / 4) / len(century)\n",
    "    pages_per_document_list.append(pages_per_document)\n",
    "    print('pages per document:', pages_per_document)\n",
    "    \n",
    "print(pages_per_document_list)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f84a913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of pages 1600s gt: 1757\n",
      "Total amount of pages 1600s ocr: 2339\n",
      "Total amount of pages 1700s gt: 2029\n",
      "Total amount of pages 1700s ocr: 1864\n",
      "Total amount of pages 1800s gt: 4683\n",
      "Total amount of pages 1800s ocr: 4692\n",
      "Total amount of pages 1900s gt: 4491\n",
      "Total amount of pages 1900s ocr: 4495\n",
      "Total pages gt: 12960\n",
      "Total pages ocr: 13390\n"
     ]
    }
   ],
   "source": [
    "reduced = []\n",
    "reduced_pages = []\n",
    "total_pages = []\n",
    "\n",
    "new_gt_texts = []\n",
    "new_ocr_texts = []\n",
    "\n",
    "# reduce size of books\n",
    "def reduce_size(text, century, ocr_or_gt, pages_per_document_list):\n",
    "    if century == '1600s':\n",
    "        pages = pages_per_document_list[0]\n",
    "    elif century == '1700s':\n",
    "        pages = pages_per_document_list[1]\n",
    "    elif century == '1800s':\n",
    "        pages = pages_per_document_list[2]\n",
    "    elif century == '1900s':\n",
    "        pages = pages_per_document_list[3]\n",
    "    pages = pages # pick amount of pages\n",
    "    one_page = 1361 # amount of characters on page\n",
    "    characters =  one_page * pages\n",
    "    characters = round(characters)\n",
    "    #print('test:', text[0:characters])\n",
    "    if len(text) > characters:\n",
    "        first_pages = text[0:characters].split(\".\")\n",
    "        #print('startthing:', first_pages)\n",
    "        first_pages = '.'.join(first_pages[:-1])\n",
    "        #print('first pages reduced:', first_pages)\n",
    "        #reduced = 1\n",
    "        if ocr_or_gt == 'ocr':\n",
    "            total_pages_ocr = len(text) / one_page\n",
    "            total_pages.append(total_pages_ocr)\n",
    "            reduced_pages_ocr = total_pages_ocr - pages\n",
    "            reduced_pages.append(reduced_pages_ocr)\n",
    "            reduced.append(1)\n",
    "    else:\n",
    "        \n",
    "        first_pages = text\n",
    "        #reduced = 0\n",
    "        if ocr_or_gt == 'ocr':\n",
    "            total_pages_ocr = len(text) / one_page\n",
    "            total_pages.append(total_pages_ocr)\n",
    "            reduced_pages.append(0)\n",
    "            reduced.append(0)\n",
    "        #print('first pages not reduced:', first_pages)\n",
    "    #print(type(first_pages))\n",
    "    #print('end:', first_pages, reduced)\n",
    "    \n",
    "    if ocr_or_gt == 'ocr':\n",
    "        new_ocr_texts.append(first_pages)\n",
    "    elif ocr_or_gt == 'gt':\n",
    "        new_gt_texts.append(first_pages)\n",
    "\n",
    "df['gt text org'] = df['gt text']\n",
    "df['ocr text org'] = df['ocr text'] \n",
    "\n",
    "#df['gt text'], df['reduced'] = df['gt text'].apply(lambda x: reduce_size(x))\n",
    "#df['ocr text short'], df['reduced'] = df['ocr text'].apply(lambda x: reduce_size(x))\n",
    "#df['gt text org'] = df['gt text']\n",
    "#df['ocr text org'] = df['ocr text']\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    reduce_size(row['gt text'], row['century'], 'gt', pages_per_document_list)\n",
    "    reduce_size(row['ocr text'], row['century'], 'ocr', pages_per_document_list)\n",
    "\n",
    "df['gt text'] = new_gt_texts\n",
    "df['ocr text'] = new_ocr_texts\n",
    "    \n",
    "    \n",
    "total_pages_gt = 0\n",
    "total_pages_ocr = 0\n",
    "    \n",
    "for century in ['1600s', '1700s', '1800s', '1900s']:\n",
    "    pages_gt = round(len(''.join(list(df.loc[df['century'] == century]['gt text'])))/1361)\n",
    "    pages_ocr = round(len(''.join(list(df.loc[df['century'] == century]['ocr text'])))/1361)\n",
    "    total_pages_gt += pages_gt\n",
    "    total_pages_ocr += pages_ocr\n",
    "    print(f'Total amount of pages {century} gt:', pages_gt)\n",
    "    print(f'Total amount of pages {century} ocr:', pages_ocr)\n",
    "\n",
    "print('Total pages gt:', total_pages_gt)\n",
    "print('Total pages ocr:', total_pages_ocr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c5e48eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same texts: 4264\n",
      "different texts: 34534\n"
     ]
    }
   ],
   "source": [
    "# check if ocr text and gt text never got mixed up\n",
    "same_texts = 0\n",
    "different_texts = 0\n",
    "list_gt = list(df['gt text'])\n",
    "list_ocr = list(df['ocr text'])\n",
    "for i in range(len(list_gt)):\n",
    "    if list_gt[i] == list_ocr[i]:\n",
    "        same_texts += 1\n",
    "    elif list_gt[i] != list_ocr[i]:\n",
    "        different_texts +=1\n",
    "# the texts should in the vast majority of cases be different\n",
    "print('same texts:', same_texts)\n",
    "print('different texts:', different_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a02d6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of docs reduced: 0.9254085262126914\n",
      "Percentage of dataset removed: 0.9577041808251007\n",
      "sum:\n",
      "            reduced  reduced pages   docs  percentage reduced\n",
      "source                                                       \n",
      "Meertens      33756   68509.747841  34808            0.969777\n",
      "anp              25       8.238588    204            0.122549\n",
      "books             5       8.256318   1567            0.003191\n",
      "dbnl            219  269326.544521    219            1.000000\n",
      "newspapers     1899   24167.392482   2000            0.949500\n",
      "mean:\n",
      "            total pages  reduced pages\n",
      "source                                \n",
      "Meertens       2.106047       1.968218\n",
      "anp            1.746211       0.040385\n",
      "books          1.051324       0.005269\n",
      "dbnl        1236.968000    1229.801573\n",
      "newspapers    15.900721      12.083696\n",
      "Percentage of dataset outside of books removed pages: 0.7500430728965978\n",
      "Percentage of dataset outside of books reduced documents: 0.9249850955182871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvanthof/.local/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# percentage of documents reduced\n",
    "print('Percentage of docs reduced:', reduced.count(1)/len(reduced))\n",
    "# percentage of dataset removed\n",
    "print('Percentage of dataset removed:', sum(reduced_pages)/sum(total_pages))\n",
    "\n",
    "\n",
    "reduced_count_df = pd.DataFrame({'reduced': reduced, 'source': df['source'], 'reduced pages': reduced_pages, 'total pages': total_pages, 'docs': [1]*len(df)})\n",
    "sums = reduced_count_df[['reduced', 'source', 'reduced pages', 'docs']].groupby('source').sum()\n",
    "means = reduced_count_df[['total pages', 'source', 'reduced pages']].groupby('source').mean()\n",
    "sums['percentage reduced'] = sums['reduced'] / sums['docs']\n",
    "print('sum:')\n",
    "print(sums)\n",
    "print('mean:')\n",
    "print(means)\n",
    "without_books = reduced_count_df.loc[reduced_count_df['source'] != 'dbnl']\n",
    "without_books_removed = sum(list(without_books['reduced']))/ sum(list(without_books['docs']))\n",
    "without_books['percentage removed'] = without_books['reduced pages'] / without_books['total pages']\n",
    "print('Percentage of dataset outside of books removed pages:', s.mean(list(without_books['percentage removed'])))\n",
    "print('Percentage of dataset outside of books reduced documents:', without_books_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f78618",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf1dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2969cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ce01bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b794a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_17th = df[df['century'] == '1600s']\n",
    "df_18th = df[df['century'] == '1700s']\n",
    "df_19th = df[df['century'] == '1800s']\n",
    "df_20th = df[df['century'] == '1900s']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "813a4f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets = [df_17th, df_18th, df_19th, df_20th] #datasets[0] == df_17th etc.\n",
    "#datasets = [df_17th.head(), df_18th.head(), df_19th.head(), df_20th.head()]\n",
    "#print(len(df_17th), len(df_18th),len(df_19th),len(df_20th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd9b2cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'identifier', 'gt text', 'ocr text', 'CER', 'WER',\n",
      "       'WER (order independent)', 'dictionary lookup gt',\n",
      "       'dictionary lookup ocr', 'jaccard_coefficient',\n",
      "       'levenshtein_dist_normalized', 'source', 'word count gt',\n",
      "       'word count ocr', 'year', 'century', 'old index', 'gt text org',\n",
      "       'ocr text org'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# column selection\n",
    "print(df.columns)\n",
    "#df = df[['identifier', 'gt text', 'ocr text', 'CER', 'WER', 'source', 'word count gt',\n",
    "#       'word count ocr', 'century']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e67a69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89000bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = re.sub(\"[^a-zA-Z0-9-. ]+\", '', text) # remove punctuation except for hyphens and dots\n",
    "    text = text.lower() # lowercase\n",
    "    return text\n",
    "\n",
    "# preprocess identifier\n",
    "df['identifier'] = df['identifier'].apply(lambda x: str(x))\n",
    "df['identifier'] = df['identifier'].apply(lambda x: x.replace('_', '.'))\n",
    "df['identifier'] = df['identifier'].apply(lambda x: x.replace(':', '.'))\n",
    "#df['identifier'] = df['identifier'].apply(lambda x: x + '_None')\n",
    "# preprocess OCR and GT\n",
    "df['gt text'] = df['gt text'].apply(lambda x: preprocessing(x))\n",
    "df['ocr text'] = df['ocr text'].apply(lambda x: preprocessing(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eac1b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parts for cross-validation, train, validation oand test set\n",
    "df_17th = df[df['century'] == '1600s']\n",
    "df_18th = df[df['century'] == '1700s']\n",
    "df_19th = df[df['century'] == '1800s']\n",
    "df_20th = df[df['century'] == '1900s']\n",
    "\n",
    "datasets = [df_17th, df_18th, df_19th, df_20th]\n",
    "# create a list of train, validation and test set\n",
    "\n",
    "train = pd.DataFrame()\n",
    "val = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "for dataset in datasets:\n",
    "    splits = np.array_split(dataset, 5)\n",
    "    train_sub = splits[:3]\n",
    "    train_sub = pd.concat(train_sub)\n",
    "    val_sub = splits[3]\n",
    "    test_sub = splits[4]\n",
    "    train = pd.concat([train, train_sub])\n",
    "    val = pd.concat([val, val_sub])\n",
    "    test = pd.concat([test, test_sub])\n",
    "\n",
    "train_indices = train.index.values.tolist() \n",
    "val_indices = val.index.values.tolist()  \n",
    "test_indices = test.index.values.tolist()  \n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index in train_indices:\n",
    "        df.at[index, 'set'] = 'train'\n",
    "    elif index in val_indices:\n",
    "        df.at[index, 'set'] = 'val'\n",
    "    elif index in test_indices:\n",
    "        df.at[index, 'set'] = 'test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2b2e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8ea44b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_matching(row): # create files with matched sentences\n",
    "    #save_path = r\"C:\\Users\\Gebruiker\\Desktop\\Thesis\\Documenten\\Documenten\"\n",
    "    \n",
    "    ID = row['identifier']\n",
    "    page = 'None'\n",
    "    \n",
    "    # split GT and OCR-output of each row into sentences (based on full stops).\n",
    "    gt_sentences = row['gt text'].split(\".\")\n",
    "    ocr_sentences = row['ocr text'].split(\".\")\n",
    "\n",
    "    match_list = []\n",
    "\n",
    "    for sentence in gt_sentences:\n",
    "        for match in ocr_sentences:\n",
    "            if SequenceMatcher(None, sentence, match).ratio() > 0.75:\n",
    "                match_list.extend([[sentence, match]])\n",
    "                \n",
    "    matched_gt_sentences = []\n",
    "    matched_ocr_sentences = []\n",
    "\n",
    "    for sentence in match_list:\n",
    "        if len(sentence[0]) or len(sentence[1]) > 0:\n",
    "            matched_gt_sentences.append(sentence[0])\n",
    "            matched_ocr_sentences.append(sentence[1])\n",
    "    \n",
    "    # .join() with lists\n",
    "    separator = '.'\n",
    "    doc_GT = separator.join(matched_gt_sentences)\n",
    "    doc_OCR = separator.join(matched_ocr_sentences)\n",
    "    \n",
    "    #return matched_gt_sentences, matched_ocr_sentences\n",
    "    return doc_GT, doc_OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cfeac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b4e4f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "jar_file = \"ocrevalUAtion-1.3.4-jar-with-dependencies.jar\"\n",
    "\n",
    "def evaluation(index, row, doc_GT, doc_OCR):\n",
    "    ID = row['identifier']\n",
    "    page = 'None'\n",
    "    \n",
    "    doc_OCR = doc_OCR.replace('.', '')\n",
    "    doc_OCR = re.sub(' +', ' ', doc_OCR)\n",
    "    filename_ocr = f\"{ID}_{page}_OCR.txt\"\n",
    "    #file_ocr = open(os.path.join(save_path, filename),\"w+\", encoding=\"utf-8\")\n",
    "    file_ocr = open(filename_ocr,\"w+\", encoding=\"utf-8\")\n",
    "    file_ocr.write(doc_OCR)\n",
    "    file_ocr.close()\n",
    "    \n",
    "    doc_GT = doc_GT.replace('.', '')\n",
    "    doc_GT = re.sub(' +', ' ', doc_GT)\n",
    "    filename_gt = f\"{ID}_{page}_GT.txt\"\n",
    "    #file_gt = open(os.path.join(save_path, filename),\"w+\", encoding=\"utf-8\")\n",
    "    file_gt = open(filename_gt,\"w+\", encoding=\"utf-8\")\n",
    "    file_gt.write(doc_GT)\n",
    "    file_gt.close()\n",
    "    \n",
    "    #output = ID + '_' + page + \".html\"\n",
    "    output = f\"{ID}_{page}.html\"\n",
    "    \n",
    "    #process = subprocess.call(\"/home/nvanthof/jdk-16.0.1/bin/java -cp \" + jar_file  + \" eu.digitisation.Main -gt \" + filename_gt + \" -ocr \"+ filename_ocr +\" -o \" + output + \"\")\n",
    "    #os.system(\"/home/nvanthof/jdk-16.0.1/bin/java -cp /home/nvanthof/ocrevalUAtion-1.3.4-jar-with-dependencies.jar eu.digitisation.Main -gt /home/nvanthof/ddd.010728187.mpeg21.a0005_None_GT.txt -ocr /home/nvanthof/ddd.010728187.mpeg21.a0005_None_OCR.txt  -o /home/nvanthof/OUTPUT2.html\")\n",
    "    command = f\"/home/nvanthof/jdk-16.0.1/bin/java -cp /home/nvanthof/ocrevalUAtion-1.3.4-jar-with-dependencies.jar eu.digitisation.Main -gt /home/nvanthof/{filename_gt} -ocr /home/nvanthof/{filename_ocr}  -o /home/nvanthof/{output}\"\n",
    "    #command = f\"/usr/bin/java -cp /home/nynkegpu/ocrevalUAtion-1.3.4-jar-with-dependencies.jar eu.digitisation.Main -gt /home/nynkegpu/{filename_gt} -ocr /home/nynkegpu/{filename_ocr}  -o /home/nynkegpu/{output}\"\n",
    "    os.system(command)\n",
    "    sleep(5)\n",
    "    \n",
    "    soup = BeautifulSoup(open(output, encoding='utf-8'))\n",
    "    table = soup.find(\"table\", attrs={'border': '1'})\n",
    "    # Split the filename, and extract the identifier and pagenr together as identifier \n",
    "    # Find the first table (this is the table in which the scores are stored)\n",
    "    # Find the tags in which 'CER', 'WER', and 'WER (order independent)' are stored and take the next tag to get the score \n",
    "    cer = table.find('td', text='CER')\n",
    "    cerScore = cer.findNext('td')\n",
    "    wer = table.find('td', text='WER')\n",
    "    werScore = wer.findNext('td')\n",
    "    werOI = table.find('td', text='WER (order independent)')\n",
    "    werOIScore = werOI.findNext('td')\n",
    "    \n",
    "    os.remove(filename_gt)\n",
    "    os.remove(filename_ocr)\n",
    "    os.remove(output)\n",
    "    return float(cerScore.text), float(werScore.text)   \n",
    "    \n",
    "    return cerScore.text, werScore.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829fc0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bfc4636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n"
     ]
    }
   ],
   "source": [
    "#for index, row in df.loc[df['set'].isin(['train', 'val'])].iterrows():\n",
    "for index, row in df.iterrows():\n",
    "    #print(index)\n",
    "    if index%1000 == 0:\n",
    "        print(index)\n",
    "    doc_GT, doc_OCR = fuzzy_matching(row)\n",
    "    #GT_matched_docs_list.append(matched_gt_sentences)\n",
    "    #OCR_matched_docs_list.append(matched_ocr_sentences)\n",
    "    if (row['set'] == 'val') or (row['set'] == 'test'):\n",
    "        df.at[index, 'gt sentences matched'], df.at[index, 'ocr sentences matched'] = doc_GT, doc_OCR\n",
    "    if row['set'] == 'test':\n",
    "        df.at[index, 'CER matched sentences'], df.at[index, 'WER matched sentences'] = evaluation(index, row, doc_GT, doc_OCR)\n",
    "\n",
    "#for index, row in df.loc[df['set'].isin(['test'])].iterrows():\n",
    "    #print(index)\n",
    "#    if index%1000 == 0:\n",
    "#        print(index)\n",
    "#    doc_GT, doc_OCR = fuzzy_matching(row)\n",
    "    #GT_matched_docs_list.append(matched_gt_sentences)\n",
    "    #OCR_matched_docs_list.append(matched_ocr_sentences)\n",
    "#    df.at[index, 'gt sentences matched'], df.at[index, 'ocr sentences matched'] = doc_GT, doc_OCR\n",
    "    #df.at[index, 'CER matched sentences'], df.at[index, 'WER matched sentences'] = evaluation(index, row, doc_GT, doc_OCR)\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cd06360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count sentences (longer than 4 words) in GT and OCR\n",
    "# count words in sentences longer than 4 words\n",
    "\n",
    "#meta_df = pd.DataFrame(columns=['identifier', 'sentences gt', 'sentences ocr', 'avg sentence length gt', ' avg sentence length ocr'])\n",
    "meta_df = pd.DataFrame()\n",
    "\n",
    "def sentence_count(row):\n",
    "# count of sentences in GT >= 4 words\n",
    "    sentences_gt = 0\n",
    "    words_GT_sentences = []\n",
    "    #characters_GT_sentences = []\n",
    "    for k in row['gt text'].split('.'):\n",
    "        k = [word for word in k.split(' ') if len(word) > 1]\n",
    "        if len(k) >= 4:\n",
    "            sentences_gt += 1\n",
    "            words_GT_sentences.append(len(k))\n",
    "            #characters_GT_sentences.append(len(' '.join(k)))\n",
    "    # count of sentences in OCR >= 4 words\n",
    "    sentences_ocr = 0\n",
    "    words_OCR_sentences = []\n",
    "    #characters_OCR_sentences = []\n",
    "    for k in row['ocr text'].split('.'):\n",
    "        k = [word for word in k.split(' ') if len(word) > 1]\n",
    "        if len(k) >= 4:\n",
    "            sentences_ocr += 1\n",
    "            words_OCR_sentences.append(len(k))\n",
    "            #characters_OCR_sentences.append(len(' '.join(k)))\n",
    "    if len(words_GT_sentences) == 0:\n",
    "        words_GT_sentences.append(0)\n",
    "    if len(words_OCR_sentences) == 0:\n",
    "        words_OCR_sentences.append(0)\n",
    "    \n",
    "    return sentences_gt, sentences_ocr, words_GT_sentences, words_OCR_sentences\n",
    "\n",
    "\n",
    "#df['gt text'] = df['gt text'].apply(lambda x: preprocessing(x))\n",
    "\n",
    "for index, row in df.iterrows(): \n",
    "    sentences_gt, sentences_ocr, words_GT_sentences, words_OCR_sentences = sentence_count(row)\n",
    "    meta_df = meta_df.append({'identifier': row['identifier'], \\\n",
    "                              'sentences gt (fuzzy matched)': sentences_gt, 'sentences ocr (fuzzy matched)': sentences_ocr, \\\n",
    "                              'avg sentence length gt (fuzzy matched)': s.mean(words_GT_sentences), 'avg sentence length ocr (fuzzy matched)': s.mean(words_OCR_sentences), \\\n",
    "                              'max sentence length gt (fuzzy matched)': max(words_GT_sentences), 'max sentence length ocr (fuzzy matched)': max(words_OCR_sentences), \\\n",
    "                              'word count gt (fuzzy matched)': sum(words_GT_sentences), 'word count ocr (fuzzy matched)': sum(words_OCR_sentences)}, ignore_index = True)\n",
    "    \n",
    "df = pd.merge(left=df, right=meta_df, on='identifier')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "894fdee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('tussendoor_df_ER.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "145d70dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>identifier</th>\n",
       "      <th>gt text</th>\n",
       "      <th>ocr text</th>\n",
       "      <th>CER</th>\n",
       "      <th>WER</th>\n",
       "      <th>WER (order independent)</th>\n",
       "      <th>dictionary lookup gt</th>\n",
       "      <th>dictionary lookup ocr</th>\n",
       "      <th>jaccard_coefficient</th>\n",
       "      <th>...</th>\n",
       "      <th>CER matched sentences</th>\n",
       "      <th>WER matched sentences</th>\n",
       "      <th>avg sentence length gt (fuzzy matched)</th>\n",
       "      <th>avg sentence length ocr (fuzzy matched)</th>\n",
       "      <th>max sentence length gt (fuzzy matched)</th>\n",
       "      <th>max sentence length ocr (fuzzy matched)</th>\n",
       "      <th>sentences gt (fuzzy matched)</th>\n",
       "      <th>sentences ocr (fuzzy matched)</th>\n",
       "      <th>word count gt (fuzzy matched)</th>\n",
       "      <th>word count ocr (fuzzy matched)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22517</td>\n",
       "      <td>ddd.010555149.mpeg21.a0002</td>\n",
       "      <td>wt soloturen den 28 dito</td>\n",
       "      <td>wt soloturen den 28 dito</td>\n",
       "      <td>57.53</td>\n",
       "      <td>114.86</td>\n",
       "      <td>113.51</td>\n",
       "      <td>85.71</td>\n",
       "      <td>42.25</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  identifier                   gt text  \\\n",
       "0       22517  ddd.010555149.mpeg21.a0002  wt soloturen den 28 dito   \n",
       "\n",
       "                   ocr text    CER     WER  WER (order independent)  \\\n",
       "0  wt soloturen den 28 dito  57.53  114.86                   113.51   \n",
       "\n",
       "   dictionary lookup gt  dictionary lookup ocr  jaccard_coefficient  ...  \\\n",
       "0                 85.71                  42.25                 0.14  ...   \n",
       "\n",
       "   CER matched sentences WER matched sentences  \\\n",
       "0                    NaN                   NaN   \n",
       "\n",
       "   avg sentence length gt (fuzzy matched)  \\\n",
       "0                                     5.0   \n",
       "\n",
       "   avg sentence length ocr (fuzzy matched)  \\\n",
       "0                                      5.0   \n",
       "\n",
       "  max sentence length gt (fuzzy matched)  \\\n",
       "0                                    5.0   \n",
       "\n",
       "  max sentence length ocr (fuzzy matched)  sentences gt (fuzzy matched)  \\\n",
       "0                                     5.0                           1.0   \n",
       "\n",
       "  sentences ocr (fuzzy matched) word count gt (fuzzy matched)  \\\n",
       "0                           1.0                           5.0   \n",
       "\n",
       "  word count ocr (fuzzy matched)  \n",
       "0                            5.0  \n",
       "\n",
       "[1 rows x 32 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df2 = pd.read_csv('tussendoor_df_ER.csv')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e4ab0b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'identifier', 'gt text', 'ocr text', 'CER', 'WER',\n",
       "       'WER (order independent)', 'dictionary lookup gt',\n",
       "       'dictionary lookup ocr', 'jaccard_coefficient',\n",
       "       'levenshtein_dist_normalized', 'source', 'word count gt',\n",
       "       'word count ocr', 'year', 'century', 'old index', 'gt text org',\n",
       "       'ocr text org', 'set', 'gt sentences matched', 'ocr sentences matched',\n",
       "       'CER matched sentences', 'WER matched sentences',\n",
       "       'avg sentence length gt (fuzzy matched)',\n",
       "       'avg sentence length ocr (fuzzy matched)',\n",
       "       'max sentence length gt (fuzzy matched)',\n",
       "       'max sentence length ocr (fuzzy matched)',\n",
       "       'sentences gt (fuzzy matched)', 'sentences ocr (fuzzy matched)',\n",
       "       'word count gt (fuzzy matched)', 'word count ocr (fuzzy matched)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "64e0511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2e3ff8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CER</th>\n",
       "      <th>WER</th>\n",
       "      <th>WER (order independent)</th>\n",
       "      <th>dictionary lookup gt</th>\n",
       "      <th>dictionary lookup ocr</th>\n",
       "      <th>jaccard_coefficient</th>\n",
       "      <th>levenshtein_dist_normalized</th>\n",
       "      <th>word count gt</th>\n",
       "      <th>word count ocr</th>\n",
       "      <th>old index</th>\n",
       "      <th>CER matched sentences</th>\n",
       "      <th>WER matched sentences</th>\n",
       "      <th>avg sentence length gt (fuzzy matched)</th>\n",
       "      <th>avg sentence length ocr (fuzzy matched)</th>\n",
       "      <th>max sentence length gt (fuzzy matched)</th>\n",
       "      <th>max sentence length ocr (fuzzy matched)</th>\n",
       "      <th>sentences gt (fuzzy matched)</th>\n",
       "      <th>sentences ocr (fuzzy matched)</th>\n",
       "      <th>word count gt (fuzzy matched)</th>\n",
       "      <th>word count ocr (fuzzy matched)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>century</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1600s</th>\n",
       "      <td>605782832</td>\n",
       "      <td>1255153.02</td>\n",
       "      <td>2528920.30</td>\n",
       "      <td>2330336.97</td>\n",
       "      <td>2.755678e+06</td>\n",
       "      <td>1753530.50</td>\n",
       "      <td>6276.398158</td>\n",
       "      <td>11738.650591</td>\n",
       "      <td>16257868</td>\n",
       "      <td>14864108</td>\n",
       "      <td>605855582</td>\n",
       "      <td>49167.80</td>\n",
       "      <td>165834.75</td>\n",
       "      <td>210246.466667</td>\n",
       "      <td>227472.466667</td>\n",
       "      <td>260197.0</td>\n",
       "      <td>273782.0</td>\n",
       "      <td>43470.0</td>\n",
       "      <td>46480.0</td>\n",
       "      <td>321388.0</td>\n",
       "      <td>357118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700s</th>\n",
       "      <td>1284987</td>\n",
       "      <td>23676.87</td>\n",
       "      <td>48893.63</td>\n",
       "      <td>43489.96</td>\n",
       "      <td>1.453241e+05</td>\n",
       "      <td>136368.20</td>\n",
       "      <td>935.724665</td>\n",
       "      <td>240.738298</td>\n",
       "      <td>11131140</td>\n",
       "      <td>12094006</td>\n",
       "      <td>58750748</td>\n",
       "      <td>2224.54</td>\n",
       "      <td>7041.49</td>\n",
       "      <td>60625.193973</td>\n",
       "      <td>52397.386499</td>\n",
       "      <td>138781.0</td>\n",
       "      <td>118880.0</td>\n",
       "      <td>15212.0</td>\n",
       "      <td>15285.0</td>\n",
       "      <td>429338.0</td>\n",
       "      <td>379760.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800s</th>\n",
       "      <td>455002</td>\n",
       "      <td>13621.04</td>\n",
       "      <td>17616.52</td>\n",
       "      <td>10546.14</td>\n",
       "      <td>5.433854e+04</td>\n",
       "      <td>47984.05</td>\n",
       "      <td>398.767493</td>\n",
       "      <td>31.416851</td>\n",
       "      <td>49618121</td>\n",
       "      <td>52559527</td>\n",
       "      <td>23797627</td>\n",
       "      <td>832.22</td>\n",
       "      <td>2213.94</td>\n",
       "      <td>14067.522872</td>\n",
       "      <td>12643.913376</td>\n",
       "      <td>58733.0</td>\n",
       "      <td>53743.0</td>\n",
       "      <td>45992.0</td>\n",
       "      <td>48836.0</td>\n",
       "      <td>938414.0</td>\n",
       "      <td>908588.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900s</th>\n",
       "      <td>9809921</td>\n",
       "      <td>15208.24</td>\n",
       "      <td>28981.14</td>\n",
       "      <td>24579.25</td>\n",
       "      <td>1.515944e+05</td>\n",
       "      <td>143450.56</td>\n",
       "      <td>1243.443495</td>\n",
       "      <td>135.340352</td>\n",
       "      <td>5704994</td>\n",
       "      <td>5676973</td>\n",
       "      <td>64219046</td>\n",
       "      <td>2087.25</td>\n",
       "      <td>5109.26</td>\n",
       "      <td>30979.437012</td>\n",
       "      <td>29863.835202</td>\n",
       "      <td>92591.0</td>\n",
       "      <td>91758.0</td>\n",
       "      <td>52382.0</td>\n",
       "      <td>54127.0</td>\n",
       "      <td>889611.0</td>\n",
       "      <td>879512.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0         CER         WER  WER (order independent)  \\\n",
       "century                                                                \n",
       "1600s     605782832  1255153.02  2528920.30               2330336.97   \n",
       "1700s       1284987    23676.87    48893.63                 43489.96   \n",
       "1800s        455002    13621.04    17616.52                 10546.14   \n",
       "1900s       9809921    15208.24    28981.14                 24579.25   \n",
       "\n",
       "         dictionary lookup gt  dictionary lookup ocr  jaccard_coefficient  \\\n",
       "century                                                                     \n",
       "1600s            2.755678e+06             1753530.50          6276.398158   \n",
       "1700s            1.453241e+05              136368.20           935.724665   \n",
       "1800s            5.433854e+04               47984.05           398.767493   \n",
       "1900s            1.515944e+05              143450.56          1243.443495   \n",
       "\n",
       "         levenshtein_dist_normalized  word count gt  word count ocr  \\\n",
       "century                                                               \n",
       "1600s                   11738.650591       16257868        14864108   \n",
       "1700s                     240.738298       11131140        12094006   \n",
       "1800s                      31.416851       49618121        52559527   \n",
       "1900s                     135.340352        5704994         5676973   \n",
       "\n",
       "         old index  CER matched sentences  WER matched sentences  \\\n",
       "century                                                            \n",
       "1600s    605855582               49167.80              165834.75   \n",
       "1700s     58750748                2224.54                7041.49   \n",
       "1800s     23797627                 832.22                2213.94   \n",
       "1900s     64219046                2087.25                5109.26   \n",
       "\n",
       "         avg sentence length gt (fuzzy matched)  \\\n",
       "century                                           \n",
       "1600s                             210246.466667   \n",
       "1700s                              60625.193973   \n",
       "1800s                              14067.522872   \n",
       "1900s                              30979.437012   \n",
       "\n",
       "         avg sentence length ocr (fuzzy matched)  \\\n",
       "century                                            \n",
       "1600s                              227472.466667   \n",
       "1700s                               52397.386499   \n",
       "1800s                               12643.913376   \n",
       "1900s                               29863.835202   \n",
       "\n",
       "         max sentence length gt (fuzzy matched)  \\\n",
       "century                                           \n",
       "1600s                                  260197.0   \n",
       "1700s                                  138781.0   \n",
       "1800s                                   58733.0   \n",
       "1900s                                   92591.0   \n",
       "\n",
       "         max sentence length ocr (fuzzy matched)  \\\n",
       "century                                            \n",
       "1600s                                   273782.0   \n",
       "1700s                                   118880.0   \n",
       "1800s                                    53743.0   \n",
       "1900s                                    91758.0   \n",
       "\n",
       "         sentences gt (fuzzy matched)  sentences ocr (fuzzy matched)  \\\n",
       "century                                                                \n",
       "1600s                         43470.0                        46480.0   \n",
       "1700s                         15212.0                        15285.0   \n",
       "1800s                         45992.0                        48836.0   \n",
       "1900s                         52382.0                        54127.0   \n",
       "\n",
       "         word count gt (fuzzy matched)  word count ocr (fuzzy matched)  \n",
       "century                                                                 \n",
       "1600s                         321388.0                        357118.0  \n",
       "1700s                         429338.0                        379760.0  \n",
       "1800s                         938414.0                        908588.0  \n",
       "1900s                         889611.0                        879512.0  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum of values\n",
    "df.groupby('century').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d140dc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CER</th>\n",
       "      <th>WER</th>\n",
       "      <th>WER (order independent)</th>\n",
       "      <th>dictionary lookup gt</th>\n",
       "      <th>dictionary lookup ocr</th>\n",
       "      <th>jaccard_coefficient</th>\n",
       "      <th>levenshtein_dist_normalized</th>\n",
       "      <th>word count gt</th>\n",
       "      <th>word count ocr</th>\n",
       "      <th>old index</th>\n",
       "      <th>CER matched sentences</th>\n",
       "      <th>WER matched sentences</th>\n",
       "      <th>avg sentence length gt (fuzzy matched)</th>\n",
       "      <th>avg sentence length ocr (fuzzy matched)</th>\n",
       "      <th>max sentence length gt (fuzzy matched)</th>\n",
       "      <th>max sentence length ocr (fuzzy matched)</th>\n",
       "      <th>sentences gt (fuzzy matched)</th>\n",
       "      <th>sentences ocr (fuzzy matched)</th>\n",
       "      <th>word count gt (fuzzy matched)</th>\n",
       "      <th>word count ocr (fuzzy matched)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>century</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1600s</th>\n",
       "      <td>17402.551910</td>\n",
       "      <td>36.057254</td>\n",
       "      <td>72.649247</td>\n",
       "      <td>66.944469</td>\n",
       "      <td>79.163402</td>\n",
       "      <td>50.374332</td>\n",
       "      <td>0.180304</td>\n",
       "      <td>0.337221</td>\n",
       "      <td>467.045906</td>\n",
       "      <td>427.006837</td>\n",
       "      <td>17404.641827</td>\n",
       "      <td>7.574765</td>\n",
       "      <td>25.548413</td>\n",
       "      <td>6.039830</td>\n",
       "      <td>6.534687</td>\n",
       "      <td>7.474777</td>\n",
       "      <td>7.865039</td>\n",
       "      <td>1.248779</td>\n",
       "      <td>1.335248</td>\n",
       "      <td>9.232634</td>\n",
       "      <td>10.259063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700s</th>\n",
       "      <td>780.672539</td>\n",
       "      <td>14.384490</td>\n",
       "      <td>29.704514</td>\n",
       "      <td>26.421604</td>\n",
       "      <td>88.289247</td>\n",
       "      <td>82.848238</td>\n",
       "      <td>0.568484</td>\n",
       "      <td>0.148421</td>\n",
       "      <td>6762.539490</td>\n",
       "      <td>7347.512758</td>\n",
       "      <td>35693.042527</td>\n",
       "      <td>6.887121</td>\n",
       "      <td>21.800279</td>\n",
       "      <td>36.831831</td>\n",
       "      <td>31.833163</td>\n",
       "      <td>84.314095</td>\n",
       "      <td>72.223572</td>\n",
       "      <td>9.241798</td>\n",
       "      <td>9.286148</td>\n",
       "      <td>260.837181</td>\n",
       "      <td>230.716889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800s</th>\n",
       "      <td>721.080824</td>\n",
       "      <td>21.586434</td>\n",
       "      <td>27.918415</td>\n",
       "      <td>16.713376</td>\n",
       "      <td>86.114960</td>\n",
       "      <td>76.044453</td>\n",
       "      <td>0.631961</td>\n",
       "      <td>0.072057</td>\n",
       "      <td>78634.106181</td>\n",
       "      <td>83295.605388</td>\n",
       "      <td>37714.147385</td>\n",
       "      <td>6.604921</td>\n",
       "      <td>17.570952</td>\n",
       "      <td>22.294014</td>\n",
       "      <td>20.037898</td>\n",
       "      <td>93.079239</td>\n",
       "      <td>85.171157</td>\n",
       "      <td>72.887480</td>\n",
       "      <td>77.394612</td>\n",
       "      <td>1487.185420</td>\n",
       "      <td>1439.917591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900s</th>\n",
       "      <td>5733.443016</td>\n",
       "      <td>8.888510</td>\n",
       "      <td>16.938130</td>\n",
       "      <td>14.365430</td>\n",
       "      <td>88.599860</td>\n",
       "      <td>83.840187</td>\n",
       "      <td>0.726735</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>3334.303916</td>\n",
       "      <td>3317.926943</td>\n",
       "      <td>37533.048510</td>\n",
       "      <td>6.138971</td>\n",
       "      <td>15.027235</td>\n",
       "      <td>18.106042</td>\n",
       "      <td>17.454024</td>\n",
       "      <td>54.115137</td>\n",
       "      <td>53.628288</td>\n",
       "      <td>30.614845</td>\n",
       "      <td>31.634717</td>\n",
       "      <td>519.936295</td>\n",
       "      <td>514.033898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Unnamed: 0        CER        WER  WER (order independent)  \\\n",
       "century                                                                \n",
       "1600s    17402.551910  36.057254  72.649247                66.944469   \n",
       "1700s      780.672539  14.384490  29.704514                26.421604   \n",
       "1800s      721.080824  21.586434  27.918415                16.713376   \n",
       "1900s     5733.443016   8.888510  16.938130                14.365430   \n",
       "\n",
       "         dictionary lookup gt  dictionary lookup ocr  jaccard_coefficient  \\\n",
       "century                                                                     \n",
       "1600s               79.163402              50.374332             0.180304   \n",
       "1700s               88.289247              82.848238             0.568484   \n",
       "1800s               86.114960              76.044453             0.631961   \n",
       "1900s               88.599860              83.840187             0.726735   \n",
       "\n",
       "         levenshtein_dist_normalized  word count gt  word count ocr  \\\n",
       "century                                                               \n",
       "1600s                       0.337221     467.045906      427.006837   \n",
       "1700s                       0.148421    6762.539490     7347.512758   \n",
       "1800s                       0.072057   78634.106181    83295.605388   \n",
       "1900s                       0.079100    3334.303916     3317.926943   \n",
       "\n",
       "            old index  CER matched sentences  WER matched sentences  \\\n",
       "century                                                               \n",
       "1600s    17404.641827               7.574765              25.548413   \n",
       "1700s    35693.042527               6.887121              21.800279   \n",
       "1800s    37714.147385               6.604921              17.570952   \n",
       "1900s    37533.048510               6.138971              15.027235   \n",
       "\n",
       "         avg sentence length gt (fuzzy matched)  \\\n",
       "century                                           \n",
       "1600s                                  6.039830   \n",
       "1700s                                 36.831831   \n",
       "1800s                                 22.294014   \n",
       "1900s                                 18.106042   \n",
       "\n",
       "         avg sentence length ocr (fuzzy matched)  \\\n",
       "century                                            \n",
       "1600s                                   6.534687   \n",
       "1700s                                  31.833163   \n",
       "1800s                                  20.037898   \n",
       "1900s                                  17.454024   \n",
       "\n",
       "         max sentence length gt (fuzzy matched)  \\\n",
       "century                                           \n",
       "1600s                                  7.474777   \n",
       "1700s                                 84.314095   \n",
       "1800s                                 93.079239   \n",
       "1900s                                 54.115137   \n",
       "\n",
       "         max sentence length ocr (fuzzy matched)  \\\n",
       "century                                            \n",
       "1600s                                   7.865039   \n",
       "1700s                                  72.223572   \n",
       "1800s                                  85.171157   \n",
       "1900s                                  53.628288   \n",
       "\n",
       "         sentences gt (fuzzy matched)  sentences ocr (fuzzy matched)  \\\n",
       "century                                                                \n",
       "1600s                        1.248779                       1.335248   \n",
       "1700s                        9.241798                       9.286148   \n",
       "1800s                       72.887480                      77.394612   \n",
       "1900s                       30.614845                      31.634717   \n",
       "\n",
       "         word count gt (fuzzy matched)  word count ocr (fuzzy matched)  \n",
       "century                                                                 \n",
       "1600s                         9.232634                       10.259063  \n",
       "1700s                       260.837181                      230.716889  \n",
       "1800s                      1487.185420                     1439.917591  \n",
       "1900s                       519.936295                      514.033898  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean of values\n",
    "df.groupby('century').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bc4f0e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381.0\n"
     ]
    }
   ],
   "source": [
    "# to find out longest sentence\n",
    "print(max(list(df['max sentence length gt (fuzzy matched)'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a962a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word alignment\n",
    "def word_alignment(GT, OCR):\n",
    "    shortest_list = min([GT, OCR], key=len)\n",
    "    longest_list = max([GT, OCR], key=len)\n",
    "    min_ratio = 0.65\n",
    "\n",
    "    for i in range(len(longest_list)):\n",
    "        match = False\n",
    "        #i = -i-1\n",
    "        # check if there is a direct match:\n",
    "        try:\n",
    "            if OCR[i] == '' and GT[i] == '':\n",
    "                del OCR[i]\n",
    "                del GT[i]\n",
    "            if SequenceMatcher(None, GT[i], OCR[i]).ratio() >= min_ratio:\n",
    "                match = True\n",
    "                continue\n",
    "            elif match == False:\n",
    "                # check if there is a match with a word before in OCR:\n",
    "                try:\n",
    "                    for j in range(1,10):\n",
    "                        if i-j >= 0:\n",
    "                            if j >= 5:\n",
    "                                if (GT[i] >= 5) or (OCR[i-j] >= 5):\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    continue\n",
    "                            if SequenceMatcher(None, GT[i], OCR[i-j]).ratio() >= min_ratio:\n",
    "                                if not (GT[i] == '' and OCR[i-j] == ''):\n",
    "                                    for k in range(0,j):\n",
    "                                        OCR.insert(i-j, '')\n",
    "                                    match = True\n",
    "                                    continue\n",
    "                        if i+j <= len(OCR):\n",
    "                            if j >= 5:\n",
    "                                if (GT[i] >= 5) or (OCR[i+j] >= 5):\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    continue\n",
    "                            if SequenceMatcher(None, GT[i], OCR[i+j]).ratio() >= min_ratio:\n",
    "                                if not (GT[i] == '' and OCR[i+j] == ''):\n",
    "                                    for k in range(0,j):\n",
    "                                        GT.insert(i, '')\n",
    "                                    match = True\n",
    "                                    continue\n",
    "                except:\n",
    "                    pass\n",
    "        except IndexError:\n",
    "            min([GT, OCR], key=len).append('')\n",
    "\n",
    "        if match == False:\n",
    "            try:\n",
    "                if SequenceMatcher(None, GT[i], OCR[i]).ratio() >= 0.30:\n",
    "                    pass\n",
    "            except:\n",
    "                pass\n",
    "            else:\n",
    "                #print('bad match', GT[i], OCR[i])\n",
    "                #print('before forwards match GT:', GT)\n",
    "                #print('before forwards bad match OCR:', OCR)\n",
    "                if i-1 >= 0:\n",
    "                    GT.insert(i, '')\n",
    "                else:\n",
    "                    GT.insert(0, '')\n",
    "                if i+1 <= len(GT):\n",
    "                    OCR.insert(i+1, '')\n",
    "                else:\n",
    "                    GT.append('')\n",
    "                #print('after forwards match GT:', GT)\n",
    "                #print('after forwards bad match OCR:', OCR)\n",
    "                \n",
    "                continue\n",
    "    \n",
    "   #print('after forwards GT:', GT)\n",
    "    #print('after forwards OCR:', OCR)\n",
    "    \n",
    "    while len(GT) != len(OCR):\n",
    "        min([GT, OCR], key=len).append('')\n",
    "    \n",
    "    if True:\n",
    "        \n",
    "        shortest_list = min([GT, OCR], key=len)\n",
    "        longest_list = max([GT, OCR], key=len)\n",
    "\n",
    "        for i in range(len(longest_list)):\n",
    "            i = -i-1\n",
    "            #print('i:', i)\n",
    "            try:\n",
    "\n",
    "                if (OCR[i] == '') ^ (GT[i] == ''):\n",
    "\n",
    "                    continue\n",
    "                if OCR[i] == ' ' and GT[i] == '':\n",
    "                    del OCR[i]\n",
    "                    del GT[i]\n",
    "                if SequenceMatcher(None, GT[i], OCR[i]).ratio() > 0.65:\n",
    "     \n",
    "                    continue\n",
    "                else:\n",
    "                    # check if there is a match with a word before in OCR:\n",
    "                    try:\n",
    "                        for j in range(1,len(GT)):\n",
    "                            if j >= 10:\n",
    "                                extra_ratio = 0.1\n",
    "                            else:\n",
    "                                extra_ratio = 0\n",
    "                            if i-j >= -len(OCR):\n",
    "                                if j >= 5:\n",
    "                                    if (len(GT[i]) >= 5) or (len(OCR[i-j]) >=5):\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        continue\n",
    "                            if GT[i] == '' or OCR[i-j] == '':\n",
    "                                continue\n",
    "                            if SequenceMatcher(None, GT[i], OCR[i-j]).ratio() > 0.65 + extra_ratio:\n",
    "                                for k in range(0,j):\n",
    "                                    if i+k < 0:\n",
    "                                        GT.insert(i+1, '')\n",
    "                                    else: \n",
    "                                        GT.append('')\n",
    "                                break\n",
    "                            if i+j <= -1:\n",
    "                                if j >= 5:\n",
    "                                    if (GT[i] >= 5) or (OCR[i+j] >=5):\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        continue\n",
    "                            if GT[i] == '' or OCR[i+j] == '':\n",
    "                                continue\n",
    "                            if SequenceMatcher(None, GT[i], OCR[i+j]).ratio() > 0.65 + extra_ratio:\n",
    "                                for k in range(0,j):\n",
    "                                    GT.insert(i, '')\n",
    "\n",
    "                                break\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            except IndexError:\n",
    "                min([GT, OCR], key=len).append('')\n",
    "\n",
    "            # if there is no match:\n",
    "            if SequenceMatcher(None, GT[i], OCR[i]).ratio() > 0.30:\n",
    "                pass\n",
    "            else:\n",
    "                #print('bad match', GT[i], OCR[i])\n",
    "                #print('before backwards bad match GT:',  GT)\n",
    "                #print('before backwards bad match OCR:',  OCR)\n",
    "                if (OCR[i] == '') or (GT[i] == ''):\n",
    "                    continue\n",
    "                else:\n",
    "                    GT.insert(i, '')\n",
    "                    OCR.insert(i+1, '')\n",
    "                #print('after backwards bad match GT:',  GT)\n",
    "                #print('after backwards bad match OCR:',  OCR)\n",
    "                \n",
    "                \n",
    "    #print('after backwards GT:',  GT)\n",
    "    #print('after backwards OCR:',  OCR)\n",
    "                    \n",
    "    while len(GT) != len(OCR):\n",
    "        min([GT, OCR], key=len).append('')\n",
    "\n",
    "\n",
    "    for i in range(len(GT)-1):\n",
    "        try:\n",
    "            #print(i)\n",
    "            #print(GT[i], OCR[i])\n",
    "            if (GT[i] == '') and (OCR[i] == ''):\n",
    "                del GT[i]\n",
    "                del OCR[i]\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "    try:\n",
    "        if SequenceMatcher(None, GT[-1], OCR[-2]).ratio() >= 0.65 and ((GT[-1] != '') or ((OCR[-2] != ''))):\n",
    "            #print('FOUND:', GT[-1], OCR[-2])\n",
    "            OCR.insert(-2, '')\n",
    "            GT.append('')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        if SequenceMatcher(None, GT[-2], OCR[-1]).ratio() >= 0.65 and ((GT[-2] != '') or ((OCR[-1] != ''))):\n",
    "            #print('FOUND:', OCR[-1], GT[-2])\n",
    "            GT.insert(-2, '')\n",
    "            OCR.append('')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for i in range(len(OCR)):\n",
    "        if GT[i] == '' and OCR[i] == '':\n",
    "            GT[i] = 'REMOVE'\n",
    "            OCR[i] = 'REMOVE'\n",
    "\n",
    "    GT = list(filter(lambda a: a != 'REMOVE', GT))\n",
    "    OCR = list(filter(lambda a: a != 'REMOVE', OCR))\n",
    "    \n",
    "    \n",
    "    #print('end GT:',  GT)\n",
    "    #print('end OCR:',  OCR)\n",
    "    return GT, OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9639ef18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'identifier', 'gt text', 'ocr text', 'CER', 'WER',\n",
       "       'WER (order independent)', 'dictionary lookup gt',\n",
       "       'dictionary lookup ocr', 'jaccard_coefficient',\n",
       "       'levenshtein_dist_normalized', 'source', 'word count gt',\n",
       "       'word count ocr', 'year', 'century', 'old index', 'gt text org',\n",
       "       'ocr text org', 'set', 'gt sentences matched', 'ocr sentences matched',\n",
       "       'CER matched sentences', 'WER matched sentences',\n",
       "       'avg sentence length gt (fuzzy matched)',\n",
       "       'avg sentence length ocr (fuzzy matched)',\n",
       "       'max sentence length gt (fuzzy matched)',\n",
       "       'max sentence length ocr (fuzzy matched)',\n",
       "       'sentences gt (fuzzy matched)', 'sentences ocr (fuzzy matched)',\n",
       "       'word count gt (fuzzy matched)', 'word count ocr (fuzzy matched)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e8a39ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify word alignment quality\n",
    "def verify_alignment(GT, OCR):\n",
    "    \n",
    "    if len(GT) != len(OCR):\n",
    "        alignment_validation = False\n",
    "        return alignment_validation\n",
    "    \n",
    "    min_word_length = 8 # the higher this number, the more likely a alignment will be seen as valid, thus higher chance at false positive, lower chance at false negative. \n",
    "    match_ratio = 0.70\n",
    "    anchors_list = []\n",
    "    for i in range(len(GT)):\n",
    "        if len(GT[i]) >= min_word_length:  \n",
    "            anchors_list.append(GT[i])\n",
    "        if len(OCR[i]) >= min_word_length:\n",
    "            anchors_list.append(OCR[i])\n",
    "    #print(anchors_list)\n",
    "\n",
    "\n",
    "    alignment_validation = True\n",
    "\n",
    "    for anchor in anchors_list:\n",
    "        #print('ANCHOR:', anchor)\n",
    "        matches_GT = set()\n",
    "        matches_OCR = set()\n",
    "        match = False\n",
    "        for i in range(len(GT)):\n",
    "            if (SequenceMatcher(None, GT[i], anchor).ratio() >= 0.65) and (len(GT[i]) >= min_word_length or len(GT[i]) == 0):\n",
    "                matches_GT.add(i)\n",
    "                #print('added as GT match:', GT[i])\n",
    "            if SequenceMatcher(None, OCR[i], anchor).ratio() >= 0.65 and (len(OCR[i]) >= min_word_length or len(OCR[i]) == 0):\n",
    "                matches_OCR.add(i)\n",
    "                #print('added as OCR match:', OCR[i])\n",
    "        #print('anchor:', anchor)\n",
    "        #print(matches_GT)\n",
    "        #print(matches_OCR)\n",
    "        if matches_GT == matches_OCR:\n",
    "            match = True\n",
    "        if match == False:\n",
    "            match = True\n",
    "            points_set1 = set() # counts when the matched opposite word is '' for OCR\n",
    "            points_set2 = set() # counts when the matched opposite word is '' for GT\n",
    "            for index in matches_GT.difference(matches_OCR): # elements in matches_GT that are not in matches_OCR\n",
    "                if OCR[index] == '':\n",
    "                    points_set1.add(index)\n",
    "                if OCR[index] != '' and len(matches_OCR.difference(matches_GT)) != 0:\n",
    "                    match = False\n",
    "            for index in matches_OCR.difference(matches_GT): # elements in matches_OCR that are not in matches_GT\n",
    "                if GT[index] == '':\n",
    "                    points_set2.add(index)\n",
    "                if GT[index] != '' and len(matches_GT.difference(matches_OCR)) != 0:\n",
    "                    match = False\n",
    "            if len(points_set1.intersection(points_set2)) != 0: # if an anchor words was match in both OCR and GT with an '', it's a bad match.\n",
    "                match = False\n",
    "        if match == False:\n",
    "            for index in (matches_GT.difference(matches_OCR)):\n",
    "                index += 1\n",
    "                if index in matches_OCR.difference(matches_GT):\n",
    "                    if (GT[index-1] == '' or OCR[index-1] == '') and (SequenceMatcher(None, GT[index], OCR[index]).ratio() >= match_ratio):\n",
    "                        match = True\n",
    "            for index in (matches_OCR.difference(matches_GT)):\n",
    "                index += 1\n",
    "                if index in matches_GT.difference(matches_OCR):\n",
    "                    if (GT[index] == '' or OCR[index] == '') and (SequenceMatcher(None, GT[index-1], OCR[index-1]).ratio() >= match_ratio):\n",
    "                        match = True\n",
    "\n",
    "        if match == False:\n",
    "            alignment_validation = False # bad alignment\n",
    "\n",
    "    return alignment_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "753d77f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         good alignments percentage\n",
      "century                            \n",
      "1600s                      0.965870\n",
      "1700s                      0.777526\n",
      "1800s                      0.918942\n",
      "1900s                      0.918161\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "good_alignments_docs = []\n",
    "bad_alignments_docs = []\n",
    "aligned_OCR_sentences_docs = []\n",
    "aligned_GT_sentences_docs = []\n",
    "\n",
    "#df['aligned_GT_sentences'] = [np.nan] * len(df)\n",
    "#df['aligned_OCR_sentences'] = [np.nan] * len(df)\n",
    "#df['good_alignments'] = [np.nan] * len(df)\n",
    "#df['bad_alignments'] = [np.nan] * len(df)\n",
    "\n",
    "#for index, row in df.loc[df['set'].isin(['val','test'])].iterrows():\n",
    "for index, row in df.iterrows():\n",
    "    if (row['set'] == 'val') or (row['set'] == 'test'):\n",
    "        GT_sentences = row['gt sentences matched'].split('.')\n",
    "        OCR_sentences = row['ocr sentences matched'].split('.')\n",
    "        good_alignments = 0\n",
    "        bad_alignments = 0\n",
    "        aligned_GT = []\n",
    "        aligned_OCR = []\n",
    "        for i in range(len(GT_sentences)):\n",
    "            GT = GT_sentences[i].split(' ')\n",
    "            OCR = OCR_sentences[i].split(' ')\n",
    "            GT, OCR = word_alignment(GT, OCR)\n",
    "            verification = verify_alignment(GT, OCR)\n",
    "            if verification == False: # bad alignment\n",
    "                GT = ['REMOVED'] * len(GT)\n",
    "                bad_alignments += 1\n",
    "            else:\n",
    "                good_alignments += 1\n",
    "            aligned_GT.append(GT)\n",
    "            aligned_OCR.append(OCR)\n",
    "        # an alignment has been made for one document\n",
    "        #df.loc[index, 'aligned_GT_sentences'] = str(aligned_GT)\n",
    "        #df.loc[index, 'aligned_OCR_sentences'] = str(aligned_OCR)\n",
    "        #df.at[index, 'good alignments'] = good_alignments\n",
    "        #df.at[index, 'bad alignments'] = bad_alignments\n",
    "        good_alignments_docs.append(good_alignments)\n",
    "        bad_alignments_docs.append(bad_alignments)\n",
    "        aligned_OCR_sentences_docs.append(aligned_OCR)\n",
    "        aligned_GT_sentences_docs.append(aligned_GT)\n",
    "        \n",
    "    else:\n",
    "        #df.loc[index, 'aligned_GT_sentences'] = df.loc[index, 'aligned_OCR_sentences'] = df.at[index, 'good alignments'] = df.at[index, 'bad alignments'] = np.nan\n",
    "        good_alignments_docs.append(np.nan)\n",
    "        bad_alignments_docs.append(np.nan)\n",
    "        aligned_OCR_sentences_docs.append(np.nan)\n",
    "        aligned_GT_sentences_docs.append(np.nan)\n",
    "\n",
    "    # this list should only apply to rows in the val and test set\n",
    "df['aligned_GT_sentences'] = aligned_GT_sentences_docs\n",
    "df['aligned_OCR_sentences'] = aligned_OCR_sentences_docs\n",
    "df['good_alignments'] = good_alignments_docs\n",
    "df['bad_alignments'] = bad_alignments_docs\n",
    "df['good alignments percentage'] = df['good_alignments'] / (df['good_alignments'] + df['bad_alignments'])\n",
    "#print('percentage of sentences in dataset that is well aligned:', good_alignments_percentage)\n",
    "print(df[['good alignments percentage', 'century']].groupby('century').mean())\n",
    "\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3d039512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#df['longest_streak'], df['avg_longest_streaks'] = df['avg_total_missing_words'] = df['avg_perc_missing_words '] = [np.nan] * len(df)\n",
    "longest_streak_docs = []\n",
    "avg_longest_streaks_docs = []\n",
    "avg_total_missing_words_docs = []\n",
    "avg_perc_missing_words_docs = [] \n",
    "print(len(longest_streak_docs))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if (row['set'] == 'val') or (row['set'] == 'test'):\n",
    "        ocr_aligned_sentences = row['aligned_OCR_sentences']\n",
    "        # analysing the word alignment\n",
    "        # word alignment for each sentence\n",
    "        longest_streaks = []\n",
    "        total_missing_words = []\n",
    "        perc_missing_words = []\n",
    "        for sentence in ocr_aligned_sentences: # for each sentence\n",
    "            streaks = []\n",
    "            streak = 0\n",
    "            for word in sentence: # for each word\n",
    "                if word == '':\n",
    "                    streak = streak + 1\n",
    "                else:\n",
    "                    streaks.append(streak)\n",
    "                    streak = 0\n",
    "            try:\n",
    "                longest_streaks.append(max(streaks))# longest streak of missing word in sentence\n",
    "                total_missing_words.append(sum(streaks)) # total of missing words in a sentence\n",
    "                perc_missing_words.append(sum(streaks) / len(ocr_aligned_sentences))\n",
    "            except ValueError:\n",
    "                longest_streaks.append(0)# longest streak of missing word in sentence\n",
    "                total_missing_words.append(0) # total of missing words in a sentence\n",
    "                perc_missing_words.append(0)\n",
    "        # statistics word alignment doc level:\n",
    "        # find longest streak of all longest streaks:\n",
    "        import statistics as s\n",
    "        longest_streak = max(longest_streaks)\n",
    "        avg_longest_streaks = s.mean(longest_streaks)\n",
    "        avg_total_missing_words = s.mean(total_missing_words)\n",
    "        avg_perc_missing_words = s.mean(perc_missing_words)\n",
    "        longest_streak_docs.append(longest_streak)\n",
    "        avg_longest_streaks_docs.append(avg_longest_streaks)\n",
    "        avg_total_missing_words_docs.append(avg_total_missing_words)\n",
    "        avg_perc_missing_words_docs.append(avg_perc_missing_words)\n",
    "    elif (row['set'] == 'train'):\n",
    "        longest_streak_docs.append(np.nan)\n",
    "        avg_longest_streaks_docs.append(np.nan)\n",
    "        avg_total_missing_words_docs.append(np.nan)\n",
    "        avg_perc_missing_words_docs.append(np.nan)\n",
    "   \n",
    "df['longest_streak'] = longest_streak_docs\n",
    "df['avg_longest_streaks'] = avg_longest_streaks_docs\n",
    "df['avg_total_missing_words'] = avg_total_missing_words_docs\n",
    "df['avg_perc_missing_words '] = avg_perc_missing_words_docs\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8222a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "#df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "54f453e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nvanthof/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nvanthof/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def find_names(text, names):\n",
    "    NNP_tags = \"ik je jij jou u hij zij ze het wij we jullie mij me hem haar ons hen hun\".split(' ')\n",
    "    text = word_tokenize(text)\n",
    "    tags = nltk.pos_tag(text)\n",
    "    \n",
    "    for tag in tags:\n",
    "        if (tag[1] == 'NNP'):\n",
    "            # we want words that are NN(P)s, but are not in NNP_tags\n",
    "            NN = tag[0]\n",
    "            NN = NN.lower()\n",
    "            if NN not in NNP_tags:\n",
    "                names.append(NN)\n",
    "    return names\n",
    "\n",
    "# find proper pronouns\n",
    "gt_names = []\n",
    "ocr_names = []\n",
    "# these both can look into train, val, and test set\n",
    "for text in list(df['gt text']):\n",
    "    gt_names_org = find_names(text, gt_names)\n",
    "for text in list(df['ocr text']):\n",
    "    ocr_names_org = find_names(text, ocr_names)\n",
    "\n",
    "gt_names = []\n",
    "for name in gt_names_org:\n",
    "    if len(name) >= 5:\n",
    "        gt_names.append(name)\n",
    "ocr_names = []\n",
    "for name in ocr_names_org:\n",
    "    if len(name) >= 5:\n",
    "        ocr_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "89d23c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "df.to_csv('preprocessed_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9d693e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lists:\n",
    "with open(\"gt_names.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(gt_names, fp)\n",
    "with open(\"ocr_names.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(ocr_names, fp)\n",
    "#with open(\"test.txt\", \"rb\") as fp:   # Unpickling\n",
    "#    b = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e696953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
