{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.7.0'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessed_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'identifier', 'gt text', 'ocr text', 'CER', 'WER',\n",
      "       'WER (order independent)', 'dictionary lookup gt',\n",
      "       'dictionary lookup ocr', 'jaccard_coefficient',\n",
      "       'levenshtein_dist_normalized', 'source', 'word count gt',\n",
      "       'word count ocr', 'year', 'century', 'old index', 'gt text org',\n",
      "       'ocr text org', 'set', 'gt sentences matched', 'ocr sentences matched',\n",
      "       'CER matched sentences', 'WER matched sentences',\n",
      "       'avg sentence length gt (fuzzy matched)',\n",
      "       'avg sentence length ocr (fuzzy matched)',\n",
      "       'max sentence length gt (fuzzy matched)',\n",
      "       'max sentence length ocr (fuzzy matched)',\n",
      "       'sentences gt (fuzzy matched)', 'sentences ocr (fuzzy matched)',\n",
      "       'word count gt (fuzzy matched)', 'word count ocr (fuzzy matched)',\n",
      "       'aligned_GT_sentences', 'aligned_OCR_sentences', 'good_alignments',\n",
      "       'bad_alignments', 'good alignments percentage', 'longest_streak',\n",
      "       'avg_longest_streaks', 'avg_total_missing_words',\n",
      "       'avg_perc_missing_words '],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# select relevant columns\n",
    "print(df.columns)\n",
    "#df = df[['identifier', 'gt text org', 'ocr text org', 'gt text', 'ocr text', 'source', 'century', 'gt sentences matched', 'aligned_GT_sentences', 'aligned_OCR_sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>identifier</th>\n",
       "      <th>gt text</th>\n",
       "      <th>ocr text</th>\n",
       "      <th>CER</th>\n",
       "      <th>WER</th>\n",
       "      <th>WER (order independent)</th>\n",
       "      <th>dictionary lookup gt</th>\n",
       "      <th>dictionary lookup ocr</th>\n",
       "      <th>jaccard_coefficient</th>\n",
       "      <th>...</th>\n",
       "      <th>word count ocr (fuzzy matched)</th>\n",
       "      <th>aligned_GT_sentences</th>\n",
       "      <th>aligned_OCR_sentences</th>\n",
       "      <th>good_alignments</th>\n",
       "      <th>bad_alignments</th>\n",
       "      <th>good alignments percentage</th>\n",
       "      <th>longest_streak</th>\n",
       "      <th>avg_longest_streaks</th>\n",
       "      <th>avg_total_missing_words</th>\n",
       "      <th>avg_perc_missing_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1776</td>\n",
       "      <td>DDD.010614007.001.jp2.ocr</td>\n",
       "      <td>204e jaargang maandag 24 januari 1955 no. 19 l...</td>\n",
       "      <td>204e jaargang maandag 24 januari 1955 no. 19 l...</td>\n",
       "      <td>0.88</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>89.79</td>\n",
       "      <td>88.24</td>\n",
       "      <td>0.911313</td>\n",
       "      <td>...</td>\n",
       "      <td>3050.0</td>\n",
       "      <td>[['204e', 'jaargang', 'maandag', '24', 'januar...</td>\n",
       "      <td>[['204e', 'jaargang', 'maandag', '24', 'januar...</td>\n",
       "      <td>287.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.114983</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                 identifier  \\\n",
       "39        1776  DDD.010614007.001.jp2.ocr   \n",
       "\n",
       "                                              gt text  \\\n",
       "39  204e jaargang maandag 24 januari 1955 no. 19 l...   \n",
       "\n",
       "                                             ocr text   CER  WER  \\\n",
       "39  204e jaargang maandag 24 januari 1955 no. 19 l...  0.88  2.2   \n",
       "\n",
       "    WER (order independent)  dictionary lookup gt  dictionary lookup ocr  \\\n",
       "39                      1.8                 89.79                  88.24   \n",
       "\n",
       "    jaccard_coefficient  ...  word count ocr (fuzzy matched)  \\\n",
       "39             0.911313  ...                          3050.0   \n",
       "\n",
       "                                 aligned_GT_sentences  \\\n",
       "39  [['204e', 'jaargang', 'maandag', '24', 'januar...   \n",
       "\n",
       "                                aligned_OCR_sentences  good_alignments  \\\n",
       "39  [['204e', 'jaargang', 'maandag', '24', 'januar...            287.0   \n",
       "\n",
       "   bad_alignments good alignments percentage  longest_streak  \\\n",
       "39            0.0                        1.0             2.0   \n",
       "\n",
       "   avg_longest_streaks avg_total_missing_words avg_perc_missing_words   \n",
       "39            0.114983                0.142857                0.000498  \n",
       "\n",
       "[1 rows x 41 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preprocess for models\n",
    "def replace_num(text, character):\n",
    "    print()\n",
    "    numbers = re.findall(r'\\d+', text) \n",
    "    res = list(map(int, numbers))\n",
    "    for numText in res:\n",
    "        text = text.replace(str(numText), character)\n",
    "    return text\n",
    "\n",
    "def replace_proper_nouns(text, character, names): \n",
    "    res = list(map(str, names))\n",
    "    for nounText in res:\n",
    "        text = text.replace(str(nounText), character)\n",
    "    return text\n",
    "    \n",
    "with open(\"gt_names.txt\", \"rb\") as fp:   # Unpickling\n",
    "    gt_names_org = pickle.load(fp)\n",
    "    gt_names = []\n",
    "    for name in gt_names_org:\n",
    "        if len(name) >= 5:\n",
    "            gt_names.append(name)\n",
    "\n",
    "\n",
    "df['gt for training'] = df['gt text']\n",
    "# replace numbers\n",
    "df['gt for training'] = df['gt for training'].apply(lambda x: replace_num(x, '%NUMBER%'))\n",
    "# replace proper nouns\n",
    "df['gt for training'] = df['gt for training'].apply(lambda x: replace_proper_nouns(x, \"%NNP%\", gt_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    nederlanden. brussel den %NUMBER% augustus. si...\n",
       "1    wt venetien den %NUMBER%. %NNP%. des ducq de d...\n",
       "2    de staten des quartiers van nymegen maken beke...\n",
       "3    nederlanden. luyk den %NUMBER% juny. eergister...\n",
       "4    nederlanden. brussel den %NUMBER% maert. briev...\n",
       "5    nederlanden. brussel den %NUMBER% %NNP%. den h...\n",
       "6    engelant ampc. dublin den %NUMBER% %NNP%. voor...\n",
       "7    nederlanden. maestricht den %NUMBER% january. ...\n",
       "8    poolen pruyssen ampc. warschouw den %NUMBER% m...\n",
       "9    engelandt schotlandt ampc. edenburgh den %NUMB...\n",
       "Name: gt for training, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gt for training'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parts for cross-validation, train, validation oand test set\n",
    "#df_17th = df[df['century'] == '1600s']\n",
    "#df_18th = df[df['century'] == '1700s']\n",
    "#df_19th = df[df['century'] == '1800s']\n",
    "#df_20th = df[df['century'] == '1900s']\n",
    "\n",
    "#datasets = [df_17th, df_18th, df_19th, df_20th]\n",
    "# create a list of train, validation and test set\n",
    "\n",
    "#train = pd.DataFrame()\n",
    "#val = pd.DataFrame()\n",
    "#test = pd.DataFrame()\n",
    "#for dataset in datasets:\n",
    "#    splits = np.array_split(dataset, 5)\n",
    "#    train_sub = splits[:3]\n",
    "#    train_sub = pd.concat(train_sub)\n",
    "#    val_sub = splits[3]\n",
    "#    test_sub = splits[4]\n",
    "#    train = pd.concat([train, train_sub])\n",
    "#   val = pd.concat([val, val_sub])\n",
    "#    test = pd.concat([test, test_sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine_tuning the models\n",
    "\n",
    "word2vec_models = []\n",
    "BERT_models = []\n",
    "\n",
    "def finetune_word2vec(train, window=5):\n",
    "    sentences = train.split('.')\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sentences = [tokenizer.tokenize(i) for i in sentences]\n",
    "    total_examples = len(sentences)\n",
    "    \n",
    "    model_w2v = Word2Vec(size=160, min_count=1, window=window)\n",
    "    model_w2v.build_vocab(sentences)\n",
    "    total_examples = model_w2v.corpus_count\n",
    "    model = KeyedVectors.load_word2vec_format(r\"combined-160.txt\", binary=False)\n",
    "    model_w2v.build_vocab([list(model.vocab.keys())], update=True)\n",
    "    model_w2v.intersect_word2vec_format(r\"combined-160.txt\", binary=False, lockf=1.0)\n",
    "    model_w2v.train(sentences, total_examples=total_examples, epochs=model_w2v.iter)\n",
    "    return model_w2v\n",
    "    \n",
    "def finetune_BERT(train):\n",
    "    \n",
    "    class TrainingDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings):\n",
    "            self.encodings = encodings\n",
    "        def __getitem__(self, idx):\n",
    "            return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        def __len__(self):\n",
    "            return len(self.encodings.input_ids)\n",
    "    \n",
    "    text = train.split('.')\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "    model = BertForMaskedLM.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "    # BERT tuning code from https://github.com/jamescalam/transformers/blob/main/course/training/03_mlm_training.ipynb\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "    inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "    # create random array of floats with equal dimensions to input_ids tensor\n",
    "    rand = torch.rand(inputs.input_ids.shape)\n",
    "    # create mask array\n",
    "    mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "           (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
    "    # we take take the indices of each True value, within each individual vector.\n",
    "    selection = []\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        selection.append(\n",
    "            torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        )\n",
    "    # apply these indices to each respective row in input_ids, assigning each of the values at these indices as 103.\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        inputs.input_ids[i, selection[i]] = 103\n",
    "    # initialize data\n",
    "    dataset = TrainingDataset(inputs)\n",
    "    \n",
    "    # start training loop\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print('Cuda available?', torch.cuda.is_available())\n",
    "    # and move our model over to the selected device\n",
    "    model.to(device)\n",
    "    # activate training mode\n",
    "    model.train()\n",
    "    # initialize optimizer\n",
    "    optim = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    epochs = 2\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # setup loop with TQDM and dataloader\n",
    "        loop = tqdm(loader, leave=True)\n",
    "        for batch in loop:\n",
    "            # initialize calculated gradients (from prev step)\n",
    "            optim.zero_grad()\n",
    "            # pull all tensor batches required for training\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask']\n",
    "            #input_ids = batch['input_ids']\n",
    "            #attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels'].to(device)\n",
    "            #labels = batch['labels']\n",
    "            # process\n",
    "            outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                            labels=labels).to(device)\n",
    "            # extract loss\n",
    "            loss = outputs.loss\n",
    "            # calculate loss for every parameter that needs grad update\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optim.step()\n",
    "            # print relevant info to progress bar\n",
    "            loop.set_description(f'Epoch {epoch}')\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training word2vec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-60-fd1ee8014c77>:18: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  model_w2v.train(sentences, total_examples=total_examples, epochs=model_w2v.iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec trained.\n",
      "Training BERTje.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Invalid device string: 'cuda0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-04456c0cb238>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Word2vec trained.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training BERTje.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mBERT_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mfinetune_BERT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BERTje trained.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-fd1ee8014c77>\u001b[0m in \u001b[0;36mfinetune_BERT\u001b[0;34m(train)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# start training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda0'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cuda available?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# and move our model over to the selected device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Invalid device string: 'cuda0'"
     ]
    }
   ],
   "source": [
    "# train models\n",
    "train = df[df['set']=='train']\n",
    "train = '.'.join(list(train['gt for training']))\n",
    "print('Training word2vec.')\n",
    "word2vec_model = finetune_word2vec(train)\n",
    "print('Word2vec trained.')\n",
    "print('Training BERTje.')\n",
    "BERT_model= finetune_BERT(train)\n",
    "print('BERTje trained.')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.save(\"word2vec_finetuned.model\")\n",
    "#word2vec_model = Word2Vec.load(\"word2vec_finetuned.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(BERT_model, 'BERT_finetuned.pt')\n",
    "#BERT_model = torch.load('BERT_finetuned.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
