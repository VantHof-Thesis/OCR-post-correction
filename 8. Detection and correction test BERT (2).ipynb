{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3abfad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\n",
    "\n",
    "# do not forget to set the parameters topn_detection, topn_correction, and method\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle\n",
    "import ast\n",
    "import statistics as s\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "import copy\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import subprocess\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.dammit import EncodingDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22bf6d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1+cu102'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "#transformers.__version__\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "462f74fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec model\n",
    "#BERT_model = ...\n",
    "# load BERT model\n",
    "#BERT_model = ...\n",
    "# load dataframe\n",
    "#df = ...\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "BERT_model = BertForMaskedLM.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "#BERT_model.intersect_word2vec_format(r\"combined-160.txt\", binary=False, lockf=1.0)\n",
    "# https://github.com/clips/dutchembeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee6e0a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessed_df100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84474392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3143c8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('all_lists_tokens.txt', 'rb') as f:\n",
    "#    all_lists_tokens = pickle.load(f)\n",
    "    \n",
    "#all_lists_tokens = ast.literal_eval(all_lists_tokens)\n",
    "#vocab_BERT, vocab_word2vec, hist_expressions, modern_vocab, dictionary = all_lists_tokens\n",
    "\n",
    "with open('homonyms.txt', 'rb') as f:\n",
    "    homonyms = pickle.load(f)\n",
    "with open('vocab_BERT', 'rb') as f:\n",
    "    vocab_BERT = pickle.load(f)\n",
    "with open('vocab_word2vec.txt', 'rb') as f:\n",
    "    vocab_word2vec = pickle.load(f)\n",
    "with open('hist_expressions.txt', 'rb') as f:\n",
    "    hist_expressions = pickle.load(f)\n",
    "with open('infrequent_expressions.txt', 'rb') as f:\n",
    "    infrequent_expressions = pickle.load(f)\n",
    "with open('dictionary.txt', 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20a39125",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lists_tokens = [homonyms, vocab_BERT, vocab_word2vec, hist_expressions, infrequent_expressions, dictionary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d279f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skiplist (words that should not be corrected: names)\n",
    "with open(\"ocr_names.txt\", \"rb\") as fp:   # Unpickling\n",
    "    ocr_names = pickle.load(fp)\n",
    "\n",
    "ocr_names = []\n",
    "for name in ocr_names:\n",
    "    if len(name) >= 5:\n",
    "        ocr_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80f53f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_merger(lists):\n",
    "    #normal_list = False\n",
    "    #for elem in lists:\n",
    "    #    if type(elem) != list:\n",
    "    #        normal_list = True\n",
    "    #if normal_list == True:\n",
    "    #    return lists\n",
    "    #else:\n",
    "    new_list = []\n",
    "    for elem in lists:\n",
    "        new_list = new_list + elem\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0615dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sorted(candidates, sim_or_probs, LD): # sorts first by LD, then by similarity/probability\n",
    "    paired_sorted = sorted(zip(LD,sim_or_probs,candidates),key = lambda x: (x[0],x[1]), reverse=True)\n",
    "    LD,sim_or_probs,candidates = zip(*paired_sorted)\n",
    "    correction = candidates[0]\n",
    "    return correction\n",
    "    \n",
    "def correct_calculated(candidates, sim_or_probs, LD): # calculates a score from LD and normalised similarity/probability\n",
    "    inv_LD = 1 - LD\n",
    "    sim_or_probs = np.array(sim_or_probs)\n",
    "    sim_or_probs = np.interp(sim_or_probs, (sim_or_probs.min(), sim_or_probs.max()), (0, 1)).tolist()\n",
    "    score = sim_or_probs / inv_LD\n",
    "    zipped_pairs = zip(score.tolist(), candidates)\n",
    "    sorted_by_score = [x for _, x in sorted(zipped_pairs, reverse=True)]\n",
    "    correction = sorted_by_score[0]\n",
    "    return correction\n",
    "\n",
    "def remove_stopwords(candidates, cosine, LD):\n",
    "    #nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('dutch'))\n",
    "    candidates_nostopwords = []\n",
    "    cosine_nostopwords = []\n",
    "    LD_nostopwords = []\n",
    "    for i in range(len(candidates)):\n",
    "        if candidates[i] not in stop_words:\n",
    "            candidates_nostopwords.append(candidates[i])\n",
    "            cosine_nostopwords.append(cosine[i])\n",
    "            LD_nostopwords.append(LD[i])\n",
    "    LD_nostopwords = np.array(LD_nostopwords)\n",
    "    return candidates_nostopwords, cosine_nostopwords, LD_nostopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aee0dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of all TP, FN, FP, TN detection:\n",
    "homonyms_detection_list_BERT = [[],[],[],[]]\n",
    "homonyms_detection_context_list_BERT = [[],[],[],[]]\n",
    "histexp_detection_list_BERT = [[],[],[],[]]\n",
    "histexp_detection_context_list_BERT = [[],[],[],[]]\n",
    "OOV_detection_list_BERT = [[],[],[],[]]\n",
    "OOV_detection_context_list_BERT = [[],[],[],[]]\n",
    "infreq_detection_list_BERT = [[],[],[],[]]\n",
    "infreq_detection_context_list_BERT = [[],[],[],[]]\n",
    "RWE_detection_list_BERT = [[],[],[],[]]\n",
    "RWE_detection_context_list_BERT = [[],[],[],[]]\n",
    "all_detection_list_BERT = [[],[],[],[]]\n",
    "none_detection_list_BERT = [[],[],[],[]]\n",
    "none_detection_context_list_BERT = [[],[],[],[]]\n",
    "\n",
    "# list of all right / wrong correction\n",
    "homonyms_correction_list_BERT = [[],[]]\n",
    "homonyms_correction_context_list_BERT = [[],[]]\n",
    "histexp_correction_list_BERT = [[],[]]\n",
    "histexp_correction_context_list_BERT = [[],[]]\n",
    "OOV_correction_list_BERT = [[],[]]\n",
    "OOV_correction_context_list_BERT = [[],[]]\n",
    "infreq_correction_list_BERT = [[],[]]\n",
    "infreq_correction_context_list_BERT = [[],[]]\n",
    "RWE_correction_list_BERT = [[],[]]\n",
    "RWE_correction_context_list_BERT = [[],[]]\n",
    "all_correction_list_BERT = [[],[]]\n",
    "none_correction_list_BERT = [[],[],[],[]]\n",
    "none_correction_context_list_BERT = [[],[],[],[]]\n",
    "\n",
    "#list of outputs corrected texts\n",
    "new_documents = []\n",
    "\n",
    "#list of improved and worsened\n",
    "improved_all = []\n",
    "worsened_all = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6b325efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_result(predicted_error, actual_error):\n",
    "    if actual_error == True:\n",
    "        if predicted_error == True: # TP\n",
    "            result = 'TP'\n",
    "        if predicted_error == False: # FN\n",
    "            result = 'FN'\n",
    "    if actual_error == False:\n",
    "        if predicted_error == True: # FP\n",
    "            result = 'FP'\n",
    "        if predicted_error == False: # TN\n",
    "            result = 'TN'\n",
    "    return result\n",
    "\n",
    "def special_tokens_detection_word(ocr_word, gt_word, detection_list_BERT, all_lists_tokens, result): \n",
    "    homonyms, vocab_BERT, vocab_word2vec, hist_expressions, infrequent_expressions, dictionary = all_lists_tokens\n",
    "    special_token = False\n",
    "    homonym, hist_exp, OOV, infreq, RWE = False, False, False, False, False\n",
    "    # check if word is homonym\n",
    "    if gt_word in homonyms:\n",
    "        homonym = True\n",
    "        special_token = True\n",
    "    # check if word is historical expression\n",
    "    if gt_word in hist_expressions:\n",
    "        hist_exp = True\n",
    "        special_token = True\n",
    "    # check if word is OOV\n",
    "    if gt_word not in vocab_BERT:\n",
    "        OOV = True\n",
    "        special_token = True\n",
    "    # check if word is infrequent\n",
    "    if gt_word in infrequent_expressions:\n",
    "        infreq = True\n",
    "        special_token = True\n",
    "    # check if word is RWE\n",
    "    if (ocr_word in dictionary) and ((result == 'TP') or (result == 'FN')):\n",
    "        RWE = True\n",
    "        special_token = True\n",
    "    # adding the results to the right list\n",
    "    if result == 'TP': # TP = [0]\n",
    "        # all = detection_lit[5]\n",
    "        detection_list_BERT[5][0] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_BERT[0][0] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_BERT[1][0] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_BERT[2][0] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_BERT[3][0] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_BERT[4][0] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_BERT[6][0] += 1\n",
    "    if result == 'FN': # FN = [1]\n",
    "        # all = detection_lit[5]\n",
    "        detection_list_BERT[5][1] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_BERT[0][1] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_BERT[1][1] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_BERT[2][1] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_BERT[3][1] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_BERT[4][1] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_BERT[6][1] += 1\n",
    "    if result == 'FP': # FP = [2]\n",
    "        # all = detection_lit[5]\n",
    "        detection_list_BERT[5][2] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_BERT[0][2] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_BERT[1][2] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_BERT[2][2] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_BERT[3][2] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_BERT[4][2] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_BERT[6][2] += 1\n",
    "    if result == 'TN': # TN = [3]\n",
    "        # all = detection_list[5]\n",
    "        detection_list_BERT[5][3] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_BERT[0][3] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_BERT[1][3] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_BERT[2][3] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_BERT[3][3] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_BERT[4][3] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_BERT[6][3] += 1\n",
    "    return detection_list_BERT\n",
    "\n",
    "def special_tokens_detection_context(ocr_context, gt_context, detection_list_context_BERT, all_lists_tokens, result): \n",
    "    homonyms, vocab_BERT, vocab_word2vec, hist_expressions, infrequent_expressions, dictionary = all_lists_tokens\n",
    "    special_token = False\n",
    "    homonym, hist_exp, OOV, infreq, RWE = False, False, False, False, False\n",
    "    # check if context contains homonym\n",
    "    homonym = False\n",
    "    hist_exp = False\n",
    "    OOV = False\n",
    "    infreq = False\n",
    "    RWE = False\n",
    "    for word in gt_context:\n",
    "        if word in homonyms:\n",
    "            homonym = True\n",
    "            special_token = True\n",
    "        # check if word is historical expression\n",
    "        if word in hist_expressions:\n",
    "            hist_exp = True\n",
    "            special_token = True\n",
    "        # check if word is OOV\n",
    "        if word not in vocab_BERT:\n",
    "            OOV = True\n",
    "            special_token = True\n",
    "        # check if word is infrequent\n",
    "        if word in infrequent_expressions:\n",
    "            infreq = True\n",
    "            special_token = True\n",
    "        # check if word is RWE\n",
    "        for i in range(len(ocr_context)):\n",
    "            if (ocr_context[i] != gt_context[i]) and (ocr_context[i] in dictionary):\n",
    "                RWE = True\n",
    "                special_token = True\n",
    "    # adding the results to the right list\n",
    "    if result == 'TP': # TP = [0]\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_context_BERT[0][0] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_context_BERT[1][0] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_context_BERT[2][0] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_context_BERT[3][0] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_context_BERT[4][0] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_context_BERT[5][0] += 1\n",
    "    if result == 'FN': # FN = [1]\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_context_BERT[0][1] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_context_BERT[1][1] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_context_BERT[2][1] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_context_BERT[3][1] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_context_BERT[4][1] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_context_BERT[5][1] += 1\n",
    "    if result == 'FP': # FP = [2]\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_context_BERT[0][2] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_context_BERT[1][2] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_context_BERT[2][2] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_context_BERT[3][2] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_context_BERT[4][2] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_context_BERT[5][2] += 1\n",
    "    if result == 'TN': # TN = [3]\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_context_BERT[0][3] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_context_BERT[1][3] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_context_BERT[2][3] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_context_BERT[3][3] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_context_BERT[4][3] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_context_BERT[5][3] += 1\n",
    "    return detection_list_context_BERT\n",
    "    \n",
    "\n",
    "def special_tokens_correction_word(ocr_word, gt_word, correction_list_BERT, all_lists_tokens, result): \n",
    "    homonyms, vocab_BERT, vocab_word2vec, hist_expressions, infrequent_expressions, dictionary = all_lists_tokens\n",
    "    special_token = False\n",
    "    homonym, hist_exp, OOV, infreq, RWE = False, False, False, False, False\n",
    "    # check if word is homonym\n",
    "    if gt_word in homonyms:\n",
    "        homonym = True\n",
    "        special_token = True\n",
    "    # check if word is historical expression\n",
    "    if gt_word in hist_expressions:\n",
    "        hist_exp = True\n",
    "        special_token = True\n",
    "    # check if word is OOV\n",
    "    if gt_word not in vocab_BERT\n",
    "        OOV = True\n",
    "        special_token = True\n",
    "    # check if word is infrequent\n",
    "    if gt_word in infrequent_expressions:\n",
    "        infreq = True\n",
    "        special_token = True\n",
    "    # check if word is RWE\n",
    "    if ocr_word in dictionary:\n",
    "        RWE = True\n",
    "        special_token = True\n",
    "    # adding the results to the right list\n",
    "    if result == 'right': # wrong = [0]\n",
    "        # all = detection_lit[5]\n",
    "        correction_list_BERT[5][0] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            correction_list_BERT[0][0] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            correction_list_BERT[1][0] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            correction_list_BERT[2][0] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            correction_list_BERT[3][0] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            correction_list_BERT[4][0] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            correction_list_BERT[6][0] += 1\n",
    "    if result == 'wrong': # right = [1]\n",
    "        # all = detection_lit[5]\n",
    "        correction_list_BERT[5][1] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            correction_list_BERT[0][1] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            correction_list_BERT[1][1] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            correction_list_BERT[2][1] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            correction_list_BERT[3][1] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            correction_list_BERT[4][1] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            correction_list_BERT[6][1] += 1\n",
    "    return correction_list_BERT\n",
    "\n",
    "def special_tokens_correction_context(ocr_context, gt_context, correction_list_context_BERT, all_lists_tokens, result): \n",
    "    homonyms, vocab_BERT, vocab_word2vec, hist_expressions, infrequent_expressions, dictionary = all_lists_tokens\n",
    "    special_token = False\n",
    "    homonym, hist_exp, OOV, infreq, RWE = False, False, False, False, False\n",
    "    # check if context contains homonym\n",
    "    homonym = False\n",
    "    hist_exp = False\n",
    "    OOV = False\n",
    "    infreq = False\n",
    "    RWE = False\n",
    "    for word in gt_context:\n",
    "        if word in homonyms:\n",
    "            homonym = True\n",
    "            special_token = True\n",
    "        # check if word is historical expression\n",
    "        if word in hist_expressions:\n",
    "            hist_exp = True\n",
    "            special_token = True\n",
    "        # check if word is OOV\n",
    "        if word not in vocab_BERT:\n",
    "            OOV = True\n",
    "            special_token = True\n",
    "        # check if word is infrequent\n",
    "        if word in infrequent_expressions:\n",
    "            infreq = True\n",
    "            special_token = True\n",
    "        # check if word is RWE\n",
    "        for i in range(len(ocr_context)):\n",
    "            if (ocr_context[i] != gt_context[i]) and (ocr_context[i] in dictionary):\n",
    "                RWE = True\n",
    "                special_token = True\n",
    "    # adding the results to the right list\n",
    "    if result == 'right': # right = [0]\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            correction_list_context_BERT[0][0] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            correction_list_context_BERT[1][0] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            correction_list_context_BERT[2][0] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            correction_list_context_BERT[3][0] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            correction_list_context_BERT[4][0] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            correction_list_context_BERT[5][0] += 1\n",
    "    if result == 'wrong': # wrong = [1]\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            correction_list_context_BERT[0][1] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            correction_list_context_BERT[1][1] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            correction_list_context_BERT[2][1] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            correction_list_context_BERT[3][1] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            correction_list_context_BERT[4][1] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            correction_list_context_BERT[5][1] += 1\n",
    "    return correction_list_context_BERT\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3f39ebe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR_text: [['12', 'Een', 'hoekenpan', 'of', 'kortweg', '879', 'pan', 'is', 'een', 'platte', 'pan', 'met', 'een', 'hang', 'handvat'], ['\\nDe', 'pan', 'ontleent', 'zijn', 'naam', 'haan', 'het', 'feit', 'dat', 'in', \"zo'n\", 'pan', '12', 'pannenkoeken', 'horden', 'gebakken'], ['', 'Ook', 'ander', 'voedsel,', 'zoals', 'vlees,', 'word', 'in', 'een', 'hoekenpan', 'gebraden', '12', 'coninghs-merck']]\n",
      "skipped: 12\n",
      "context Een: ['%NUMBER%', 'hoekenpan', 'of', 'kortweg', '%NUMBER%', 'pan', 'is', 'een', 'platte', 'pan', 'met', 'een', 'hang', 'handvat']\n",
      "sentence Een: 12 [MASK] hoekenpan of kortweg 879 pan is een platte pan met een hang handvat\n",
      "context hoekenpan: ['%NUMBER%', 'Een', 'of', 'kortweg', '%NUMBER%', 'pan', 'is', 'een', 'platte', 'pan', 'met', 'een', 'hang', 'handvat']\n",
      "sentence hoekenpan: 12 Een [MASK] of kortweg 879 pan is een platte pan met een hang handvat\n",
      "skipped: of\n",
      "context kortweg: ['%NUMBER%', 'Een', 'hoekenpan', 'of', '%NUMBER%', 'pan', 'is', 'een', 'platte', 'pan', 'met', 'een', 'hang', 'handvat']\n",
      "sentence kortweg: 12 Een hoekenpan of [MASK] 879 pan is een platte pan met een hang handvat\n",
      "skipped: 879\n",
      "context pan: ['%NUMBER%', 'Een', 'hoekenpan', 'of', 'kortweg', '%NUMBER%', 'is', 'een', 'platte', 'pan', 'met', 'een', 'hang', 'handvat']\n",
      "sentence pan: 12 Een hoekenpan of kortweg 879 [MASK] is een platte pan met een hang handvat\n",
      "skipped: is\n",
      "context een: ['%NUMBER%', 'Een', 'hoekenpan', 'of', 'kortweg', '%NUMBER%', 'pan', 'is', 'platte', 'pan', 'met', 'een', 'hang', 'handvat']\n",
      "sentence een: 12 Een hoekenpan of kortweg 879 pan is [MASK] platte pan met een hang handvat\n",
      "context platte: ['%NUMBER%', 'Een', 'hoekenpan', 'of', 'kortweg', '%NUMBER%', 'pan', 'is', 'een', 'pan', 'met', 'een', 'hang', 'handvat']\n",
      "sentence platte: 12 Een hoekenpan of kortweg 879 pan is een [MASK] pan met een hang handvat\n",
      "context pan: ['%NUMBER%', 'Een', 'hoekenpan', 'of', 'kortweg', '%NUMBER%', 'pan', 'is', 'een', 'platte', 'met', 'een', 'hang', 'handvat']\n",
      "sentence pan: 12 Een hoekenpan of kortweg 879 pan is een platte [MASK] met een hang handvat\n",
      "context met: ['%NUMBER%', 'Een', 'hoekenpan', 'of', 'kortweg', '%NUMBER%', 'pan', 'is', 'een', 'platte', 'pan', 'een', 'hang', 'handvat']\n",
      "sentence met: 12 Een hoekenpan of kortweg 879 pan is een platte pan [MASK] een hang handvat\n",
      "context een: ['%NUMBER%', 'Een', 'hoekenpan', 'of', 'kortweg', '%NUMBER%', 'pan', 'is', 'een', 'platte', 'pan', 'met', 'hang', 'handvat']\n",
      "sentence een: 12 Een hoekenpan of kortweg 879 pan is een platte pan met [MASK] hang handvat\n",
      "context hang: ['%NUMBER%', 'Een', 'hoekenpan', 'of', 'kortweg', '%NUMBER%', 'pan', 'is', 'een', 'platte', 'pan', 'met', 'een', 'handvat']\n",
      "sentence hang: 12 Een hoekenpan of kortweg 879 pan is een platte pan met een [MASK] handvat\n",
      "context handvat: ['%NUMBER%', 'Een', 'hoekenpan', 'of', 'kortweg', '%NUMBER%', 'pan', 'is', 'een', 'platte', 'pan', 'met', 'een', 'hang']\n",
      "sentence handvat: 12 Een hoekenpan of kortweg 879 pan is een platte pan met een hang [MASK]\n",
      "skipped: \n",
      "De\n",
      "context pan: ['\\nDe', 'ontleent', 'zijn', 'naam', 'haan', 'het', 'feit', 'dat', 'in', \"zo'n\", 'pan', '%NUMBER%', 'pannenkoeken', 'horden', 'gebakken']\n",
      "sentence pan: \n",
      "De [MASK] ontleent zijn naam haan het feit dat in zo'n pan 12 pannenkoeken horden gebakken\n",
      "context ontleent: ['\\nDe', 'pan', 'zijn', 'naam', 'haan', 'het', 'feit', 'dat', 'in', \"zo'n\", 'pan', '%NUMBER%', 'pannenkoeken', 'horden', 'gebakken']\n",
      "sentence ontleent: \n",
      "De pan [MASK] zijn naam haan het feit dat in zo'n pan 12 pannenkoeken horden gebakken\n",
      "context zijn: ['\\nDe', 'pan', 'ontleent', 'naam', 'haan', 'het', 'feit', 'dat', 'in', \"zo'n\", 'pan', '%NUMBER%', 'pannenkoeken', 'horden', 'gebakken']\n",
      "sentence zijn: \n",
      "De pan ontleent [MASK] naam haan het feit dat in zo'n pan 12 pannenkoeken horden gebakken\n",
      "context naam: ['\\nDe', 'pan', 'ontleent', 'zijn', 'haan', 'het', 'feit', 'dat', 'in', \"zo'n\", 'pan', '%NUMBER%', 'pannenkoeken', 'horden', 'gebakken']\n",
      "sentence naam: \n",
      "De pan ontleent zijn [MASK] haan het feit dat in zo'n pan 12 pannenkoeken horden gebakken\n",
      "context haan: ['\\nDe', 'pan', 'ontleent', 'zijn', 'naam', 'het', 'feit', 'dat', 'in', \"zo'n\", 'pan', '%NUMBER%', 'pannenkoeken', 'horden', 'gebakken']\n",
      "sentence haan: \n",
      "De pan ontleent zijn naam [MASK] het feit dat in zo'n pan 12 pannenkoeken horden gebakken\n",
      "context het: ['\\nDe', 'pan', 'ontleent', 'zijn', 'naam', 'haan', 'feit', 'dat', 'in', \"zo'n\", 'pan', '%NUMBER%', 'pannenkoeken', 'horden', 'gebakken']\n",
      "sentence het: \n",
      "De pan ontleent zijn naam haan [MASK] feit dat in zo'n pan 12 pannenkoeken horden gebakken\n",
      "context feit: ['\\nDe', 'pan', 'ontleent', 'zijn', 'naam', 'haan', 'het', 'dat', 'in', \"zo'n\", 'pan', '%NUMBER%', 'pannenkoeken', 'horden', 'gebakken']\n",
      "sentence feit: \n",
      "De pan ontleent zijn naam haan het [MASK] dat in zo'n pan 12 pannenkoeken horden gebakken\n",
      "context dat: ['\\nDe', 'pan', 'ontleent', 'zijn', 'naam', 'haan', 'het', 'feit', 'in', \"zo'n\", 'pan', '%NUMBER%', 'pannenkoeken', 'horden', 'gebakken']\n",
      "sentence dat: \n",
      "De pan ontleent zijn naam haan het feit [MASK] in zo'n pan 12 pannenkoeken horden gebakken\n",
      "skipped: in\n",
      "skipped: zo'n\n",
      "context pan: ['\\nDe', 'pan', 'ontleent', 'zijn', 'naam', 'haan', 'het', 'feit', 'dat', 'in', \"zo'n\", '%NUMBER%', 'pannenkoeken', 'horden', 'gebakken']\n",
      "sentence pan: \n",
      "De pan ontleent zijn naam haan het feit dat in zo'n [MASK] 12 pannenkoeken horden gebakken\n",
      "skipped: 12\n",
      "context pannenkoeken: ['\\nDe', 'pan', 'ontleent', 'zijn', 'naam', 'haan', 'het', 'feit', 'dat', 'in', \"zo'n\", 'pan', '%NUMBER%', 'horden', 'gebakken']\n",
      "sentence pannenkoeken: \n",
      "De pan ontleent zijn naam haan het feit dat in zo'n pan 12 [MASK] horden gebakken\n",
      "context horden: ['\\nDe', 'pan', 'ontleent', 'zijn', 'naam', 'haan', 'het', 'feit', 'dat', 'in', \"zo'n\", 'pan', '%NUMBER%', 'pannenkoeken', 'gebakken']\n",
      "sentence horden: \n",
      "De pan ontleent zijn naam haan het feit dat in zo'n pan 12 pannenkoeken [MASK] gebakken\n",
      "context gebakken: ['\\nDe', 'pan', 'ontleent', 'zijn', 'naam', 'haan', 'het', 'feit', 'dat', 'in', \"zo'n\", 'pan', '%NUMBER%', 'pannenkoeken', 'horden']\n",
      "sentence gebakken: \n",
      "De pan ontleent zijn naam haan het feit dat in zo'n pan 12 pannenkoeken horden [MASK]\n",
      "skipped: \n",
      "context Ook: ['', 'ander', 'voedsel,', 'zoals', 'vlees,', 'word', 'in', 'een', 'hoekenpan', 'gebraden', '%NUMBER%', 'coninghs-merck']\n",
      "sentence Ook:  [MASK] ander voedsel, zoals vlees, word in een hoekenpan gebraden 12 coninghs-merck\n",
      "context ander: ['', 'Ook', 'voedsel,', 'zoals', 'vlees,', 'word', 'in', 'een', 'hoekenpan', 'gebraden', '%NUMBER%', 'coninghs-merck']\n",
      "sentence ander:  Ook [MASK] voedsel, zoals vlees, word in een hoekenpan gebraden 12 coninghs-merck\n",
      "skipped: voedsel,\n",
      "context zoals: ['', 'Ook', 'ander', 'voedsel,', 'vlees,', 'word', 'in', 'een', 'hoekenpan', 'gebraden', '%NUMBER%', 'coninghs-merck']\n",
      "sentence zoals:  Ook ander voedsel, [MASK] vlees, word in een hoekenpan gebraden 12 coninghs-merck\n",
      "skipped: vlees,\n",
      "context word: ['', 'Ook', 'ander', 'voedsel,', 'zoals', 'vlees,', 'in', 'een', 'hoekenpan', 'gebraden', '%NUMBER%', 'coninghs-merck']\n",
      "sentence word:  Ook ander voedsel, zoals vlees, [MASK] in een hoekenpan gebraden 12 coninghs-merck\n",
      "skipped: in\n",
      "context een: ['', 'Ook', 'ander', 'voedsel,', 'zoals', 'vlees,', 'word', 'in', 'hoekenpan', 'gebraden', '%NUMBER%', 'coninghs-merck']\n",
      "sentence een:  Ook ander voedsel, zoals vlees, word in [MASK] hoekenpan gebraden 12 coninghs-merck\n",
      "context hoekenpan: ['', 'Ook', 'ander', 'voedsel,', 'zoals', 'vlees,', 'word', 'in', 'een', 'gebraden', '%NUMBER%', 'coninghs-merck']\n",
      "sentence hoekenpan:  Ook ander voedsel, zoals vlees, word in een [MASK] gebraden 12 coninghs-merck\n",
      "context gebraden: ['', 'Ook', 'ander', 'voedsel,', 'zoals', 'vlees,', 'word', 'in', 'een', 'hoekenpan', '%NUMBER%', 'coninghs-merck']\n",
      "sentence gebraden:  Ook ander voedsel, zoals vlees, word in een hoekenpan [MASK] 12 coninghs-merck\n",
      "skipped: 12\n",
      "skipped: coninghs-merck\n"
     ]
    }
   ],
   "source": [
    "new_documents = []\n",
    "#detection test word2vec\n",
    "def detection_and_correction_BERT(row, BERT_model, ocr_names,  all_lists_token, topn_detection=1000, topn_correction=1000, correction_method = 'sorted'):  # choose 'sorted'/ 'sorted_nosw', 'calculated'\n",
    "    if row['set'] != 'test':\n",
    "        return np.nan\n",
    "    else:\n",
    "        biggest_param = max(topn_detection, topn_correction)\n",
    "        identifier = row['identifier']\n",
    "        OCR_text = row['aligned_OCR_sentences']\n",
    "        GT_text = row['aligned_GT_sentences']\n",
    "        OCR_text = ast.literal_eval(OCR_text)\n",
    "        GT_text = ast.literal_eval(GT_text)\n",
    "\n",
    "        \n",
    "        print('OCR_text:', OCR_text)\n",
    "        \n",
    "        # keep track of performance detection\n",
    "        homonyms_detection_BERT = [0,0,0,0]\n",
    "        homonyms_detection_context_BERT = [0,0,0,0]\n",
    "        histexp_detection_BERT = [0,0,0,0]\n",
    "        histexp_detection_context_BERT = [0,0,0,0]\n",
    "        OOV_detection_BERT = [0,0,0,0]\n",
    "        OOV_detection_context_BERT = [0,0,0,0]\n",
    "        infreq_detection_BERT = [0,0,0,0]\n",
    "        infreq_detection_context_BERT = [0,0,0,0]\n",
    "        RWE_detection_BERT = [0,0,0,0]\n",
    "        RWE_detection_context_BERT = [0,0,0,0]\n",
    "        all_detection_BERT = [0,0,0,0]\n",
    "        none_detection_BERT = [0,0,0,0]\n",
    "        none_detection_context_BERT = [0,0,0,0]\n",
    "        \n",
    "        # keep track of performance correction right / wrong\n",
    "        homonyms_correction_BERT = [0,0]\n",
    "        homonyms_correction_context_BERT = [0,0]\n",
    "        histexp_correction_BERT = [0,0]\n",
    "        histexp_correction_context_BERT = [0,0]\n",
    "        OOV_correction_BERT = [0,0]\n",
    "        OOV_correction_context_BERT = [0,0]\n",
    "        infreq_correction_BERT = [0,0]\n",
    "        infreq_correction_context_BERT = [0,0]\n",
    "        RWE_correction_BERT = [0,0]\n",
    "        RWE_correction_context_BERT = [0,0]\n",
    "        all_correction_BERT = [0,0]\n",
    "        none_correction_BERT = [0,0]\n",
    "        none_correction_context_BERT = [0,0]\n",
    "        \n",
    "        # create lists that save evaluation scores for this documents\n",
    "        detection_list_BERT = [homonyms_detection_BERT, histexp_detection_BERT, OOV_detection_BERT, infreq_detection_BERT, RWE_detection_BERT, all_detection_BERT, none_detection_BERT]\n",
    "        detection_list_context_BERT = [homonyms_detection_context_BERT, histexp_detection_context_BERT, OOV_detection_context_BERT, infreq_detection_context_BERT, RWE_detection_context_BERT, none_detection_context_BERT]\n",
    "        correction_list_BERT = [homonyms_correction_BERT, histexp_correction_BERT, OOV_correction_BERT, infreq_correction_BERT, RWE_correction_BERT, all_correction_BERT, none_correction_BERT]\n",
    "        correction_list_context_BERT = [homonyms_correction_context_BERT, histexp_correction_context_BERT, OOV_correction_context_BERT, infreq_correction_context_BERT, RWE_correction_context_BERT, none_correction_context_BERT]\n",
    "        \n",
    "        improved = 0 # when actual error is detected, and corrected rightly\n",
    "        worsened  = 0 # when actual non error is wrongfully detected, and corrected wrongly\n",
    "        \n",
    "        # create corrected file\n",
    "        new_document = []\n",
    "        for s in range(len(OCR_text)): # for each sentence\n",
    "            for i in range(len(OCR_text[s])): # for each word in a sentence\n",
    "                if (OCR_text[s][i] in ocr_names) or (OCR_text[s][i].isalpha() == False) or (len(OCR_text[s][i]) <= 2)  or (GT_text[s][i] == 'REMOVED'):\n",
    "                    # add word to document if left unchanged\n",
    "                    new_document.append(OCR_text[s][i])\n",
    "                    print('skipped:', OCR_text[s][i])\n",
    "                    continue\n",
    "                sentence = copy.deepcopy(OCR_text[s])\n",
    "                gt_sentence = copy.deepcopy(GT_text[s])\n",
    "                sentence[i] = '[MASK]'\n",
    "                context = copy.deepcopy(sentence)\n",
    "                del context[i]\n",
    "                sentence = ' '.join(sentence)\n",
    "                GT_context = gt_sentence\n",
    "                del GT_context[i]\n",
    "                for t in range(len(context)):\n",
    "                    if any(str.isdigit(c) for c in context[t]) == True:\n",
    "                        context[t] = '%NUMBER%'\n",
    "                    elif context[t] in ocr_names:\n",
    "                        context[t] = '%NNP%'\n",
    "                print(f'context {OCR_text[s][i]}:', context)\n",
    "                print(f'sentence {OCR_text[s][i]}:', sentence)\n",
    "                # generate list of all candidates\n",
    "                whole_list_candidates = []\n",
    "                whole_list_probabilities = []\n",
    "                pipe = pipeline('fill-mask', model=BERT_model, tokenizer = tokenizer, top_k=biggest_param)\n",
    "                for res in pipe(sentence):\n",
    "                    whole_list_candidates.append(res['token_str'].replace(' ', ''))\n",
    "                    whole_list_probabilities.append(res['score'])\n",
    "                # remove punctuation except for hyphen from candidates\n",
    "                whole_list_candidates = [re.sub(r'[^\\w\\d\\s\\-]+', '', x) for x in whole_list_candidates]\n",
    "                # score down for detection task\n",
    "                candidates = copy.deepcopy(whole_list_candidates[:topn_detection])\n",
    "                probabilities = copy.deepcopy(whole_list_probabilities[:topn_detection])\n",
    "                #calculate positions detection task\n",
    "                # determine if token is predicted error or not\n",
    "                if OCR_text[s][i] in candidates:\n",
    "                    predicted_error = False\n",
    "                elif OCR_text[s][i] not in candidates:\n",
    "                    predicted_error = True\n",
    "                # determine if token is actual error or not\n",
    "                if OCR_text[s][i] != GT_text[s][i]:\n",
    "                    actual_error = True\n",
    "                elif OCR_text[s][i] == GT_text[s][i]:\n",
    "                    actual_error = False\n",
    "                result_det = calculate_result(predicted_error, actual_error)\n",
    "                # evaluate detection\n",
    "                detection_list_BERT = special_tokens_detection_word(OCR_text[s][i], GT_text[s][i], detection_list_BERT, all_lists_token, result_det)\n",
    "                detection_context_list_BERT = special_tokens_detection_context(context, GT_context, detection_list_context_BERT, all_lists_tokens, result_det)\n",
    "                # return detection evaluation values:\n",
    "\n",
    "\n",
    "                # place old detection evaluation\n",
    "\n",
    "                # correction evaluation\n",
    "                if actual_error == True:\n",
    "                    candidates = copy.deepcopy(whole_list_candidates[:topn_correction])\n",
    "                    probabilities = copy.deepcopy(whole_list_probabilities[:topn_correction])\n",
    "                    # calculate positions detection task\n",
    "                    # try two correction methods\n",
    "                    # first calculate the normalized LDs:\n",
    "                    LD = np.array([fuzz.ratio(OCR_text[s][i], word)/100 for word in candidates])\n",
    "                    # try sorting method\n",
    "                    if correction_method == 'sorted':\n",
    "                        correction = correct_sorted(candidates, probabilities, LD)\n",
    "                    elif correction_method == 'sorted_nosw':\n",
    "                    # try again the sorting methods, but without stopwords\n",
    "                        candidates_nostopwords, cosine_nostopwords, LD_nostopwords = remove_stopwords(candidates, probabilities, LD)\n",
    "                        correction = correct_sorted(candidates_nostopwords, cosine_nostopwords, LD_nostopwords)\n",
    "                    # try score calculation method\n",
    "                    elif correction_method == 'calculated':\n",
    "                        correction = correct_calculated(candidates, probabilities, LD)\n",
    "                    # evaluation\n",
    "                    if correction == GT_text[s][i]:\n",
    "                        result_cor = 'right'\n",
    "                    elif correction != GT_text[s][i]:\n",
    "                        result_cor = 'wrong'\n",
    "                    correction_list_BERT = special_tokens_correction_word(OCR_text[s][i], GT_text[s][i], detection_list_BERT, all_lists_token, result_cor)\n",
    "                    correction_context_list_BERT = special_tokens_correction_context(context, GT_context, correction_list_context_BERT, all_lists_tokens, result_cor)\n",
    "\n",
    "                    # place old correction evaluation\n",
    "\n",
    "                # perform whole task\n",
    "                # first, add OCR-word to file if skipped (see above)\n",
    "                # add word to document if not detected as an error\n",
    "                if predicted_error == False:\n",
    "                    new_document.append(OCR_text[s][i])\n",
    "                    continue\n",
    "                # if predicted to be an error, perform correction:\n",
    "                if actual_error == True:\n",
    "                    correction = correction # correction was already created\n",
    "                elif actual_error == False:\n",
    "                    candidates = copy.deepcopy(whole_list_candidates[:topn_detection])\n",
    "                    probabilities = copy.deepcopy(whole_list_probabilities[:topn_detection])\n",
    "                # first calculate the normalized LDs:\n",
    "                LD = np.array([fuzz.ratio(OCR_text[s][i], word)/100 for word in candidates])\n",
    "                # try sorting method\n",
    "                if correction_method == 'sorted':\n",
    "                    correction = correct_sorted(candidates, probabilities, LD)\n",
    "                elif correction_method == 'sorted_nosw':\n",
    "                # try again the sorting methods, but without stopwords\n",
    "                    candidates_nostopwords, cosine_nostopwords, LD_nostopwords = remove_stopwords(candidates, probabilities, LD)\n",
    "                    correction = correct_sorted(candidates_nostopwords, cosine_nostopwords, LD_nostopwords)\n",
    "                # try score calculation method\n",
    "                elif correction_method == 'calculated':\n",
    "                    correction = correct_calculated(candidates, probabilities, LD)\n",
    "                if correction == GT_text[s][i]:\n",
    "                        result_cor = 'right'\n",
    "                elif correction != GT_text[s][i]:\n",
    "                        result_cor = 'wrong'\n",
    "                new_document.append(correction)\n",
    "\n",
    "                if (result_det == 'TP') and (result_cor == 'right'):\n",
    "                    improved += 1\n",
    "                elif (result_det == 'FP') and (result_cor == 'wrong'):\n",
    "                    worsened += 1\n",
    "                \n",
    "        improved_all.append(improved)\n",
    "        worsened_all.append(worsened)\n",
    "            \n",
    "        new_document = (' ').join(new_document)\n",
    "        new_document = re.sub(' +', ' ', new_document)\n",
    "        new_documents.append(new_document)\n",
    "        \n",
    "        \n",
    "        for k in range(len(detection_list_BERT[0])): # for each result: 0 = TP, 1 = TN, 2 = FP, 3 = TN\n",
    "                # homonyms = index 0 in detection_list_BERT    \n",
    "                homonyms_detection_list_BERT[k].append(detection_list_BERT[0][k])\n",
    "                # hist_exp = index 1\n",
    "                histexp_detection_list_BERT[k].append(detection_list_BERT[1][k])\n",
    "                # OOV = index 2\n",
    "                OOV_detection_list_BERT[k].append(detection_list_BERT[2][k])\n",
    "                # infreq = index 3\n",
    "                infreq_detection_list_BERT[k].append(detection_list_BERT[3][k])\n",
    "                # RWE = index 4\n",
    "                RWE_detection_list_BERT[k].append(detection_list_BERT[4][k])\n",
    "                # all = index 5\n",
    "                all_detection_list_BERT[k].append(detection_list_BERT[5][k])\n",
    "                # non = index 6\n",
    "                none_detection_list_BERT[k].append(detection_list_BERT[6][k])\n",
    "        for k in range(len(detection_list_context_BERT[0])): # for each result: 0 = TP, 1 = TN, 2 = FP, 3 = TN\n",
    "                # homonyms = index 0 in detection_list_BERT    \n",
    "                homonyms_detection_context_list_BERT[k].append(detection_context_list_BERT[0][k])\n",
    "                # hist_exp = index 1\n",
    "                histexp_detection_context_list_BERT[k].append(detection_context_list_BERT[1][k])\n",
    "                # OOV = index 2\n",
    "                OOV_detection_context_list_BERT[k].append(detection_context_list_BERT[2][k])\n",
    "                # infreq = index 3\n",
    "                infreq_detection_context_list_BERT[k].append(detection_context_list_BERT[3][k])\n",
    "                # RWE = index 4\n",
    "                RWE_detection_context_list_BERT[k].append(detection_context_list_BERT[4][k])\n",
    "                # non = index 5\n",
    "                none_detection_context_list_BERT[k].append(detection_context_list_BERT[5][k])\n",
    "    \n",
    "        \n",
    "        # return correction evaluation values:\n",
    "        for k in range(2): # for each result: 0 = right, 1 = wrong\n",
    "                # homonyms = index 0 in detection_list_BERT    \n",
    "                homonyms_correction_list_BERT[k].append(correction_list_BERT[0][k])\n",
    "                # hist_exp = index 1\n",
    "                histexp_correction_list_BERT[k].append(correction_list_BERT[1][k])\n",
    "                # OOV = index 2\n",
    "                OOV_correction_list_BERT[k].append(correction_list_BERT[2][k])\n",
    "                # infreq = index 3\n",
    "                infreq_correction_list_BERT[k].append(correction_list_BERT[3][k])\n",
    "                # RWE = index 4\n",
    "                RWE_correction_list_BERT[k].append(correction_list_BERT[4][k])\n",
    "                # all = index 5\n",
    "                all_correction_list_BERT[k].append(correction_list_BERT[5][k])\n",
    "                # non = index 6\n",
    "                none_correction_list_BERT[k].append(correction_list_BERT[6][k])\n",
    "        for k in range(2): # for each result: 0 = right, 1 = wrong\n",
    "                # homonyms = index 0 in detection_list_BERT    \n",
    "                homonyms_correction_context_list_BERT[k].append(correction_context_list_BERT[0][k])\n",
    "                # hist_exp = index 1\n",
    "                histexp_correction_context_list_BERT[k].append(correction_context_list_BERT[1][k])\n",
    "                # OOV = index 2\n",
    "                OOV_correction_context_list_BERT[k].append(correction_context_list_BERT[2][k])\n",
    "                # infreq = index 3\n",
    "                infreq_correction_context_list_BERT[k].append(correction_context_list_BERT[3][k])\n",
    "                # RWE = index 4\n",
    "                RWE_correction_context_list_BERT[k].append(correction_context_list_BERT[4][k])\n",
    "                # non = index 5\n",
    "                none_correction_context_list_BERT[k].append(correction_context_list_BERT[5][k])\n",
    "        \n",
    "        \n",
    "#for index, row in df.iterrows():\n",
    "#    detection_word2vec(row)\n",
    "# df.loc[70]\n",
    "fake_test_list_GT_aligned = \"\"\"12 Een koekenpan of kortweg 879 pan is een platte pan met een lang handvat.\n",
    "De pan ontleent zijn naam aan het feit dat in zo'n pan 12 pannenkoeken worden gebakken. Ook ander voedsel, zoals vlees, wordt in een koekenpan gebraden 12 coninghs-merck\"\"\"\n",
    "fake_test_list_OCR_aligned = \"\"\"12 Een hoekenpan of kortweg 879 pan is een platte pan met een hang handvat.\n",
    "De pan ontleent zijn naam haan het feit dat in zo'n pan 12 pannenkoeken horden gebakken. Ook ander voedsel, zoals vlees, word in een hoekenpan gebraden 12 coninghs-merck\"\"\"\n",
    "fake_test_list_GT_aligned = fake_test_list_GT_aligned.split('.')\n",
    "fake_test_list_OCR_aligned = fake_test_list_OCR_aligned.split('.')\n",
    "fake_test_list_GT_aligned = [x.split(' ') for x in fake_test_list_GT_aligned]\n",
    "fake_test_list_OCR_aligned = [x.split(' ') for x in fake_test_list_OCR_aligned]\n",
    "d = {'identifier': ['111'], 'aligned_OCR_sentences': [str(fake_test_list_OCR_aligned)], 'aligned_GT_sentences': [str(fake_test_list_GT_aligned)], 'set': ['test'], 'century': ['1600s'], 'source': ['Meertens']}\n",
    "\n",
    "df_probeer = pd.DataFrame(data=d)\n",
    "\n",
    "detection_and_correction_BERT(df_probeer.loc[0], BERT_model, ocr_names, all_lists_tokens)  # choose 'sorted'/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ca3b8ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"12 Een Boeken of kortweg 879 pan is een platte pan met een handig vat \\nDe pan ontleen zijn naam aan het feit dat in zo'n pan 12 pannen horen gebakken Ook ander voedsel, zoals vlees, word in een koekenpan graden 12 coninghs-merck\"]\n"
     ]
    }
   ],
   "source": [
    "print(new_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb923bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b8d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f9890e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6f659ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2], [0], [0], [8]]\n",
      "[[5], [1], [4], [20]]\n",
      "[[4], [4], [2], [17]]\n",
      "[[5], [1], [4], [20]]\n",
      "[[0], [0], [0], [2]]\n",
      "[[5], [1], [4], [20]]\n",
      "[[0], [0], [0], [0]]\n",
      "[[0], [0], [0], [0]]\n",
      "[[4], [4], [0], [0]]\n",
      "[[4], [0], [4], [20]]\n",
      "[[7], [5], [4], [20]]\n",
      "[[3], [1], [2], [1]]\n",
      "[[0], [0], [0], [0]]\n"
     ]
    }
   ],
   "source": [
    "print(homonyms_detection_list_BERT)\n",
    "print(homonyms_detection_context_list_BERT)\n",
    "print(histexp_detection_list_BERT)\n",
    "print(histexp_detection_context_list_BERT)\n",
    "print(OOV_detection_list_BERT)\n",
    "print(OOV_detection_context_list_BERT)\n",
    "print(infreq_detection_list_BERT)\n",
    "print(infreq_detection_context_list_BERT)\n",
    "print(RWE_detection_list_BERT)\n",
    "print(RWE_detection_context_list_BERT)\n",
    "print(all_detection_list_BERT)\n",
    "print(none_detection_list_BERT)\n",
    "print(none_detection_context_list_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b424ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "75b8fac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>homonyms_detection TP</th>\n",
       "      <th>homonyms_detection FN</th>\n",
       "      <th>homonyms_detection FP</th>\n",
       "      <th>homonyms_detection TN</th>\n",
       "      <th>homonyms_detection context TP</th>\n",
       "      <th>homonyms_detection context FN</th>\n",
       "      <th>homonyms_detection context FP</th>\n",
       "      <th>homonyms_detection context TN</th>\n",
       "      <th>histexp_detection TP</th>\n",
       "      <th>histexp_detection FN</th>\n",
       "      <th>histexp_detection FP</th>\n",
       "      <th>histexp_detection TN</th>\n",
       "      <th>histexp_detection context TP</th>\n",
       "      <th>histexp_detection context FN</th>\n",
       "      <th>histexp_detection context FP</th>\n",
       "      <th>histexp_detection context TN</th>\n",
       "      <th>OOV_detection TP</th>\n",
       "      <th>OOV_detection FN</th>\n",
       "      <th>OOV_detection FP</th>\n",
       "      <th>OOV_detection TN</th>\n",
       "      <th>OOV_detection context TP</th>\n",
       "      <th>OOV_detection context FN</th>\n",
       "      <th>OOV_detection context FP</th>\n",
       "      <th>OOV_detection context TN</th>\n",
       "      <th>infreq_detection TP</th>\n",
       "      <th>infreq_detection FN</th>\n",
       "      <th>infreq_detection FP</th>\n",
       "      <th>infreq_detection TN</th>\n",
       "      <th>infreq_detection context TP</th>\n",
       "      <th>infreq_detection context FN</th>\n",
       "      <th>infreq_detection context FP</th>\n",
       "      <th>infreq_detection context TN</th>\n",
       "      <th>RWE_detection TP</th>\n",
       "      <th>RWE_detection FN</th>\n",
       "      <th>RWE_detection FP</th>\n",
       "      <th>RWE_detection TN</th>\n",
       "      <th>RWE_detection context TP</th>\n",
       "      <th>RWE_detection context FN</th>\n",
       "      <th>RWE_detection context FP</th>\n",
       "      <th>RWE_detection context TN</th>\n",
       "      <th>all_detection TP</th>\n",
       "      <th>all_detection FN</th>\n",
       "      <th>all_detection FP</th>\n",
       "      <th>all_detection TN</th>\n",
       "      <th>none_detection TP</th>\n",
       "      <th>none_detection FN</th>\n",
       "      <th>none_detection FP</th>\n",
       "      <th>none_detection TN</th>\n",
       "      <th>none_detection context TP</th>\n",
       "      <th>none_detection context FN</th>\n",
       "      <th>none_detection context FP</th>\n",
       "      <th>none_detection context TN</th>\n",
       "      <th>identifier</th>\n",
       "      <th>century</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1600s</td>\n",
       "      <td>Meertens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   homonyms_detection TP  homonyms_detection FN  homonyms_detection FP  \\\n",
       "0                      2                      0                      0   \n",
       "\n",
       "   homonyms_detection TN  homonyms_detection context TP  \\\n",
       "0                      8                              5   \n",
       "\n",
       "   homonyms_detection context FN  homonyms_detection context FP  \\\n",
       "0                              1                              4   \n",
       "\n",
       "   homonyms_detection context TN  histexp_detection TP  histexp_detection FN  \\\n",
       "0                             20                     4                     4   \n",
       "\n",
       "   histexp_detection FP  histexp_detection TN  histexp_detection context TP  \\\n",
       "0                     2                    17                             5   \n",
       "\n",
       "   histexp_detection context FN  histexp_detection context FP  \\\n",
       "0                             1                             4   \n",
       "\n",
       "   histexp_detection context TN  OOV_detection TP  OOV_detection FN  \\\n",
       "0                            20                 0                 0   \n",
       "\n",
       "   OOV_detection FP  OOV_detection TN  OOV_detection context TP  \\\n",
       "0                 0                 2                         5   \n",
       "\n",
       "   OOV_detection context FN  OOV_detection context FP  \\\n",
       "0                         1                         4   \n",
       "\n",
       "   OOV_detection context TN  infreq_detection TP  infreq_detection FN  \\\n",
       "0                        20                    0                    0   \n",
       "\n",
       "   infreq_detection FP  infreq_detection TN  infreq_detection context TP  \\\n",
       "0                    0                    0                            0   \n",
       "\n",
       "   infreq_detection context FN  infreq_detection context FP  \\\n",
       "0                            0                            0   \n",
       "\n",
       "   infreq_detection context TN  RWE_detection TP  RWE_detection FN  \\\n",
       "0                            0                 4                 4   \n",
       "\n",
       "   RWE_detection FP  RWE_detection TN  RWE_detection context TP  \\\n",
       "0                 0                 0                         4   \n",
       "\n",
       "   RWE_detection context FN  RWE_detection context FP  \\\n",
       "0                         0                         4   \n",
       "\n",
       "   RWE_detection context TN  all_detection TP  all_detection FN  \\\n",
       "0                        20                 7                 5   \n",
       "\n",
       "   all_detection FP  all_detection TN  none_detection TP  none_detection FN  \\\n",
       "0                 4                20                  3                  1   \n",
       "\n",
       "   none_detection FP  none_detection TN  none_detection context TP  \\\n",
       "0                  2                  1                          0   \n",
       "\n",
       "   none_detection context FN  none_detection context FP  \\\n",
       "0                          0                          0   \n",
       "\n",
       "   none_detection context TN identifier century    source  \n",
       "0                          0        111   1600s  Meertens  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'homonyms_detection TP': homonyms_detection_list_BERT[0], 'homonyms_detection FN': homonyms_detection_list_BERT[1], 'homonyms_detection FP': homonyms_detection_list_BERT[2], 'homonyms_detection TN': homonyms_detection_list_BERT[3], \\\n",
    "    'homonyms_detection context TP': homonyms_detection_context_list_BERT[0], 'homonyms_detection context FN': homonyms_detection_context_list_BERT[1], 'homonyms_detection context FP': homonyms_detection_context_list_BERT[2], 'homonyms_detection context TN': homonyms_detection_context_list_BERT[3], \\\n",
    "    'histexp_detection TP': histexp_detection_list_BERT[0], 'histexp_detection FN': histexp_detection_list_BERT[1], 'histexp_detection FP': histexp_detection_list_BERT[2], 'histexp_detection TN': histexp_detection_list_BERT[3], \\\n",
    "    'histexp_detection context TP': histexp_detection_context_list_BERT[0], 'histexp_detection context FN': histexp_detection_context_list_BERT[1], 'histexp_detection context FP': histexp_detection_context_list_BERT[2], 'histexp_detection context TN': histexp_detection_context_list_BERT[3], \\\n",
    "    'OOV_detection TP': OOV_detection_list_BERT[0], 'OOV_detection FN': OOV_detection_list_BERT[1], 'OOV_detection FP': OOV_detection_list_BERT[2], 'OOV_detection TN': OOV_detection_list_BERT[3], \\\n",
    "    'OOV_detection context TP': OOV_detection_context_list_BERT[0], 'OOV_detection context FN': OOV_detection_context_list_BERT[1], 'OOV_detection context FP': OOV_detection_context_list_BERT[2], 'OOV_detection context TN': OOV_detection_context_list_BERT[3], \\\n",
    "    'infreq_detection TP': infreq_detection_list_BERT[0], 'infreq_detection FN': infreq_detection_list_BERT[1], 'infreq_detection FP': infreq_detection_list_BERT[2], 'infreq_detection TN': infreq_detection_list_BERT[3], \\\n",
    "    'infreq_detection context TP': infreq_detection_context_list_BERT[0], 'infreq_detection context FN': infreq_detection_context_list_BERT[1], 'infreq_detection context FP': infreq_detection_context_list_BERT[2], 'infreq_detection context TN': infreq_detection_context_list_BERT[3], \\\n",
    "    'RWE_detection TP': RWE_detection_list_BERT[0], 'RWE_detection FN': RWE_detection_list_BERT[1], 'RWE_detection FP': RWE_detection_list_BERT[2], 'RWE_detection TN': RWE_detection_list_BERT[3], \\\n",
    "    'RWE_detection context TP': RWE_detection_context_list_BERT[0], 'RWE_detection context FN': RWE_detection_context_list_BERT[1], 'RWE_detection context FP': RWE_detection_context_list_BERT[2], 'RWE_detection context TN': RWE_detection_context_list_BERT[3], \\\n",
    "    'all_detection TP': all_detection_list_BERT[0], 'all_detection FN': all_detection_list_BERT[1], 'all_detection FP': all_detection_list_BERT[2], 'all_detection TN': all_detection_list_BERT[3], \\\n",
    "    'none_detection TP': none_detection_list_BERT[0], 'none_detection FN': none_detection_list_BERT[1], 'none_detection FP': none_detection_list_BERT[2], 'none_detection TN': none_detection_list_BERT[3], \\\n",
    "    'none_detection context TP': none_detection_context_list_BERT[0], 'none_detection context FN': none_detection_context_list_BERT[1], 'none_detection context FP': none_detection_context_list_BERT[2], 'none_detection context TN': none_detection_context_list_BERT[3], \\\n",
    "    'identifier': list(df_probeer[df_probeer[\"set\"] == 'test']['identifier']), 'century': list(df_probeer[df_probeer[\"set\"] == 'test']['century']), 'source': list(df_probeer[df_probeer[\"set\"] == 'test']['source'])  }\n",
    "BERT_detection = pd.DataFrame(data=d)\n",
    "\n",
    "BERT_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3a747377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>homonyms_correction right</th>\n",
       "      <th>homonyms_correction wrong</th>\n",
       "      <th>homonyms_correction context right</th>\n",
       "      <th>homonyms_correction context wrong</th>\n",
       "      <th>histexp_correction right</th>\n",
       "      <th>histexp_correction wrong</th>\n",
       "      <th>histexp_correction context right</th>\n",
       "      <th>histexp_correction context wrong</th>\n",
       "      <th>OOV_correction right</th>\n",
       "      <th>OOV_correction wrong</th>\n",
       "      <th>OOV_correction context right</th>\n",
       "      <th>OOV_correction context wrong</th>\n",
       "      <th>infreq_correction right</th>\n",
       "      <th>infreq_correction wrong</th>\n",
       "      <th>infreq_correction context right</th>\n",
       "      <th>infreq_correction context wrong</th>\n",
       "      <th>RWE_correction right</th>\n",
       "      <th>RWE_correction wrong</th>\n",
       "      <th>RWE_correction context right</th>\n",
       "      <th>RWE_correction context wrong</th>\n",
       "      <th>all_correction right</th>\n",
       "      <th>all_correction wrong</th>\n",
       "      <th>none_correction right</th>\n",
       "      <th>none_correction wrong</th>\n",
       "      <th>none_correction context right</th>\n",
       "      <th>none_correction context wrong</th>\n",
       "      <th>identifier</th>\n",
       "      <th>century</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1600s</td>\n",
       "      <td>Meertens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   homonyms_correction right  homonyms_correction wrong  \\\n",
       "0                          2                          0   \n",
       "\n",
       "   homonyms_correction context right  homonyms_correction context wrong  \\\n",
       "0                                  2                                  4   \n",
       "\n",
       "   histexp_correction right  histexp_correction wrong  \\\n",
       "0                         4                         4   \n",
       "\n",
       "   histexp_correction context right  histexp_correction context wrong  \\\n",
       "0                                 2                                 4   \n",
       "\n",
       "   OOV_correction right  OOV_correction wrong  OOV_correction context right  \\\n",
       "0                     0                     0                             2   \n",
       "\n",
       "   OOV_correction context wrong  infreq_correction right  \\\n",
       "0                             4                        0   \n",
       "\n",
       "   infreq_correction wrong  infreq_correction context right  \\\n",
       "0                        0                                0   \n",
       "\n",
       "   infreq_correction context wrong  RWE_correction right  \\\n",
       "0                                0                     4   \n",
       "\n",
       "   RWE_correction wrong  RWE_correction context right  \\\n",
       "0                     4                             2   \n",
       "\n",
       "   RWE_correction context wrong  all_correction right  all_correction wrong  \\\n",
       "0                             2                     7                     5   \n",
       "\n",
       "   none_correction right  none_correction wrong  \\\n",
       "0                      3                      1   \n",
       "\n",
       "   none_correction context right  none_correction context wrong identifier  \\\n",
       "0                              0                              0        111   \n",
       "\n",
       "  century    source  \n",
       "0   1600s  Meertens  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'homonyms_correction right': homonyms_correction_list_BERT[0], 'homonyms_correction wrong': homonyms_correction_list_BERT[1],\\\n",
    "    'homonyms_correction context right': homonyms_correction_context_list_BERT[0], 'homonyms_correction context wrong': homonyms_correction_context_list_BERT[1], \\\n",
    "    'histexp_correction right': histexp_correction_list_BERT[0], 'histexp_correction wrong': histexp_correction_list_BERT[1], \\\n",
    "    'histexp_correction context right': histexp_correction_context_list_BERT[0], 'histexp_correction context wrong': histexp_correction_context_list_BERT[1], \\\n",
    "    'OOV_correction right': OOV_correction_list_BERT[0], 'OOV_correction wrong': OOV_correction_list_BERT[1],\\\n",
    "    'OOV_correction context right': OOV_correction_context_list_BERT[0], 'OOV_correction context wrong': OOV_correction_context_list_BERT[1],\\\n",
    "    'infreq_correction right': infreq_correction_list_BERT[0], 'infreq_correction wrong': infreq_correction_list_BERT[1],\\\n",
    "    'infreq_correction context right': infreq_correction_context_list_BERT[0], 'infreq_correction context wrong': infreq_correction_context_list_BERT[1], \\\n",
    "    'RWE_correction right': RWE_correction_list_BERT[0], 'RWE_correction wrong': RWE_correction_list_BERT[1],\\\n",
    "    'RWE_correction context right': RWE_correction_context_list_BERT[0], 'RWE_correction context wrong': RWE_correction_context_list_BERT[1],\\\n",
    "    'all_correction right': all_correction_list_BERT[0], 'all_correction wrong': all_correction_list_BERT[1],\\\n",
    "    'none_correction right': none_correction_list_BERT[0], 'none_correction wrong': none_correction_list_BERT[1],\\\n",
    "    'none_correction context right': none_correction_context_list_BERT[0], 'none_correction context wrong': none_correction_context_list_BERT[1], \\\n",
    "     'identifier': list(df_probeer[df_probeer[\"set\"] == 'test']['identifier']), 'century': list(df_probeer[df_probeer[\"set\"] == 'test']['century']), 'source': list(df_probeer[df_probeer[\"set\"] == 'test']['source'])}\n",
    "BERT_correction = pd.DataFrame(data=d)\n",
    "\n",
    "BERT_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "752c3924",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_categories_BERT = \"homonyms_detection_BERT, histexp_detection_BERT, OOV_detection_BERT, infreq_detection_BERT, RWE_detection_BERT, all_detection_BERT, none_detection_BERT, homonyms_detection context_BERT, histexp_detection context_BERT, OOV_detection context_BERT, infreq_detection context_BERT, RWE_detection context_BERT, none_detection context_BERT\".replace('_BERT', '').split(', ')\n",
    "\n",
    "for category in detection_categories_BERT:\n",
    "    TP, FN, FP, TN = int(BERT_detection[f'{category} TP']), int(BERT_detection[f'{category} FN']),  int(BERT_detection[f'{category} FP']),  int(BERT_detection[f'{category} TN']),    \n",
    "    try:\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        F1 = 2*((precision*recall)/(precision+recall))\n",
    "    except ZeroDivisionError:\n",
    "        if (TP == 0) and (FP == 0) and (FN == 0):\n",
    "            precision = recall = F1 = 1\n",
    "        elif (TP == 0) and ((FP > 0) or (FN > 0)):\n",
    "            precision = recall = F1 = 0 \n",
    "    try:\n",
    "        accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "    except ZeroDivisionError:\n",
    "        accuracy = np.nan\n",
    "    BERT_detection[f'{category} precision'] = precision\n",
    "    BERT_detection[f'{category} recall'] = recall\n",
    "    BERT_detection[f'{category} F1'] = F1\n",
    "    BERT_detection[f'{category} accuracy'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a152b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_categories_BERT = \"homonyms_correction_BERT, histexp_correction_BERT, OOV_correction_BERT, infreq_correction_BERT, RWE_correction_BERT, all_correction_BERT, none_correction_BERT, homonyms_correction context_BERT, histexp_correction context_BERT, OOV_correction context_BERT, infreq_correction context_BERT, RWE_correction context_BERT, none_correction context_BERT\".replace('_BERT', '').split(', ')\n",
    "\n",
    "for category in correction_categories_BERT:\n",
    "    right, wrong = int(BERT_correction[f'{category} right']), int(BERT_correction[f'{category} wrong'])    \n",
    "    try:\n",
    "        accuracy = right/(right+wrong)\n",
    "    except ZeroDivisionError:\n",
    "        accuracy = np.nan\n",
    "    BERT_correction[f'{category} accuracy'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e87f2ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>homonyms_correction right</th>\n",
       "      <th>homonyms_correction wrong</th>\n",
       "      <th>homonyms_correction context right</th>\n",
       "      <th>homonyms_correction context wrong</th>\n",
       "      <th>histexp_correction right</th>\n",
       "      <th>histexp_correction wrong</th>\n",
       "      <th>histexp_correction context right</th>\n",
       "      <th>histexp_correction context wrong</th>\n",
       "      <th>OOV_correction right</th>\n",
       "      <th>OOV_correction wrong</th>\n",
       "      <th>OOV_correction context right</th>\n",
       "      <th>OOV_correction context wrong</th>\n",
       "      <th>infreq_correction right</th>\n",
       "      <th>infreq_correction wrong</th>\n",
       "      <th>infreq_correction context right</th>\n",
       "      <th>infreq_correction context wrong</th>\n",
       "      <th>RWE_correction right</th>\n",
       "      <th>RWE_correction wrong</th>\n",
       "      <th>RWE_correction context right</th>\n",
       "      <th>RWE_correction context wrong</th>\n",
       "      <th>all_correction right</th>\n",
       "      <th>all_correction wrong</th>\n",
       "      <th>none_correction right</th>\n",
       "      <th>none_correction wrong</th>\n",
       "      <th>none_correction context right</th>\n",
       "      <th>none_correction context wrong</th>\n",
       "      <th>identifier</th>\n",
       "      <th>century</th>\n",
       "      <th>source</th>\n",
       "      <th>homonyms_correction accuracy</th>\n",
       "      <th>histexp_correction accuracy</th>\n",
       "      <th>OOV_correction accuracy</th>\n",
       "      <th>infreq_correction accuracy</th>\n",
       "      <th>RWE_correction accuracy</th>\n",
       "      <th>all_correction accuracy</th>\n",
       "      <th>none_correction accuracy</th>\n",
       "      <th>homonyms_correction context accuracy</th>\n",
       "      <th>histexp_correction context accuracy</th>\n",
       "      <th>OOV_correction context accuracy</th>\n",
       "      <th>infreq_correction context accuracy</th>\n",
       "      <th>RWE_correction context accuracy</th>\n",
       "      <th>none_correction context accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1600s</td>\n",
       "      <td>Meertens</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   homonyms_correction right  homonyms_correction wrong  \\\n",
       "0                          2                          0   \n",
       "\n",
       "   homonyms_correction context right  homonyms_correction context wrong  \\\n",
       "0                                  2                                  4   \n",
       "\n",
       "   histexp_correction right  histexp_correction wrong  \\\n",
       "0                         4                         4   \n",
       "\n",
       "   histexp_correction context right  histexp_correction context wrong  \\\n",
       "0                                 2                                 4   \n",
       "\n",
       "   OOV_correction right  OOV_correction wrong  OOV_correction context right  \\\n",
       "0                     0                     0                             2   \n",
       "\n",
       "   OOV_correction context wrong  infreq_correction right  \\\n",
       "0                             4                        0   \n",
       "\n",
       "   infreq_correction wrong  infreq_correction context right  \\\n",
       "0                        0                                0   \n",
       "\n",
       "   infreq_correction context wrong  RWE_correction right  \\\n",
       "0                                0                     4   \n",
       "\n",
       "   RWE_correction wrong  RWE_correction context right  \\\n",
       "0                     4                             2   \n",
       "\n",
       "   RWE_correction context wrong  all_correction right  all_correction wrong  \\\n",
       "0                             2                     7                     5   \n",
       "\n",
       "   none_correction right  none_correction wrong  \\\n",
       "0                      3                      1   \n",
       "\n",
       "   none_correction context right  none_correction context wrong identifier  \\\n",
       "0                              0                              0        111   \n",
       "\n",
       "  century    source  homonyms_correction accuracy  \\\n",
       "0   1600s  Meertens                           1.0   \n",
       "\n",
       "   histexp_correction accuracy  OOV_correction accuracy  \\\n",
       "0                          0.5                      NaN   \n",
       "\n",
       "   infreq_correction accuracy  RWE_correction accuracy  \\\n",
       "0                         NaN                      0.5   \n",
       "\n",
       "   all_correction accuracy  none_correction accuracy  \\\n",
       "0                 0.583333                      0.75   \n",
       "\n",
       "   homonyms_correction context accuracy  histexp_correction context accuracy  \\\n",
       "0                              0.333333                             0.333333   \n",
       "\n",
       "   OOV_correction context accuracy  infreq_correction context accuracy  \\\n",
       "0                         0.333333                                 NaN   \n",
       "\n",
       "   RWE_correction context accuracy  none_correction context accuracy  \n",
       "0                              0.5                               NaN  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "BERT_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "09bf2116",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_text = \"\"\"12 Een koekenpan of kortweg pan is een platte pan met een lang handvat.\n",
    "De pan ontleent zijn naam aan het feit dat in zo'n pan 12 pannenkoeken worden gebakken. Ook ander voedsel, zoals vlees, wordt in een koekenpan gebraden 12 coninghs-merck\"\"\".lower()\n",
    "df_probeer['gt text'] = [gt_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ddb700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fa497044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = BERT_detection.filter(regex='homonyms|OOV|all').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "22466421",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'corrected document': new_documents, 'gt text': list(df_probeer['gt text']),'identifier': list(df_probeer[df_probeer[\"set\"] == 'test']['identifier']), 'century': list(df_probeer[df_probeer[\"set\"] == 'test']['century']), 'source': list(df_probeer[df_probeer[\"set\"] == 'test']['source']), \\\n",
    "    'improved': improved_all, 'worsened': worsened_all, 'old WER': [0.20], 'old CER': [0.30]}\n",
    "whole_task_BERT = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2723226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d33934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "630e16e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "jar_file = \"ocrevalUAtion-1.3.4-jar-with-dependencies.jar\"\n",
    "\n",
    "def evaluation(index, row):\n",
    "    ID = row['identifier']\n",
    "    page = 'None'\n",
    "    corrected_OCR = re.sub(' +', ' ', str(row['corrected document'].replace('.', '')))\n",
    "    gt_text = re.sub(' +', ' ', str(row['gt text'].replace('.', '')))\n",
    "    filename_ocr = f\"{ID}_{page}_OCR.txt\"\n",
    "    #file_ocr = open(os.path.join(save_path, filename),\"w+\", encoding=\"utf-8\")\n",
    "    file_ocr = open(filename_ocr,\"w+\", encoding=\"utf-8\")\n",
    "    file_ocr.write(corrected_OCR)\n",
    "    file_ocr.close()\n",
    "    \n",
    "    filename_gt = f\"{ID}_{page}_GT.txt\"\n",
    "    #file_gt = open(os.path.join(save_path, filename),\"w+\", encoding=\"utf-8\")\n",
    "    file_gt = open(filename_gt,\"w+\", encoding=\"utf-8\")\n",
    "    file_gt.write(gt_text)\n",
    "    file_gt.close()\n",
    "    \n",
    "    #output = ID + '_' + page + \".html\"\n",
    "    output = f\"{ID}_{page}.html\"\n",
    "    \n",
    "    #process = subprocess.call(\"/home/nvanthof/jdk-16.0.1/bin/java -cp \" + jar_file  + \" eu.digitisation.Main -gt \" + filename_gt + \" -ocr \"+ filename_ocr +\" -o \" + output + \"\")\n",
    "    #os.system(\"/home/nvanthof/jdk-16.0.1/bin/java -cp /home/nvanthof/ocrevalUAtion-1.3.4-jar-with-dependencies.jar eu.digitisation.Main -gt /home/nvanthof/ddd.010728187.mpeg21.a0005_None_GT.txt -ocr /home/nvanthof/ddd.010728187.mpeg21.a0005_None_OCR.txt  -o /home/nvanthof/OUTPUT2.html\")\n",
    "    command = f\"/home/nvanthof/jdk-16.0.1/bin/java -cp /home/nvanthof/ocrevalUAtion-1.3.4-jar-with-dependencies.jar eu.digitisation.Main -gt /home/nvanthof/{filename_gt} -ocr /home/nvanthof/{filename_ocr}  -o /home/nvanthof/{output}\"\n",
    "    os.system(command)\n",
    "    sleep(5)\n",
    "    \n",
    "    soup = BeautifulSoup(open(output, encoding='utf-8'))\n",
    "    table = soup.find(\"table\", attrs={'border': '1'})\n",
    "    # Split the filename, and extract the identifier and pagenr together as identifier \n",
    "    # Find the first table (this is the table in which the scores are stored)\n",
    "    # Find the tags in which 'CER', 'WER', and 'WER (order independent)' are stored and take the next tag to get the score \n",
    "    cer = table.find('td', text='CER')\n",
    "    cerScore = cer.findNext('td')\n",
    "    wer = table.find('td', text='WER')\n",
    "    werScore = wer.findNext('td')\n",
    "    werOI = table.find('td', text='WER (order independent)')\n",
    "    werOIScore = werOI.findNext('td')\n",
    "    \n",
    "    os.remove(filename_gt)\n",
    "    os.remove(filename_ocr)\n",
    "    os.remove(output)\n",
    "    return float(cerScore.text), float(werScore.text)   \n",
    "    \n",
    "    return cerScore.text, werScore.text\n",
    "\n",
    "for index, row in whole_task_BERT.iterrows():\n",
    "    if index%1000 == 0:\n",
    "        print(index)\n",
    "    whole_task_BERT.at[index, 'CER after correction'], whole_task_BERT.at[index, 'WER after correction'] = evaluation(index, row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3e6996c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corrected document</th>\n",
       "      <th>gt text</th>\n",
       "      <th>identifier</th>\n",
       "      <th>century</th>\n",
       "      <th>source</th>\n",
       "      <th>improved</th>\n",
       "      <th>worsened</th>\n",
       "      <th>old WER</th>\n",
       "      <th>old CER</th>\n",
       "      <th>CER after correction</th>\n",
       "      <th>WER after correction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12 Een Boeken of kortweg 879 pan is een platte...</td>\n",
       "      <td>12 een koekenpan of kortweg pan is een platte ...</td>\n",
       "      <td>111</td>\n",
       "      <td>1600s</td>\n",
       "      <td>Meertens</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.55</td>\n",
       "      <td>28.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  corrected document  \\\n",
       "0  12 Een Boeken of kortweg 879 pan is een platte...   \n",
       "\n",
       "                                             gt text identifier century  \\\n",
       "0  12 een koekenpan of kortweg pan is een platte ...        111   1600s   \n",
       "\n",
       "     source  improved  worsened  old WER  old CER  CER after correction  \\\n",
       "0  Meertens         2         4      0.2      0.3                 12.55   \n",
       "\n",
       "   WER after correction  \n",
       "0                 28.57  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_task_BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe1f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed549f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a6cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
