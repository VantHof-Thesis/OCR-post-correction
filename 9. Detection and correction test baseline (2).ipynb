{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3c8551bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "\n",
    "# do not forget to set the parameters topn_detection, topn_correction, and method\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle\n",
    "import ast\n",
    "import statistics as s\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "import copy\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import subprocess\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.dammit import EncodingDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a96572c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1+cu102'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "#transformers.__version__\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c649e869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f4963cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessed_df100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8485893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6d1a10bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('all_lists_tokens.txt', 'rb') as f:\n",
    "#    all_lists_tokens = pickle.load(f)\n",
    "    \n",
    "#all_lists_tokens = ast.literal_eval(all_lists_tokens)\n",
    "#vocab_BERT, vocab_word2vec, hist_expressions, modern_vocab, dictionary = all_lists_tokens\n",
    "\n",
    "with open('homonyms.txt', 'rb') as f:\n",
    "    homonyms = pickle.load(f)\n",
    "with open('vocab_BERT', 'rb') as f:\n",
    "    vocab_BERT = pickle.load(f)\n",
    "with open('vocab_word2vec.txt', 'rb') as f:\n",
    "    vocab_word2vec = pickle.load(f)\n",
    "with open('hist_expressions.txt', 'rb') as f:\n",
    "    hist_expressions = pickle.load(f)\n",
    "with open('infrequent_expressions.txt', 'rb') as f:\n",
    "    infrequent_expressions = pickle.load(f)\n",
    "with open('dictionary.txt', 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1670ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lists_tokens = [homonyms, vocab_BERT, vocab_word2vec, hist_expressions, infrequent_expressions, dictionary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ea303382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skiplist (words that should not be corrected: names)\n",
    "with open(\"ocr_names.txt\", \"rb\") as fp:   # Unpickling\n",
    "    ocr_names = pickle.load(fp)\n",
    "\n",
    "ocr_names = []\n",
    "for name in ocr_names:\n",
    "    if len(name) >= 5:\n",
    "        ocr_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e5d688bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_merger(lists):\n",
    "    #normal_list = False\n",
    "    #for elem in lists:\n",
    "    #    if type(elem) != list:\n",
    "    #        normal_list = True\n",
    "    #if normal_list == True:\n",
    "    #    return lists\n",
    "    #else:\n",
    "    new_list = []\n",
    "    for elem in lists:\n",
    "        new_list = new_list + elem\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0fe505a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sorted(candidates, sim_or_probs, LD): # sorts first by LD, then by similarity/probability\n",
    "    paired_sorted = sorted(zip(LD,sim_or_probs,candidates),key = lambda x: (x[0],x[1]), reverse=True)\n",
    "    LD,sim_or_probs,candidates = zip(*paired_sorted)\n",
    "    correction = candidates[0]\n",
    "    return correction\n",
    "    \n",
    "def correct_calculated(candidates, sim_or_probs, LD): # calculates a score from LD and normalised similarity/probability\n",
    "    inv_LD = 1 - LD\n",
    "    sim_or_probs = np.array(sim_or_probs)\n",
    "    sim_or_probs = np.interp(sim_or_probs, (sim_or_probs.min(), sim_or_probs.max()), (0, 1)).tolist()\n",
    "    score = sim_or_probs / inv_LD\n",
    "    zipped_pairs = zip(score.tolist(), candidates)\n",
    "    sorted_by_score = [x for _, x in sorted(zipped_pairs, reverse=True)]\n",
    "    correction = sorted_by_score[0]\n",
    "    return correction\n",
    "\n",
    "def remove_stopwords(candidates, cosine, LD):\n",
    "    #nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('dutch'))\n",
    "    candidates_nostopwords = []\n",
    "    cosine_nostopwords = []\n",
    "    LD_nostopwords = []\n",
    "    for i in range(len(candidates)):\n",
    "        if candidates[i] not in stop_words:\n",
    "            candidates_nostopwords.append(candidates[i])\n",
    "            cosine_nostopwords.append(cosine[i])\n",
    "            LD_nostopwords.append(LD[i])\n",
    "    LD_nostopwords = np.array(LD_nostopwords)\n",
    "    return candidates_nostopwords, cosine_nostopwords, LD_nostopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f3b88f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of all TP, FN, FP, TN detection:\n",
    "homonyms_detection_list_baseline = [[],[],[],[]]\n",
    "histexp_detection_list_baseline = [[],[],[],[]]\n",
    "OOV_detection_list_baseline = [[],[],[],[]]\n",
    "infreq_detection_list_baseline = [[],[],[],[]]\n",
    "RWE_detection_list_baseline = [[],[],[],[]]\n",
    "all_detection_list_baseline = [[],[],[],[]]\n",
    "none_detection_list_baseline = [[],[],[],[]]\n",
    "\n",
    "# list of all right / wrong correction\n",
    "homonyms_correction_list_baseline = [[],[]]\n",
    "histexp_correction_list_baseline = [[],[]]\n",
    "OOV_correction_list_baseline = [[],[]]\n",
    "infreq_correction_list_baseline = [[],[]]\n",
    "RWE_correction_list_baseline = [[],[]]\n",
    "all_correction_list_baseline = [[],[]]\n",
    "none_correction_list_baseline = [[],[],[],[]]\n",
    "\n",
    "#list of outputs corrected texts\n",
    "new_documents = []\n",
    "\n",
    "#list of improved and worsened\n",
    "improved_all = []\n",
    "worsened_all = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "87cdf809",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_result(predicted_error, actual_error):\n",
    "    if actual_error == True:\n",
    "        if predicted_error == True: # TP\n",
    "            result = 'TP'\n",
    "        if predicted_error == False: # FN\n",
    "            result = 'FN'\n",
    "    if actual_error == False:\n",
    "        if predicted_error == True: # FP\n",
    "            result = 'FP'\n",
    "        if predicted_error == False: # TN\n",
    "            result = 'TN'\n",
    "    return result\n",
    "\n",
    "def special_tokens_detection_word(ocr_word, gt_word, detection_list_baseline, all_lists_tokens, result): \n",
    "    homonyms, vocab_BERT, vocab_word2vec, hist_expressions, infrequent_expressions, dictionary = all_lists_tokens\n",
    "    special_token = False\n",
    "    homonym, hist_exp, OOV, infreq, RWE = False, False, False, False, False\n",
    "    # check if word is homonym\n",
    "    if gt_word in homonyms:\n",
    "        homonym = True\n",
    "        special_token = True\n",
    "    # check if word is historical expression\n",
    "    if gt_word in hist_expressions:\n",
    "        hist_exp = True\n",
    "        special_token = True\n",
    "    # check if word is OOV\n",
    "    if gt_word not in dictionary:\n",
    "        OOV = True\n",
    "        special_token = True\n",
    "    # check if word is infrequent\n",
    "    if gt_word in infrequent_expressions:\n",
    "        infreq = True\n",
    "        special_token = True\n",
    "    # check if word is RWE\n",
    "    if (ocr_word in dictionary) and ((result == 'TP') or (result == 'FN')):\n",
    "        RWE = True\n",
    "        special_token = True\n",
    "    # adding the results to the right list\n",
    "    if result == 'TP': # TP = [0]\n",
    "        # all = detection_lit[5]\n",
    "        detection_list_baseline[5][0] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_baseline[0][0] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_baseline[1][0] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_baseline[2][0] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_baseline[3][0] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_baseline[4][0] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_baseline[6][0] += 1\n",
    "    if result == 'FN': # FN = [1]\n",
    "        # all = detection_lit[5]\n",
    "        detection_list_baseline[5][1] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_baseline[0][1] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_baseline[1][1] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_baseline[2][1] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_baseline[3][1] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_baseline[4][1] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_baseline[6][1] += 1\n",
    "    if result == 'FP': # FP = [2]\n",
    "        # all = detection_lit[5]\n",
    "        detection_list_baseline[5][2] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_baseline[0][2] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_baseline[1][2] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_baseline[2][2] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_baseline[3][2] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_baseline[4][2] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_baseline[6][2] += 1\n",
    "    if result == 'TN': # TN = [3]\n",
    "        # all = detection_list[5]\n",
    "        detection_list_baseline[5][3] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_baseline[0][3] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_baseline[1][3] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_baseline[2][3] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_baseline[3][3] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_baseline[4][3] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_baseline[6][3] += 1\n",
    "    return detection_list_baseline\n",
    "\n",
    "    \n",
    "\n",
    "def special_tokens_correction_word(ocr_word, gt_word, correction_list_baseline, all_lists_tokens, result): \n",
    "    homonyms, vocab_BERT, vocab_word2vec, hist_expressions, infrequent_expressions, dictionary = all_lists_tokens\n",
    "    special_token = False\n",
    "    homonym, hist_exp, OOV, infreq, RWE = False, False, False, False, False\n",
    "    # check if word is homonym\n",
    "    if gt_word in homonyms:\n",
    "        homonym = True\n",
    "        special_token = True\n",
    "    # check if word is historical expression\n",
    "    if gt_word in hist_expressions:\n",
    "        hist_exp = True\n",
    "        special_token = True\n",
    "    # check if word is OOV\n",
    "    if gt_word not in vocab_word2vec:\n",
    "        OOV = True\n",
    "        special_token = True\n",
    "    # check if word is infrequent\n",
    "    if gt_word in infrequent_expressions:\n",
    "        infreq = True\n",
    "        special_token = True\n",
    "    # check if word is RWE\n",
    "    if ocr_word in dictionary:\n",
    "        RWE = True\n",
    "        special_token = True\n",
    "    # adding the results to the right list\n",
    "    if result == 'right': # wrong = [0]\n",
    "        # all = detection_lit[5]\n",
    "        correction_list_baseline[5][0] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            correction_list_baseline[0][0] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            correction_list_baseline[1][0] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            correction_list_baseline[2][0] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            correction_list_baseline[3][0] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            correction_list_baseline[4][0] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            correction_list_baseline[6][0] += 1\n",
    "    if result == 'wrong': # right = [1]\n",
    "        # all = detection_lit[5]\n",
    "        correction_list_baseline[5][1] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            correction_list_baseline[0][1] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            correction_list_baseline[1][1] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            correction_list_baseline[2][1] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            correction_list_baseline[3][1] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            correction_list_baseline[4][1] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            correction_list_baseline[6][1] += 1\n",
    "    return correction_list_baseline\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "741c3b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n",
      "arrived new\n",
      "OCR_text: ['12', 'Een', 'hoekenpan', 'of', 'kortweg', 'pan', 'is', 'een', 'platte', 'pan', 'met', 'een', 'hang', 'handvat', '\\nDe', 'pan', 'ontleent', 'zijn', 'naam', 'haan', 'het', 'feit', 'dat', 'in', \"zo'n\", 'pan', '12', 'pannenkoeken', 'horden', 'gebakken', '', 'Ook', 'ander', 'voedsel,', 'zoals', 'vlees,', 'word', 'in', 'een', 'hoekenpan', 'gebraden', '12', 'coninghs-merck']\n",
      "len OCR: 43\n",
      "GT_text: ['12', 'Een', 'koekenpan', 'of', 'kortweg', 'pan', 'is', 'een', 'platte', 'pan', 'met', 'een', 'lang', 'handvat', '\\nDe', 'pan', 'ontleent', 'zijn', 'naam', 'aan', 'het', 'feit', 'dat', 'in', \"zo'n\", 'pan', '12', 'pannenkoeken', 'worden', 'gebakken', '', 'Ook', 'ander', 'voedsel,', 'zoals', 'vlees,', 'wordt', 'in', 'een', 'koekenpan', 'gebraden', '12', 'coninghs-merck']\n",
      "len GT: 43\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_documents = []\n",
    "#detection test word2vec\n",
    "def detection_and_correction_dict(row, dictionary, ocr_names, all_lists_token, window=5, topn_detection=1000, topn_correction=1000, correction_method = 'sorted'):  # choose 'sorted'/ 'sorted_nosw', 'calculated'\n",
    "    if row['set'] != 'test':\n",
    "        return np.nan\n",
    "    else:\n",
    "        print('arrived new')\n",
    "        identifier = row['identifier']\n",
    "        OCR_text = row['aligned_OCR_sentences']\n",
    "        GT_text = row['aligned_GT_sentences']\n",
    "        OCR_text = ast.literal_eval(OCR_text)\n",
    "        GT_text = ast.literal_eval(GT_text)\n",
    "        OCR_text = list_merger(OCR_text)\n",
    "        GT_text = list_merger(GT_text)\n",
    "        print('OCR_text:', OCR_text)\n",
    "        print('len OCR:', len(OCR_text))\n",
    "        print('GT_text:', GT_text)\n",
    "        print('len GT:', len(GT_text))\n",
    "\n",
    "        # keep track of performance detection\n",
    "        homonyms_detection_baseline = [0,0,0,0]\n",
    "        histexp_detection_baseline = [0,0,0,0]\n",
    "        OOV_detection_baseline = [0,0,0,0]\n",
    "        infreq_detection_baseline = [0,0,0,0]\n",
    "        RWE_detection_baseline = [0,0,0,0]\n",
    "        all_detection_baseline = [0,0,0,0]\n",
    "        none_detection_baseline = [0,0,0,0]\n",
    "        \n",
    "        # keep track of performance correction right / wrong\n",
    "        homonyms_correction_baseline = [0,0]\n",
    "        histexp_correction_baseline = [0,0]\n",
    "        OOV_correction_baseline = [0,0]\n",
    "        infreq_correction_baseline = [0,0]\n",
    "        RWE_correction_baseline = [0,0]\n",
    "        all_correction_baseline = [0,0]\n",
    "        none_correction_baseline = [0,0]\n",
    "        \n",
    "        # create lists that save evaluation scores for this documents\n",
    "        detection_list_baseline = [homonyms_detection_baseline, histexp_detection_baseline, OOV_detection_baseline, infreq_detection_baseline, RWE_detection_baseline, all_detection_baseline, none_detection_baseline]\n",
    "        correction_list_baseline = [homonyms_correction_baseline, histexp_correction_baseline, OOV_correction_baseline, infreq_correction_baseline, RWE_correction_baseline, all_correction_baseline, none_correction_baseline]\n",
    "                \n",
    "        improved = 0 # when actual error is detected, and corrected rightly\n",
    "        worsened  = 0 # when actual non error is wrongfully detected, and corrected wrongly\n",
    "        \n",
    "        # create corrected file\n",
    "        new_document = []\n",
    "        \n",
    "        \n",
    "        for i in range(len(OCR_text)):\n",
    "            if (OCR_text[i] in ocr_names) or (OCR_text[i].isalpha() == False) or (len(OCR_text[i]) <= 2)  or (GT_text[i] == 'REMOVED'):\n",
    "                # add word to document if left unchanged\n",
    "                new_document.append(OCR_text[i])\n",
    "                continue\n",
    "            # determine if token is predicted error or not\n",
    "            if OCR_text[i] in dictionary:\n",
    "                predicted_error = False\n",
    "            elif OCR_text[i] not in dictionary:\n",
    "                predicted_error = True\n",
    "            # determine if token is actual error or not\n",
    "            if OCR_text[i] != GT_text[i]:\n",
    "                actual_error = True\n",
    "            elif OCR_text[i] == GT_text[i]:\n",
    "                actual_error = False\n",
    "            result_det = calculate_result(predicted_error, actual_error)\n",
    "            # evaluate detection\n",
    "            detection_list_baseline = special_tokens_detection_word(OCR_text[i], GT_text[i], detection_list_baseline, all_lists_token, result_det)\n",
    "            # correction evaluation\n",
    "            if actual_error == True:\n",
    "                # calculate positions detection task\n",
    "                # try two correction methods\n",
    "                # first calculate the normalized LDs:\n",
    "                LD = list(np.array([fuzz.ratio(OCR_text[i], word)/100 for word in dictionary]))\n",
    "                max_index = LD.index(max(LD))\n",
    "                correction = dictionary[max_index]\n",
    "                if correction == GT_text[i]:\n",
    "                    result_cor = 'right'\n",
    "                elif correction != GT_text[i]:\n",
    "                    result_cor = 'wrong'\n",
    "                correction_list_baseline = special_tokens_correction_word(OCR_text[i], GT_text[i], detection_list_baseline, all_lists_token, result_cor)\n",
    "                \n",
    "            # perform whole task\n",
    "            # first, add OCR-word to file if skipped (see above)\n",
    "            # add word to document if not detected as an error\n",
    "            if predicted_error == False:\n",
    "                new_document.append(OCR_text[i])\n",
    "                continue\n",
    "            # if predicted to be an error, perform correction:\n",
    "            if actual_error == True:\n",
    "                correction = correction # correction was already created\n",
    "            elif actual_error == False:\n",
    "                LD = list(np.array([fuzz.ratio(OCR_text[i], word)/100 for word in dictionary]))\n",
    "                max_index = LD.index(max(LD))\n",
    "                correction = dictionary[max_index]\n",
    "                if correction == GT_text[i]:\n",
    "                    result_cor = 'right'\n",
    "                elif correction != GT_text[i]:\n",
    "                    result_cor = 'wrong'\n",
    "            new_document.append(correction)\n",
    "            \n",
    "            if (result_det == 'TP') and (result_cor == 'right'):\n",
    "                improved += 1\n",
    "            elif (result_det == 'FP') and (result_cor == 'wrong'):\n",
    "                worsened += 1\n",
    "                \n",
    "        improved_all.append(improved)\n",
    "        worsened_all.append(worsened)\n",
    "            \n",
    "        new_document = (' ').join(new_document)\n",
    "        new_document = re.sub(' +', ' ', new_document)\n",
    "        new_documents.append(new_document)\n",
    "        \n",
    "        \n",
    "        for k in range(len(detection_list_baseline[0])): # for each result: 0 = TP, 1 = TN, 2 = FP, 3 = TN\n",
    "                # homonyms = index 0 in detection_list_baseline    \n",
    "                homonyms_detection_list_baseline[k].append(detection_list_baseline[0][k])\n",
    "                # hist_exp = index 1\n",
    "                histexp_detection_list_baseline[k].append(detection_list_baseline[1][k])\n",
    "                # OOV = index 2\n",
    "                OOV_detection_list_baseline[k].append(detection_list_baseline[2][k])\n",
    "                # infreq = index 3\n",
    "                infreq_detection_list_baseline[k].append(detection_list_baseline[3][k])\n",
    "                # RWE = index 4\n",
    "                RWE_detection_list_baseline[k].append(detection_list_baseline[4][k])\n",
    "                # all = index 5\n",
    "                all_detection_list_baseline[k].append(detection_list_baseline[5][k])\n",
    "                # non = index 6\n",
    "                none_detection_list_baseline[k].append(detection_list_baseline[6][k])\n",
    "        \n",
    "        # return correction evaluation values:\n",
    "        for k in range(2): # for each result: 0 = right, 1 = wrong\n",
    "                # homonyms = index 0 in detection_list_baseline    \n",
    "                homonyms_correction_list_baseline[k].append(correction_list_baseline[0][k])\n",
    "                # hist_exp = index 1\n",
    "                histexp_correction_list_baseline[k].append(correction_list_baseline[1][k])\n",
    "                # OOV = index 2\n",
    "                OOV_correction_list_baseline[k].append(correction_list_baseline[2][k])\n",
    "                # infreq = index 3\n",
    "                infreq_correction_list_baseline[k].append(correction_list_baseline[3][k])\n",
    "                # RWE = index 4\n",
    "                RWE_correction_list_baseline[k].append(correction_list_baseline[4][k])\n",
    "                # all = index 5\n",
    "                all_correction_list_baseline[k].append(correction_list_baseline[5][k])\n",
    "                # non = index 6\n",
    "                none_correction_list_baseline[k].append(correction_list_baseline[6][k])\n",
    "        \n",
    "        \n",
    "#for index, row in df.iterrows():\n",
    "#    detection_word2vec(row)\n",
    "# df.loc[70]\n",
    "fake_test_list_GT_aligned = \"\"\"12 Een koekenpan of kortweg pan is een platte pan met een lang handvat.\n",
    "De pan ontleent zijn naam aan het feit dat in zo'n pan 12 pannenkoeken worden gebakken. Ook ander voedsel, zoals vlees, wordt in een koekenpan gebraden 12 coninghs-merck\"\"\"\n",
    "fake_test_list_OCR_aligned = \"\"\"12 Een hoekenpan of kortweg pan is een platte pan met een hang handvat.\n",
    "De pan ontleent zijn naam haan het feit dat in zo'n pan 12 pannenkoeken horden gebakken. Ook ander voedsel, zoals vlees, word in een hoekenpan gebraden 12 coninghs-merck\"\"\"\n",
    "fake_test_list_GT_aligned = fake_test_list_GT_aligned.split('.')\n",
    "fake_test_list_OCR_aligned = fake_test_list_OCR_aligned.split('.')\n",
    "fake_test_list_GT_aligned = [x.split(' ') for x in fake_test_list_GT_aligned]\n",
    "fake_test_list_OCR_aligned = [x.split(' ') for x in fake_test_list_OCR_aligned]\n",
    "d = {'identifier': ['111'], 'aligned_OCR_sentences': [str(fake_test_list_OCR_aligned)], 'aligned_GT_sentences': [str(fake_test_list_GT_aligned)], 'set': ['test'], 'century': ['1600s'], 'source': ['Meertens']}\n",
    "\n",
    "df_probeer = pd.DataFrame(data=d)\n",
    "print('started')\n",
    "detection_and_correction_dict(df_probeer.loc[0], dictionary, ocr_names, all_lists_tokens)  # choose 'sorted'/\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d8993eb",
   "metadata": {},
   "source": [
    "print(new_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f6cc36a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"12 en koekenpan of kortweg pan is een platte pan met een hang handvat \\nDe pan ontleent zijn naam haan het feit dat in zo'n pan 12 pannenkoeken horden gebakken ok ander voedsel, zoals vlees, word in een koekenpan gebraden 12 coninghs-merck\"]\n"
     ]
    }
   ],
   "source": [
    "print(new_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ce232be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(new_documents[0].split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a826094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016542ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "03b6334e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [2], [0], [8]]\n",
      "[[0], [8], [0], [19]]\n",
      "[[0], [0], [2], [0]]\n",
      "[[0], [0], [0], [0]]\n",
      "[[0], [8], [0], [0]]\n",
      "[[4], [8], [2], [22]]\n",
      "[[4], [0], [0], [3]]\n"
     ]
    }
   ],
   "source": [
    "print(homonyms_detection_list_baseline)\n",
    "print(histexp_detection_list_baseline)\n",
    "print(OOV_detection_list_baseline)\n",
    "print(infreq_detection_list_baseline)\n",
    "print(RWE_detection_list_baseline)\n",
    "print(all_detection_list_baseline)\n",
    "print(none_detection_list_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "42fe3ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['111']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "list(df_probeer[df_probeer[\"set\"] == 'test']['identifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2b6fc6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>homonyms_detection TP</th>\n",
       "      <th>homonyms_detection FN</th>\n",
       "      <th>homonyms_detection FP</th>\n",
       "      <th>homonyms_detection TN</th>\n",
       "      <th>histexp_detection TP</th>\n",
       "      <th>histexp_detection FN</th>\n",
       "      <th>histexp_detection FP</th>\n",
       "      <th>histexp_detection TN</th>\n",
       "      <th>OOV_detection TP</th>\n",
       "      <th>OOV_detection FN</th>\n",
       "      <th>...</th>\n",
       "      <th>all_detection FN</th>\n",
       "      <th>all_detection FP</th>\n",
       "      <th>all_detection TN</th>\n",
       "      <th>none_detection TP</th>\n",
       "      <th>none_detection FN</th>\n",
       "      <th>none_detection FP</th>\n",
       "      <th>none_detection TN</th>\n",
       "      <th>identifier</th>\n",
       "      <th>century</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>111</td>\n",
       "      <td>1600s</td>\n",
       "      <td>Meertens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   homonyms_detection TP  homonyms_detection FN  homonyms_detection FP  \\\n",
       "0                      0                      2                      0   \n",
       "\n",
       "   homonyms_detection TN  histexp_detection TP  histexp_detection FN  \\\n",
       "0                      8                     0                     8   \n",
       "\n",
       "   histexp_detection FP  histexp_detection TN  OOV_detection TP  \\\n",
       "0                     0                    19                 0   \n",
       "\n",
       "   OOV_detection FN  ...  all_detection FN  all_detection FP  \\\n",
       "0                 0  ...                 8                 2   \n",
       "\n",
       "   all_detection TN  none_detection TP  none_detection FN  none_detection FP  \\\n",
       "0                22                  4                  0                  0   \n",
       "\n",
       "   none_detection TN  identifier  century    source  \n",
       "0                  3         111    1600s  Meertens  \n",
       "\n",
       "[1 rows x 31 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'homonyms_detection TP': homonyms_detection_list_baseline[0], 'homonyms_detection FN': homonyms_detection_list_baseline[1], 'homonyms_detection FP': homonyms_detection_list_baseline[2], 'homonyms_detection TN': homonyms_detection_list_baseline[3], \\\n",
    "    'histexp_detection TP': histexp_detection_list_baseline[0], 'histexp_detection FN': histexp_detection_list_baseline[1], 'histexp_detection FP': histexp_detection_list_baseline[2], 'histexp_detection TN': histexp_detection_list_baseline[3], \\\n",
    "    'OOV_detection TP': OOV_detection_list_baseline[0], 'OOV_detection FN': OOV_detection_list_baseline[1], 'OOV_detection FP': OOV_detection_list_baseline[2], 'OOV_detection TN': OOV_detection_list_baseline[3], \\\n",
    "    'infreq_detection TP': infreq_detection_list_baseline[0], 'infreq_detection FN': infreq_detection_list_baseline[1], 'infreq_detection FP': infreq_detection_list_baseline[2], 'infreq_detection TN': infreq_detection_list_baseline[3], \\\n",
    "    'RWE_detection TP': RWE_detection_list_baseline[0], 'RWE_detection FN': RWE_detection_list_baseline[1], 'RWE_detection FP': RWE_detection_list_baseline[2], 'RWE_detection TN': RWE_detection_list_baseline[3], \\\n",
    "    'all_detection TP': all_detection_list_baseline[0], 'all_detection FN': all_detection_list_baseline[1], 'all_detection FP': all_detection_list_baseline[2], 'all_detection TN': all_detection_list_baseline[3], \\\n",
    "    'none_detection TP': none_detection_list_baseline[0], 'none_detection FN': none_detection_list_baseline[1], 'none_detection FP': none_detection_list_baseline[2], 'none_detection TN': none_detection_list_baseline[3], \\\n",
    "    'identifier': list(df_probeer[df_probeer[\"set\"] == 'test']['identifier']), 'century': list(df_probeer[df_probeer[\"set\"] == 'test']['century']), 'source': list(df_probeer[df_probeer[\"set\"] == 'test']['source'])  }\n",
    "baseline_detection = pd.DataFrame(data=d)\n",
    "\n",
    "baseline_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "95893ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>homonyms_correction right</th>\n",
       "      <th>homonyms_correction wrong</th>\n",
       "      <th>histexp_correction right</th>\n",
       "      <th>histexp_correction wrong</th>\n",
       "      <th>OOV_correction right</th>\n",
       "      <th>OOV_correction wrong</th>\n",
       "      <th>infreq_correction right</th>\n",
       "      <th>infreq_correction wrong</th>\n",
       "      <th>RWE_correction right</th>\n",
       "      <th>RWE_correction wrong</th>\n",
       "      <th>all_correction right</th>\n",
       "      <th>all_correction wrong</th>\n",
       "      <th>none_correction right</th>\n",
       "      <th>none_correction wrong</th>\n",
       "      <th>identifier</th>\n",
       "      <th>century</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1600s</td>\n",
       "      <td>Meertens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   homonyms_correction right  homonyms_correction wrong  \\\n",
       "0                          0                          2   \n",
       "\n",
       "   histexp_correction right  histexp_correction wrong  OOV_correction right  \\\n",
       "0                         0                         8                     0   \n",
       "\n",
       "   OOV_correction wrong  infreq_correction right  infreq_correction wrong  \\\n",
       "0                     0                        0                        0   \n",
       "\n",
       "   RWE_correction right  RWE_correction wrong  all_correction right  \\\n",
       "0                     0                     8                     4   \n",
       "\n",
       "   all_correction wrong  none_correction right  none_correction wrong  \\\n",
       "0                     8                      4                      0   \n",
       "\n",
       "  identifier century    source  \n",
       "0        111   1600s  Meertens  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'homonyms_correction right': homonyms_correction_list_baseline[0], 'homonyms_correction wrong': homonyms_correction_list_baseline[1],\\\n",
    "    'histexp_correction right': histexp_correction_list_baseline[0], 'histexp_correction wrong': histexp_correction_list_baseline[1], \\\n",
    "    'OOV_correction right': OOV_correction_list_baseline[0], 'OOV_correction wrong': OOV_correction_list_baseline[1],\\\n",
    "    'infreq_correction right': infreq_correction_list_baseline[0], 'infreq_correction wrong': infreq_correction_list_baseline[1],\\\n",
    "    'RWE_correction right': RWE_correction_list_baseline[0], 'RWE_correction wrong': RWE_correction_list_baseline[1],\\\n",
    "    'all_correction right': all_correction_list_baseline[0], 'all_correction wrong': all_correction_list_baseline[1],\\\n",
    "    'none_correction right': none_correction_list_baseline[0], 'none_correction wrong': none_correction_list_baseline[1],\\\n",
    "     'identifier': list(df_probeer[df_probeer[\"set\"] == 'test']['identifier']), 'century': list(df_probeer[df_probeer[\"set\"] == 'test']['century']), 'source': list(df_probeer[df_probeer[\"set\"] == 'test']['source'])}\n",
    "baseline_correction = pd.DataFrame(data=d)\n",
    "\n",
    "baseline_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "db0ada69",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_categories_baseline = \"homonyms_detection_baseline, histexp_detection_baseline, OOV_detection_baseline, infreq_detection_baseline, RWE_detection_baseline, all_detection_baseline, none_detection_baseline\".replace('_baseline', '').split(', ')\n",
    "\n",
    "for category in detection_categories_baseline:\n",
    "    TP, FN, FP, TN = int(baseline_detection[f'{category} TP']), int(baseline_detection[f'{category} FN']),  int(baseline_detection[f'{category} FP']),  int(baseline_detection[f'{category} TN']),    \n",
    "    try:\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        F1 = 2*((precision*recall)/(precision+recall))\n",
    "    except ZeroDivisionError:\n",
    "        if (TP == 0) and (FP == 0) and (FN == 0):\n",
    "            precision = recall = F1 = 1\n",
    "        elif (TP == 0) and ((FP > 0) or (FN > 0)):\n",
    "            precision = recall = F1 = 0 \n",
    "    try:\n",
    "        accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "    except ZeroDivisionError:\n",
    "        accuracy = np.nan\n",
    "    baseline_detection[f'{category} precision'] = precision\n",
    "    baseline_detection[f'{category} recall'] = recall\n",
    "    baseline_detection[f'{category} F1'] = F1\n",
    "    baseline_detection[f'{category} accuracy'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "37fc77cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_categories_baseline = \"homonyms_correction_baseline, histexp_correction_baseline, OOV_correction_baseline, infreq_correction_baseline, RWE_correction_baseline, all_correction_baseline, none_correction_baseline\".replace('_baseline', '').split(', ')\n",
    "\n",
    "for category in correction_categories_baseline:\n",
    "    right, wrong = int(baseline_correction[f'{category} right']), int(baseline_correction[f'{category} wrong'])    \n",
    "    try:\n",
    "        accuracy = right/(right+wrong)\n",
    "    except ZeroDivisionError:\n",
    "        accuracy = np.nan\n",
    "    baseline_correction[f'{category} accuracy'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7a6b2d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>homonyms_correction right</th>\n",
       "      <th>homonyms_correction wrong</th>\n",
       "      <th>histexp_correction right</th>\n",
       "      <th>histexp_correction wrong</th>\n",
       "      <th>OOV_correction right</th>\n",
       "      <th>OOV_correction wrong</th>\n",
       "      <th>infreq_correction right</th>\n",
       "      <th>infreq_correction wrong</th>\n",
       "      <th>RWE_correction right</th>\n",
       "      <th>RWE_correction wrong</th>\n",
       "      <th>all_correction right</th>\n",
       "      <th>all_correction wrong</th>\n",
       "      <th>none_correction right</th>\n",
       "      <th>none_correction wrong</th>\n",
       "      <th>identifier</th>\n",
       "      <th>century</th>\n",
       "      <th>source</th>\n",
       "      <th>homonyms_correction accuracy</th>\n",
       "      <th>histexp_correction accuracy</th>\n",
       "      <th>OOV_correction accuracy</th>\n",
       "      <th>infreq_correction accuracy</th>\n",
       "      <th>RWE_correction accuracy</th>\n",
       "      <th>all_correction accuracy</th>\n",
       "      <th>none_correction accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1600s</td>\n",
       "      <td>Meertens</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   homonyms_correction right  homonyms_correction wrong  \\\n",
       "0                          0                          2   \n",
       "\n",
       "   histexp_correction right  histexp_correction wrong  OOV_correction right  \\\n",
       "0                         0                         8                     0   \n",
       "\n",
       "   OOV_correction wrong  infreq_correction right  infreq_correction wrong  \\\n",
       "0                     0                        0                        0   \n",
       "\n",
       "   RWE_correction right  RWE_correction wrong  all_correction right  \\\n",
       "0                     0                     8                     4   \n",
       "\n",
       "   all_correction wrong  none_correction right  none_correction wrong  \\\n",
       "0                     8                      4                      0   \n",
       "\n",
       "  identifier century    source  homonyms_correction accuracy  \\\n",
       "0        111   1600s  Meertens                           0.0   \n",
       "\n",
       "   histexp_correction accuracy  OOV_correction accuracy  \\\n",
       "0                          0.0                      NaN   \n",
       "\n",
       "   infreq_correction accuracy  RWE_correction accuracy  \\\n",
       "0                         NaN                      0.0   \n",
       "\n",
       "   all_correction accuracy  none_correction accuracy  \n",
       "0                 0.333333                       1.0  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "baseline_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3695652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_text = \"\"\"12 Een koekenpan of kortweg pan is een platte pan met een lang handvat.\n",
    "De pan ontleent zijn naam aan het feit dat in zo'n pan 12 pannenkoeken worden gebakken. Ook ander voedsel, zoals vlees, wordt in een koekenpan gebraden 12 coninghs-merck\"\"\".lower()\n",
    "df_probeer['gt text'] = [gt_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a6eed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539be125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = baseline_detection.filter(regex='homonyms|OOV|all').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a6028ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'corrected document': new_documents, 'gt text': list(df_probeer['gt text']),'identifier': list(df_probeer[df_probeer[\"set\"] == 'test']['identifier']), 'century': list(df_probeer[df_probeer[\"set\"] == 'test']['century']), 'source': list(df_probeer[df_probeer[\"set\"] == 'test']['source']), \\\n",
    "    'improved': improved_all, 'worsened': worsened_all, 'old WER': [0.20], 'old CER': [0.30]}\n",
    "whole_task_baseline = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f8091b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d89555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a0e8e4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "jar_file = \"ocrevalUAtion-1.3.4-jar-with-dependencies.jar\"\n",
    "\n",
    "def evaluation(index, row):\n",
    "    ID = row['identifier']\n",
    "    page = 'None'\n",
    "    corrected_OCR = re.sub(' +', ' ', str(row['corrected document'].replace('.', '')))\n",
    "    gt_text = re.sub(' +', ' ', str(row['gt text'].replace('.', '')))\n",
    "    filename_ocr = f\"{ID}_{page}_OCR.txt\"\n",
    "    #file_ocr = open(os.path.join(save_path, filename),\"w+\", encoding=\"utf-8\")\n",
    "    file_ocr = open(filename_ocr,\"w+\", encoding=\"utf-8\")\n",
    "    file_ocr.write(corrected_OCR)\n",
    "    file_ocr.close()\n",
    "    \n",
    "    filename_gt = f\"{ID}_{page}_GT.txt\"\n",
    "    #file_gt = open(os.path.join(save_path, filename),\"w+\", encoding=\"utf-8\")\n",
    "    file_gt = open(filename_gt,\"w+\", encoding=\"utf-8\")\n",
    "    file_gt.write(gt_text)\n",
    "    file_gt.close()\n",
    "    \n",
    "    #output = ID + '_' + page + \".html\"\n",
    "    output = f\"{ID}_{page}.html\"\n",
    "    \n",
    "    #process = subprocess.call(\"/home/nvanthof/jdk-16.0.1/bin/java -cp \" + jar_file  + \" eu.digitisation.Main -gt \" + filename_gt + \" -ocr \"+ filename_ocr +\" -o \" + output + \"\")\n",
    "    #os.system(\"/home/nvanthof/jdk-16.0.1/bin/java -cp /home/nvanthof/ocrevalUAtion-1.3.4-jar-with-dependencies.jar eu.digitisation.Main -gt /home/nvanthof/ddd.010728187.mpeg21.a0005_None_GT.txt -ocr /home/nvanthof/ddd.010728187.mpeg21.a0005_None_OCR.txt  -o /home/nvanthof/OUTPUT2.html\")\n",
    "    command = f\"/home/nvanthof/jdk-16.0.1/bin/java -cp /home/nvanthof/ocrevalUAtion-1.3.4-jar-with-dependencies.jar eu.digitisation.Main -gt /home/nvanthof/{filename_gt} -ocr /home/nvanthof/{filename_ocr}  -o /home/nvanthof/{output}\"\n",
    "    os.system(command)\n",
    "    sleep(5)\n",
    "    \n",
    "    soup = BeautifulSoup(open(output, encoding='utf-8'))\n",
    "    table = soup.find(\"table\", attrs={'border': '1'})\n",
    "    # Split the filename, and extract the identifier and pagenr together as identifier \n",
    "    # Find the first table (this is the table in which the scores are stored)\n",
    "    # Find the tags in which 'CER', 'WER', and 'WER (order independent)' are stored and take the next tag to get the score \n",
    "    cer = table.find('td', text='CER')\n",
    "    cerScore = cer.findNext('td')\n",
    "    wer = table.find('td', text='WER')\n",
    "    werScore = wer.findNext('td')\n",
    "    werOI = table.find('td', text='WER (order independent)')\n",
    "    werOIScore = werOI.findNext('td')\n",
    "    \n",
    "    os.remove(filename_gt)\n",
    "    os.remove(filename_ocr)\n",
    "    os.remove(output)\n",
    "    return float(cerScore.text), float(werScore.text)   \n",
    "    \n",
    "    return cerScore.text, werScore.text\n",
    "\n",
    "for index, row in whole_task_baseline.iterrows():\n",
    "    if index%1000 == 0:\n",
    "        print(index)\n",
    "    whole_task_baseline.at[index, 'CER after correction'], whole_task_baseline.at[index, 'WER after correction'] = evaluation(index, row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4c11404b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corrected document</th>\n",
       "      <th>gt text</th>\n",
       "      <th>identifier</th>\n",
       "      <th>century</th>\n",
       "      <th>source</th>\n",
       "      <th>improved</th>\n",
       "      <th>worsened</th>\n",
       "      <th>old WER</th>\n",
       "      <th>old CER</th>\n",
       "      <th>CER after correction</th>\n",
       "      <th>WER after correction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12 en koekenpan of kortweg pan is een platte p...</td>\n",
       "      <td>12 een koekenpan of kortweg pan is een platte ...</td>\n",
       "      <td>111</td>\n",
       "      <td>1600s</td>\n",
       "      <td>Meertens</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.93</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  corrected document  \\\n",
       "0  12 en koekenpan of kortweg pan is een platte p...   \n",
       "\n",
       "                                             gt text identifier century  \\\n",
       "0  12 een koekenpan of kortweg pan is een platte ...        111   1600s   \n",
       "\n",
       "     source  improved  worsened  old WER  old CER  CER after correction  \\\n",
       "0  Meertens         2         2      0.2      0.3                  2.93   \n",
       "\n",
       "   WER after correction  \n",
       "0                 16.67  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_task_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "739ab641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    52.71\n",
       "1    15.36\n",
       "2    55.54\n",
       "3    19.65\n",
       "4    14.12\n",
       "Name: CER, dtype: float64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b07f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b26e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c49dfdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e886b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d0564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
