{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "066d5573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec\n",
    "\n",
    "# do not forget to set the parameters topn_detection, topn_correction, and method\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle\n",
    "import ast\n",
    "import statistics as s\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "import copy\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import subprocess\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.dammit import EncodingDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87b4f369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1+cu102'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "#transformers.__version__\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0864378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec model\n",
    "#w2v_model = ...\n",
    "# load BERT model\n",
    "#BERT_model = ...\n",
    "# load dataframe\n",
    "#df = ...\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "BERT_model = BertForMaskedLM.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "#w2v_model.intersect_word2vec_format(r\"combined-160.txt\", binary=False, lockf=1.0)\n",
    "# https://github.com/clips/dutchembeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "960e2466",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessed_df100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec7d07bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvanthof/.local/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "def finetune_word2vec(train, window=5):\n",
    "    sentences = train.split('.')\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sentences = [tokenizer.tokenize(i) for i in sentences]\n",
    "    total_examples = len(sentences)\n",
    "    \n",
    "    model_w2v = Word2Vec(size=160, min_count=1, window=window)\n",
    "    model_w2v.build_vocab(sentences)\n",
    "    total_examples = model_w2v.corpus_count\n",
    "    model = KeyedVectors.load_word2vec_format(r\"combined-160.txt\", binary=False)\n",
    "    model_w2v.build_vocab([list(model.vocab.keys())], update=True)\n",
    "    model_w2v.intersect_word2vec_format(r\"combined-160.txt\", binary=False, lockf=1.0)\n",
    "    model_w2v.train(sentences, total_examples=total_examples, epochs=model_w2v.iter)\n",
    "    return model_w2v\n",
    "\n",
    "train = df['gt text'][0]\n",
    "word2vec_model = finetune_word2vec(train)\n",
    "#word2vec_model = KeyedVectors.load_word2vec_format(r\"combined-160.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcabc4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('all_lists_tokens.txt', 'rb') as f:\n",
    "#    all_lists_tokens = pickle.load(f)\n",
    "    \n",
    "#all_lists_tokens = ast.literal_eval(all_lists_tokens)\n",
    "#vocab_BERT, vocab_word2vec, hist_expressions, modern_vocab, dictionary = all_lists_tokens\n",
    "\n",
    "with open('homonyms.txt', 'rb') as f:\n",
    "    homonyms = pickle.load(f)\n",
    "with open('vocab_BERT', 'rb') as f:\n",
    "    vocab_BERT = pickle.load(f)\n",
    "with open('vocab_word2vec.txt', 'rb') as f:\n",
    "    vocab_word2vec = pickle.load(f)\n",
    "with open('hist_expressions.txt', 'rb') as f:\n",
    "    hist_expressions = pickle.load(f)\n",
    "with open('infrequent_expressions.txt', 'rb') as f:\n",
    "    infrequent_expressions = pickle.load(f)\n",
    "with open('dictionary.txt', 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60c2651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lists_tokens = [homonyms, vocab_BERT, vocab_word2vec, hist_expressions, infrequent_expressions, dictionary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d7106fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skiplist (words that should not be corrected: names)\n",
    "with open(\"ocr_names.txt\", \"rb\") as fp:   # Unpickling\n",
    "    ocr_names = pickle.load(fp)\n",
    "\n",
    "ocr_names = []\n",
    "for name in ocr_names:\n",
    "    if len(name) >= 5:\n",
    "        ocr_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98e1cb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_merger(lists):\n",
    "    #normal_list = False\n",
    "    #for elem in lists:\n",
    "    #    if type(elem) != list:\n",
    "    #        normal_list = True\n",
    "    #if normal_list == True:\n",
    "    #    return lists\n",
    "    #else:\n",
    "    new_list = []\n",
    "    for elem in lists:\n",
    "        new_list = new_list + elem\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12afdfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sorted(candidates, sim_or_probs, LD): # sorts first by LD, then by similarity/probability\n",
    "    paired_sorted = sorted(zip(LD,sim_or_probs,candidates),key = lambda x: (x[0],x[1]), reverse=True)\n",
    "    LD,sim_or_probs,candidates = zip(*paired_sorted)\n",
    "    correction = candidates[0]\n",
    "    return correction\n",
    "    \n",
    "def correct_calculated(candidates, sim_or_probs, LD): # calculates a score from LD and normalised similarity/probability\n",
    "    inv_LD = 1 - LD\n",
    "    sim_or_probs = np.array(sim_or_probs)\n",
    "    sim_or_probs = np.interp(sim_or_probs, (sim_or_probs.min(), sim_or_probs.max()), (0, 1)).tolist()\n",
    "    score = sim_or_probs / inv_LD\n",
    "    zipped_pairs = zip(score.tolist(), candidates)\n",
    "    sorted_by_score = [x for _, x in sorted(zipped_pairs, reverse=True)]\n",
    "    correction = sorted_by_score[0]\n",
    "    return correction\n",
    "\n",
    "def remove_stopwords(candidates, cosine, LD):\n",
    "    #nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('dutch'))\n",
    "    candidates_nostopwords = []\n",
    "    cosine_nostopwords = []\n",
    "    LD_nostopwords = []\n",
    "    for i in range(len(candidates)):\n",
    "        if candidates[i] not in stop_words:\n",
    "            candidates_nostopwords.append(candidates[i])\n",
    "            cosine_nostopwords.append(cosine[i])\n",
    "            LD_nostopwords.append(LD[i])\n",
    "    LD_nostopwords = np.array(LD_nostopwords)\n",
    "    return candidates_nostopwords, cosine_nostopwords, LD_nostopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "32a2150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of all TP, FN, FP, TN detection:\n",
    "homonyms_detection_list_w2v = [[],[],[],[]]\n",
    "homonyms_detection_context_list_w2v = [[],[],[],[]]\n",
    "histexp_detection_list_w2v = [[],[],[],[]]\n",
    "histexp_detection_context_list_w2v = [[],[],[],[]]\n",
    "OOV_detection_list_w2v = [[],[],[],[]]\n",
    "OOV_detection_context_list_w2v = [[],[],[],[]]\n",
    "infreq_detection_list_w2v = [[],[],[],[]]\n",
    "infreq_detection_context_list_w2v = [[],[],[],[]]\n",
    "RWE_detection_list_w2v = [[],[],[],[]]\n",
    "RWE_detection_context_list_w2v = [[],[],[],[]]\n",
    "all_detection_list_w2v = [[],[],[],[]]\n",
    "none_detection_list_w2v = [[],[],[],[]]\n",
    "none_detection_context_list_w2v = [[],[],[],[]]\n",
    "\n",
    "# list of all right / wrong correction\n",
    "homonyms_correction_list_w2v = [[],[]]\n",
    "homonyms_correction_context_list_w2v = [[],[]]\n",
    "histexp_correction_list_w2v = [[],[]]\n",
    "histexp_correction_context_list_w2v = [[],[]]\n",
    "OOV_correction_list_w2v = [[],[]]\n",
    "OOV_correction_context_list_w2v = [[],[]]\n",
    "infreq_correction_list_w2v = [[],[]]\n",
    "infreq_correction_context_list_w2v = [[],[]]\n",
    "RWE_correction_list_w2v = [[],[]]\n",
    "RWE_correction_context_list_w2v = [[],[]]\n",
    "all_correction_list_w2v = [[],[]]\n",
    "none_correction_list_w2v = [[],[],[],[]]\n",
    "none_correction_context_list_w2v = [[],[],[],[]]\n",
    "\n",
    "#list of outputs corrected texts\n",
    "new_documents = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fbefc844",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_result(predicted_error, actual_error):\n",
    "    if actual_error == True:\n",
    "        if predicted_error == True: # TP\n",
    "            result = 'TP'\n",
    "        if predicted_error == False: # FN\n",
    "            result = 'FN'\n",
    "    if actual_error == False:\n",
    "        if predicted_error == True: # FP\n",
    "            result = 'FP'\n",
    "        if predicted_error == False: # TN\n",
    "            result = 'TN'\n",
    "    print('result')\n",
    "    return result\n",
    "\n",
    "def special_tokens_detection_word(ocr_word, gt_word, detection_list_w2v, all_lists_tokens, result): \n",
    "    homonyms, vocab_BERT, vocab_word2vec, hist_expressions, infrequent_expressions, dictionary = all_lists_tokens\n",
    "    special_token = False\n",
    "    homonym, hist_exp, OOV, infreq, RWE = False, False, False, False, False\n",
    "    # check if word is homonym\n",
    "    if gt_word in homonyms:\n",
    "        homonym = True\n",
    "        special_token = True\n",
    "    # check if word is historical expression\n",
    "    if gt_word in hist_expressions:\n",
    "        hist_exp = True\n",
    "        special_token = True\n",
    "    # check if word is OOV\n",
    "    if gt_word not in vocab_word2vec:\n",
    "        OOV = True\n",
    "        special_token = True\n",
    "    # check if word is infrequent\n",
    "    if gt_word in infrequent_expressions:\n",
    "        infreq = True\n",
    "        special_token = True\n",
    "    # check if word is RWE\n",
    "    if (ocr_word in dictionary) and ((result == 'TP') or (result == 'FN')):\n",
    "        RWE = True\n",
    "        special_token = True\n",
    "    # adding the results to the right list\n",
    "    if result == 'TP': # TP = [0]\n",
    "        # all = detection_lit[5]\n",
    "        detection_list_w2v[5][0] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_w2v[0][0] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_w2v[1][0] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_w2v[2][0] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_w2v[3][0] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_w2v[4][0] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_w2v[6][0] += 1\n",
    "    if result == 'FN': # FN = [1]\n",
    "        # all = detection_lit[5]\n",
    "        detection_list_w2v[5][1] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_w2v[0][1] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_w2v[1][1] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_w2v[2][1] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_w2v[3][1] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_w2v[4][1] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_w2v[6][1] += 1\n",
    "    if result == 'FP': # FP = [2]\n",
    "        # all = detection_lit[5]\n",
    "        detection_list_w2v[5][2] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_w2v[0][2] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_w2v[1][2] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_w2v[2][2] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_w2v[3][2] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_w2v[4][2] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_w2v[6][2] += 1\n",
    "    if result == 'TN': # TN = [3]\n",
    "        # all = detection_list[5]\n",
    "        detection_list_w2v[5][3] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_w2v[0][3] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_w2v[1][3] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_w2v[2][3] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_w2v[3][3] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_w2v[4][3] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_w2v[6][3] += 1\n",
    "    return detection_list_w2v\n",
    "\n",
    "def special_tokens_detection_context(ocr_context, gt_context, detection_list_context_w2v, all_lists_tokens, result): \n",
    "    homonyms, vocab_BERT, vocab_word2vec, hist_expressions, infrequent_expressions, dictionary = all_lists_tokens\n",
    "    special_token = False\n",
    "    homonym, hist_exp, OOV, infreq, RWE = False, False, False, False, False\n",
    "    # check if context contains homonym\n",
    "    homonym = False\n",
    "    hist_exp = False\n",
    "    OOV = False\n",
    "    infreq = False\n",
    "    RWE = False\n",
    "    for word in gt_context:\n",
    "        if word in homonyms:\n",
    "            homonym = True\n",
    "            special_token = True\n",
    "        # check if word is historical expression\n",
    "        if word in hist_expressions:\n",
    "            hist_exp = True\n",
    "            special_token = True\n",
    "        # check if word is OOV\n",
    "        if word not in vocab_word2vec:\n",
    "            OOV = True\n",
    "            special_token = True\n",
    "        # check if word is infrequent\n",
    "        if word in infrequent_expressions:\n",
    "            infreq = True\n",
    "            special_token = True\n",
    "        # check if word is RWE\n",
    "        for i in range(len(ocr_context)):\n",
    "            if (ocr_context[i] != gt_context[i]) and (ocr_context[i] in dictionary):\n",
    "                RWE = True\n",
    "                special_token = True\n",
    "    # adding the results to the right list\n",
    "    if result == 'TP': # TP = [0]\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_context_w2v[0][0] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_context_w2v[1][0] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_context_w2v[2][0] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_context_w2v[3][0] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_context_w2v[4][0] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_context_w2v[5][0] += 1\n",
    "    if result == 'FN': # FN = [1]\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_context_w2v[0][1] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_context_w2v[1][1] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_context_w2v[2][1] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_context_w2v[3][1] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_context_w2v[4][1] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_context_w2v[5][1] += 1\n",
    "    if result == 'FP': # FP = [2]\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_context_w2v[0][2] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_context_w2v[1][2] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_context_w2v[2][2] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_context_w2v[3][2] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_context_w2v[4][2] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_context_w2v[5][2] += 1\n",
    "    if result == 'TN': # TN = [3]\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            detection_list_context_w2v[0][3] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            detection_list_context_w2v[1][3] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            detection_list_context_w2v[2][3] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            detection_list_context_w2v[3][3] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            detection_list_context_w2v[4][3] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            detection_list_context_w2v[5][3] += 1\n",
    "    return detection_list_context_w2v\n",
    "    \n",
    "\n",
    "def special_tokens_correction_word(ocr_word, gt_word, correction_list_w2v, all_lists_tokens, result): \n",
    "    homonyms, vocab_BERT, vocab_word2vec, hist_expressions, infrequent_expressions, dictionary = all_lists_tokens\n",
    "    special_token = False\n",
    "    homonym, hist_exp, OOV, infreq, RWE = False, False, False, False, False\n",
    "    # check if word is homonym\n",
    "    if gt_word in homonyms:\n",
    "        homonym = True\n",
    "        special_token = True\n",
    "    # check if word is historical expression\n",
    "    if gt_word in hist_expressions:\n",
    "        hist_exp = True\n",
    "        special_token = True\n",
    "    # check if word is OOV\n",
    "    if gt_word not in vocab_word2vec:\n",
    "        OOV = True\n",
    "        special_token = True\n",
    "    # check if word is infrequent\n",
    "    if gt_word in infrequent_expressions:\n",
    "        infreq = True\n",
    "        special_token = True\n",
    "    # check if word is RWE\n",
    "    if ocr_word in dictionary:\n",
    "        RWE = True\n",
    "        special_token = True\n",
    "    # adding the results to the right list\n",
    "    if result == 'right': # wrong = [0]\n",
    "        # all = detection_lit[5]\n",
    "        correction_list_w2v[5][0] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            correction_list_w2v[0][0] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            correction_list_w2v[1][0] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            correction_list_w2v[2][0] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            correction_list_w2v[3][0] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            correction_list_w2v[4][0] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            correction_list_w2v[6][0] += 1\n",
    "    if result == 'wrong': # right = [1]\n",
    "        # all = detection_lit[5]\n",
    "        correction_list_w2v[5][1] += 1\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            correction_list_w2v[0][1] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            correction_list_w2v[1][1] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            correction_list_w2v[2][1] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            correction_list_w2v[3][1] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            correction_list_w2v[4][1] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            correction_list_w2v[6][1] += 1\n",
    "    return correction_list_w2v\n",
    "\n",
    "def special_tokens_correction_context(ocr_context, gt_context, correction_list_context_w2v, all_lists_tokens, result): \n",
    "    homonyms, vocab_BERT, vocab_word2vec, hist_expressions, infrequent_expressions, dictionary = all_lists_tokens\n",
    "    special_token = False\n",
    "    homonym, hist_exp, OOV, infreq, RWE = False, False, False, False, False\n",
    "    # check if context contains homonym\n",
    "    homonym = False\n",
    "    hist_exp = False\n",
    "    OOV = False\n",
    "    infreq = False\n",
    "    RWE = False\n",
    "    for word in gt_context:\n",
    "        if word in homonyms:\n",
    "            homonym = True\n",
    "            special_token = True\n",
    "        # check if word is historical expression\n",
    "        if word in hist_expressions:\n",
    "            hist_exp = True\n",
    "            special_token = True\n",
    "        # check if word is OOV\n",
    "        if word not in vocab_word2vec:\n",
    "            OOV = True\n",
    "            special_token = True\n",
    "        # check if word is infrequent\n",
    "        if word in infrequent_expressions:\n",
    "            infreq = True\n",
    "            special_token = True\n",
    "        # check if word is RWE\n",
    "        for i in range(len(ocr_context)):\n",
    "            if (ocr_context[i] != gt_context[i]) and (ocr_context[i] in dictionary):\n",
    "                RWE = True\n",
    "                special_token = True\n",
    "    # adding the results to the right list\n",
    "    if result == 'right': # right = [0]\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            correction_list_context_w2v[0][0] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            correction_list_context_w2v[1][0] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            correction_list_context_w2v[2][0] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            correction_list_context_w2v[3][0] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            correction_list_context_w2v[4][0] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            correction_list_context_w2v[5][0] += 1\n",
    "    if result == 'wrong': # wrong = [1]\n",
    "        if homonym == True:  # homonyms = detection_list[0]\n",
    "            correction_list_context_w2v[0][1] += 1\n",
    "        if hist_exp == True: # hist_exp = detection_list[1]\n",
    "            correction_list_context_w2v[1][1] += 1\n",
    "        if OOV == True: # OOV = detection_list[2]\n",
    "            correction_list_context_w2v[2][1] += 1\n",
    "        if infreq == True: # infreq = detection_list[3]\n",
    "            correction_list_context_w2v[3][1] += 1\n",
    "        if RWE == True: # infreq = detection_list[4]\n",
    "            correction_list_context_w2v[4][1] += 1\n",
    "        if special_token == False: #none = detection_list[6]\n",
    "            correction_list_context_w2v[5][1] += 1\n",
    "    return correction_list_context_w2v\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bd4c154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT len: 43\n",
      "OCR len: 43\n",
      "7\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "result\n",
      "final task\n",
      "detection results: [[1, 1, 6, 2], [5, 3, 17, 2], [0, 0, 2, 0], [0, 0, 0, 0], [5, 3, 0, 0], [7, 5, 22, 2], [2, 2, 3, 0]]\n",
      "correction results: [[1, 1, 6, 2], [5, 3, 17, 2], [0, 0, 2, 0], [0, 0, 0, 0], [5, 3, 0, 0], [7, 5, 22, 2], [2, 2, 3, 0]]\n",
      "k: 0\n",
      "k: 1\n",
      "total detection results: [[1, 1, 6, 2], [5, 3, 17, 2], [0, 0, 2, 0], [0, 0, 0, 0], [5, 3, 0, 0], [7, 5, 22, 2], [2, 2, 3, 0]]\n",
      "total correction results: [[1, 1, 6, 2], [5, 3, 17, 2], [0, 0, 2, 0], [0, 0, 0, 0], [5, 3, 0, 0], [7, 5, 22, 2], [2, 2, 3, 0]]\n",
      "[\"12 den schekman of korige van is den verplantte van met den lang gehandikapte \\nDe van ontzwollen sijn bnam chalan hehet eksit dat in zo'n van 12 netsnoeren den gemakkelek oock anderen voedsel, tazos vlees, overdan in den schekman graef 12 coninghs-merck\"]\n"
     ]
    }
   ],
   "source": [
    "new_documents = []\n",
    "#detection test word2vec\n",
    "def detection_and_correction_word2vec(row, w2v_model, ocr_names,  all_lists_token, window=5, topn_detection=1000, topn_correction=1000, correction_method = 'sorted'):  # choose 'sorted'/ 'sorted_nosw', 'calculated'\n",
    "    if row['set'] != 'test':\n",
    "        return np.nan\n",
    "    else:\n",
    "        biggest_param = max(topn_detection, topn_correction)\n",
    "        identifier = row['identifier']\n",
    "        OCR_text = row['aligned_OCR_sentences']\n",
    "        GT_text = row['aligned_GT_sentences']\n",
    "        OCR_text = ast.literal_eval(OCR_text)\n",
    "        GT_text = ast.literal_eval(GT_text)\n",
    "        OCR_text = list_merger(OCR_text)\n",
    "        GT_text = list_merger(GT_text)\n",
    "        window_range = list(range(0,window))\n",
    "        window_range = np.array(window_range) - ((window - 1) / 2)\n",
    "        print('GT len:', len(OCR_text))\n",
    "        print('OCR len:', len(GT_text))\n",
    "\n",
    "        # keep track of performance detection\n",
    "        homonyms_detection_w2v = [0,0,0,0]\n",
    "        homonyms_detection_context_w2v = [0,0,0,0]\n",
    "        histexp_detection_w2v = [0,0,0,0]\n",
    "        histexp_detection_context_w2v = [0,0,0,0]\n",
    "        OOV_detection_w2v = [0,0,0,0]\n",
    "        OOV_detection_context_w2v = [0,0,0,0]\n",
    "        infreq_detection_w2v = [0,0,0,0]\n",
    "        infreq_detection_context_w2v = [0,0,0,0]\n",
    "        RWE_detection_w2v = [0,0,0,0]\n",
    "        RWE_detection_context_w2v = [0,0,0,0]\n",
    "        all_detection_w2v = [0,0,0,0]\n",
    "        none_detection_w2v = [0,0,0,0]\n",
    "        none_detection_context_w2v = [0,0,0,0]\n",
    "        \n",
    "        # keep track of performance correction right / wrong\n",
    "        homonyms_correction_w2v = [0,0]\n",
    "        homonyms_correction_context_w2v = [0,0]\n",
    "        histexp_correction_w2v = [0,0]\n",
    "        histexp_correction_context_w2v = [0,0]\n",
    "        OOV_correction_w2v = [0,0]\n",
    "        OOV_correction_context_w2v = [0,0]\n",
    "        infreq_correction_w2v = [0,0]\n",
    "        infreq_correction_context_w2v = [0,0]\n",
    "        RWE_correction_w2v = [0,0]\n",
    "        RWE_correction_context_w2v = [0,0]\n",
    "        all_correction_w2v = [0,0]\n",
    "        none_correction_w2v = [0,0]\n",
    "        none_correction_context_w2v = [0,0]\n",
    "        \n",
    "        # create lists that save evaluation scores for this documents\n",
    "        detection_list_w2v = [homonyms_detection_w2v, histexp_detection_w2v, OOV_detection_w2v, infreq_detection_w2v, RWE_detection_w2v, all_detection_w2v, none_detection_w2v]\n",
    "        print(len(detection_list_w2v))\n",
    "        detection_list_context_w2v = [homonyms_detection_context_w2v, histexp_detection_context_w2v, OOV_detection_context_w2v, infreq_detection_context_w2v, RWE_detection_context_w2v, none_detection_context_w2v]\n",
    "        correction_list_w2v = [homonyms_correction_w2v, histexp_correction_w2v, OOV_correction_w2v, infreq_correction_w2v, RWE_correction_w2v, all_correction_w2v, none_correction_w2v]\n",
    "        correction_list_context_w2v = [homonyms_correction_context_w2v, histexp_correction_context_w2v, OOV_correction_context_w2v, infreq_correction_context_w2v, RWE_correction_context_w2v, none_correction_context_w2v]\n",
    "        \n",
    "        # create corrected file\n",
    "        new_document = []\n",
    "        \n",
    "        \n",
    "        for i in range(len(OCR_text)):\n",
    "            if (OCR_text[i] in ocr_names) or (OCR_text[i].isalpha() == False) or (len(OCR_text[i]) <= 2)  or (GT_text[i] == 'REMOVED'):\n",
    "                # add word to document if left unchanged\n",
    "                new_document.append(OCR_text[i])\n",
    "                continue\n",
    "            context = []\n",
    "            GT_context = []\n",
    "            for j in window_range:\n",
    "                if (i+j >= 0) and (i+j < len(OCR_text)) and j != 0:\n",
    "                    #context.append(OCR_text[i+int(j)])\n",
    "                    if any(str.isdigit(c) for c in  OCR_text[i+int(j)]) == True:\n",
    "                        context.append('%NUMBER%')\n",
    "                    elif OCR_text[i+int(j)] in ocr_names:\n",
    "                        context.append('%NNP%')\n",
    "                    else:\n",
    "                        context.append(OCR_text[i+int(j)])\n",
    "                    GT_context.append(GT_text[i+int(j)])\n",
    "                \n",
    "                else:\n",
    "                    pass\n",
    "            # generate list of all candidates\n",
    "            whole_list_candidates = []\n",
    "            whole_list_cosines = []\n",
    "            for prediction in w2v_model.predict_output_word(context, biggest_param):\n",
    "                whole_list_candidates.append(prediction[0])\n",
    "                whole_list_cosines.append(prediction[1]) \n",
    "            # remove punctuation except for hyphen from candidates\n",
    "            whole_list_candidates = [re.sub(r'[^\\w\\d\\s\\-]+', '', x) for x in whole_list_candidates]\n",
    "            # score down for detection task\n",
    "            candidates = copy.deepcopy(whole_list_candidates[:topn_detection])\n",
    "            cosines = copy.deepcopy(whole_list_cosines[:topn_detection])\n",
    "            #calculate positions detection task\n",
    "            # determine if token is predicted error or not\n",
    "            if OCR_text[i] in candidates:\n",
    "                predicted_error = False\n",
    "            elif OCR_text[i] not in candidates:\n",
    "                predicted_error = True\n",
    "            # determine if token is actual error or not\n",
    "            if OCR_text[i] != GT_text[i]:\n",
    "                actual_error = True\n",
    "            elif OCR_text[i] == GT_text[i]:\n",
    "                actual_error = False\n",
    "            result = calculate_result(predicted_error, actual_error)\n",
    "            # evaluate detection\n",
    "            detection_list_w2v = special_tokens_detection_word(OCR_text[i], GT_text[i], detection_list_w2v, all_lists_token, result)\n",
    "            detection_context_list_w2v = special_tokens_detection_context(context, GT_context, detection_list_context_w2v, all_lists_tokens, result)\n",
    "            # return detection evaluation values:\n",
    "            \n",
    "            \n",
    "            # place old detection evaluation\n",
    "            \n",
    "            # correction evaluation\n",
    "            if actual_error == True:\n",
    "                candidates = copy.deepcopy(whole_list_candidates[:topn_correction])\n",
    "                cosines = copy.deepcopy(whole_list_cosines[:topn_correction])\n",
    "                # calculate positions detection task\n",
    "                # try two correction methods\n",
    "                # first calculate the normalized LDs:\n",
    "                LD = np.array([fuzz.ratio(OCR_text[i], word)/100 for word in candidates])\n",
    "                # try sorting method\n",
    "                if correction_method == 'sorted':\n",
    "                    correction = correct_sorted(candidates, cosines, LD)\n",
    "                elif correction_method == 'sorted_nosw':\n",
    "                # try again the sorting methods, but without stopwords\n",
    "                    candidates_nostopwords, cosine_nostopwords, LD_nostopwords = remove_stopwords(candidates, cosines, LD)\n",
    "                    correction = correct_sorted(candidates_nostopwords, cosine_nostopwords, LD_nostopwords)\n",
    "                # try score calculation method\n",
    "                elif correction_method == 'calculated':\n",
    "                    correction = correct_calculated(candidates, cosines, LD)\n",
    "                # evaluation\n",
    "                if correction == GT_text[i]:\n",
    "                    result = 'right'\n",
    "                elif correction != GT_text[i]:\n",
    "                    result = 'wrong'\n",
    "                correction_list_w2v = special_tokens_correction_word(OCR_text[i], GT_text[i], detection_list_w2v, all_lists_token, result)\n",
    "                correction_context_list_w2v = special_tokens_correction_context(context, GT_context, correction_list_context_w2v, all_lists_tokens, result)\n",
    "\n",
    "                # place old correction evaluation\n",
    "\n",
    "            print('final task')\n",
    "            # perform whole task\n",
    "            # first, add OCR-word to file if skipped (see above)\n",
    "            # add word to document if not detected as an error\n",
    "            if predicted_error == False:\n",
    "                new_document.append(OCR_text[i])\n",
    "                continue\n",
    "            # if predicted to be an error, perform correction:\n",
    "            if actual_error == True:\n",
    "                correction = correction # correction was already created\n",
    "            elif actual_error == False:\n",
    "                candidates = copy.deepcopy(whole_list_candidates[:topn_detection])\n",
    "                cosines = copy.deepcopy(whole_list_cosines[:topn_detection])\n",
    "            # first calculate the normalized LDs:\n",
    "            LD = np.array([fuzz.ratio(OCR_text[i], word)/100 for word in candidates])\n",
    "            # try sorting method\n",
    "            if correction_method == 'sorted':\n",
    "                correction = correct_sorted(candidates, cosines, LD)\n",
    "            elif correction_method == 'sorted_nosw':\n",
    "            # try again the sorting methods, but without stopwords\n",
    "                candidates_nostopwords, cosine_nostopwords, LD_nostopwords = remove_stopwords(candidates, cosines, LD)\n",
    "                correction = correct_sorted(candidates_nostopwords, cosine_nostopwords, LD_nostopwords)\n",
    "            # try score calculation method\n",
    "            elif correction_method == 'calculated':\n",
    "                correction = correct_calculated(candidates, cosines, LD)\n",
    "            new_document.append(correction)\n",
    "            \n",
    "        new_document = (' ').join(new_document)\n",
    "        new_document = re.sub(' +', ' ', new_document)\n",
    "        new_documents.append(new_document)\n",
    "        \n",
    "        print('detection results:', detection_list_w2v)\n",
    "        \n",
    "        for k in range(len(detection_list_w2v[0])): # for each result: 0 = TP, 1 = TN, 2 = FP, 3 = TN\n",
    "                # homonyms = index 0 in detection_list_w2v    \n",
    "                homonyms_detection_list_w2v[k].append(detection_list_w2v[0][k])\n",
    "                # hist_exp = index 1\n",
    "                histexp_detection_list_w2v[k].append(detection_list_w2v[1][k])\n",
    "                # OOV = index 2\n",
    "                OOV_detection_list_w2v[k].append(detection_list_w2v[2][k])\n",
    "                # infreq = index 3\n",
    "                infreq_detection_list_w2v[k].append(detection_list_w2v[3][k])\n",
    "                # RWE = index 4\n",
    "                RWE_detection_list_w2v[k].append(detection_list_w2v[4][k])\n",
    "                # all = index 5\n",
    "                all_detection_list_w2v[k].append(detection_list_w2v[5][k])\n",
    "                # non = index 6\n",
    "                none_detection_list_w2v[k].append(detection_list_w2v[6][k])\n",
    "        for k in range(len(detection_list_context_w2v[0])): # for each result: 0 = TP, 1 = TN, 2 = FP, 3 = TN\n",
    "                # homonyms = index 0 in detection_list_w2v    \n",
    "                homonyms_detection_context_list_w2v[k].append(detection_context_list_w2v[0][k])\n",
    "                # hist_exp = index 1\n",
    "                histexp_detection_context_list_w2v[k].append(detection_context_list_w2v[1][k])\n",
    "                # OOV = index 2\n",
    "                OOV_detection_context_list_w2v[k].append(detection_context_list_w2v[2][k])\n",
    "                # infreq = index 3\n",
    "                infreq_detection_context_list_w2v[k].append(detection_context_list_w2v[3][k])\n",
    "                # RWE = index 4\n",
    "                RWE_detection_context_list_w2v[k].append(detection_context_list_w2v[4][k])\n",
    "                # non = index 5\n",
    "                none_detection_context_list_w2v[k].append(detection_context_list_w2v[5][k])\n",
    "        \n",
    "        print('correction results:', correction_list_w2v)\n",
    "        \n",
    "        # return correction evaluation values:\n",
    "        for k in range(2): # for each result: 0 = right, 1 = wrong\n",
    "                print('k:', k)\n",
    "                # homonyms = index 0 in detection_list_w2v    \n",
    "                homonyms_correction_list_w2v[k].append(correction_list_w2v[0][k])\n",
    "                # hist_exp = index 1\n",
    "                histexp_correction_list_w2v[k].append(correction_list_w2v[1][k])\n",
    "                # OOV = index 2\n",
    "                OOV_correction_list_w2v[k].append(correction_list_w2v[2][k])\n",
    "                # infreq = index 3\n",
    "                infreq_correction_list_w2v[k].append(correction_list_w2v[3][k])\n",
    "                # RWE = index 4\n",
    "                RWE_correction_list_w2v[k].append(correction_list_w2v[4][k])\n",
    "                # all = index 5\n",
    "                all_correction_list_w2v[k].append(correction_list_w2v[5][k])\n",
    "                # non = index 6\n",
    "                none_correction_list_w2v[k].append(correction_list_w2v[6][k])\n",
    "        for k in range(2): # for each result: 0 = right, 1 = wrong\n",
    "                # homonyms = index 0 in detection_list_w2v    \n",
    "                homonyms_correction_context_list_w2v[k].append(correction_context_list_w2v[0][k])\n",
    "                # hist_exp = index 1\n",
    "                histexp_correction_context_list_w2v[k].append(correction_context_list_w2v[1][k])\n",
    "                # OOV = index 2\n",
    "                OOV_correction_context_list_w2v[k].append(correction_context_list_w2v[2][k])\n",
    "                # infreq = index 3\n",
    "                infreq_correction_context_list_w2v[k].append(correction_context_list_w2v[3][k])\n",
    "                # RWE = index 4\n",
    "                RWE_correction_context_list_w2v[k].append(correction_context_list_w2v[4][k])\n",
    "                # non = index 5\n",
    "                none_correction_context_list_w2v[k].append(correction_context_list_w2v[5][k])\n",
    "        \n",
    "        print('total detection results:', detection_list_w2v)\n",
    "        print('total correction results:', correction_list_w2v)\n",
    "        \n",
    "#for index, row in df.iterrows():\n",
    "#    detection_word2vec(row)\n",
    "# df.loc[70]\n",
    "fake_test_list_GT_aligned = \"\"\"12 Een koekenpan of kortweg pan is een platte pan met een lang handvat.\n",
    "De pan ontleent zijn naam aan het feit dat in zo'n pan 12 pannenkoeken worden gebakken. Ook ander voedsel, zoals vlees, wordt in een koekenpan gebraden 12 coninghs-merck\"\"\"\n",
    "fake_test_list_OCR_aligned = \"\"\"12 Een hoekenpan of kortweg pan is een platte pan met een hang handvat.\n",
    "De pan ontleent zijn naam haan het feit dat in zo'n pan 12 pannenkoeken horden gebakken. Ook ander voedsel, zoals vlees, word in een hoekenpan gebraden 12 coninghs-merck\"\"\"\n",
    "fake_test_list_GT_aligned = fake_test_list_GT_aligned.split('.')\n",
    "fake_test_list_OCR_aligned = fake_test_list_OCR_aligned.split('.')\n",
    "fake_test_list_GT_aligned = [x.split(' ') for x in fake_test_list_GT_aligned]\n",
    "fake_test_list_OCR_aligned = [x.split(' ') for x in fake_test_list_OCR_aligned]\n",
    "d = {'identifier': ['111'], 'aligned_OCR_sentences': [str(fake_test_list_OCR_aligned)], 'aligned_GT_sentences': [str(fake_test_list_GT_aligned)], 'set': ['test'], 'century': ['1600s'], 'source': ['Meertens']}\n",
    "\n",
    "df_probeer = pd.DataFrame(data=d)\n",
    "\n",
    "detection_and_correction_word2vec(df_probeer.loc[0], word2vec_model, ocr_names, all_lists_tokens)  # choose 'sorted'/\n",
    "\n",
    "print(new_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1bf9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9f1b298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "[['12', 'Een', 'koekenpan', 'of', 'kortweg', 'pan', 'is', 'een', 'platte', 'pan', 'met', 'een', 'lang', 'handvat'], ['\\nDe', 'pan', 'ontleent', 'zijn', 'naam', 'aan', 'het', 'feit', 'dat', 'in', \"zo'n\", 'pan', '12', 'pannenkoeken', 'worden', 'gebakken'], ['', 'Ook', 'ander', 'voedsel,', 'zoals', 'vlees,', 'wordt', 'in', 'een', 'koekenpan', 'gebraden', '12', 'coninghs-merck']]\n",
      "378\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(len(new_documents[0].split(' ')))\n",
    "print(str(df_probeer['aligned_GT_sentences'][0]))\n",
    "print(len(df_probeer['aligned_OCR_sentences'][0]))\n",
    "print(type(df_probeer['aligned_OCR_sentences'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a9a9186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'identifier', 'gt text', 'ocr text', 'CER', 'WER',\n",
      "       'WER (order independent)', 'dictionary lookup gt',\n",
      "       'dictionary lookup ocr', 'jaccard_coefficient',\n",
      "       'levenshtein_dist_normalized', 'source', 'word count gt',\n",
      "       'word count ocr', 'year', 'century', 'old index', 'gt text org',\n",
      "       'ocr text org', 'set', 'gt sentences matched', 'ocr sentences matched',\n",
      "       'CER matched sentences', 'WER matched sentences',\n",
      "       'avg sentence length gt (fuzzy matched)_x',\n",
      "       'avg sentence length ocr (fuzzy matched)_x',\n",
      "       'max sentence length gt (fuzzy matched)_x',\n",
      "       'max sentence length ocr (fuzzy matched)_x',\n",
      "       'sentences gt (fuzzy matched)_x', 'sentences ocr (fuzzy matched)_x',\n",
      "       'word count gt (fuzzy matched)_x', 'word count ocr (fuzzy matched)_x',\n",
      "       'longest_streak', 'avg_longest_streaks', 'avg_total_missing_words',\n",
      "       'avg_perc_missing_words ', 'gt sentences matchated',\n",
      "       'avg sentence length gt (fuzzy matched)_y',\n",
      "       'avg sentence length ocr (fuzzy matched)_y',\n",
      "       'max sentence length gt (fuzzy matched)_y',\n",
      "       'max sentence length ocr (fuzzy matched)_y',\n",
      "       'sentences gt (fuzzy matched)_y', 'sentences ocr (fuzzy matched)_y',\n",
      "       'word count gt (fuzzy matched)_y', 'word count ocr (fuzzy matched)_y',\n",
      "       'aligned_GT_sentences', 'aligned_OCR_sentences', 'good_alignments',\n",
      "       'bad_alignments'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f125c9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df['aligned_OCR_sentences'][99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d3fd2fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], []]\n",
      "[[], [], [], []]\n",
      "[[], [], [], []]\n",
      "[[], [], [], []]\n",
      "[[], [], [], []]\n",
      "[[], [], [], []]\n",
      "[[], [], [], []]\n",
      "[[], [], [], []]\n",
      "[[], [], [], []]\n",
      "[[], [], [], []]\n",
      "[[], [], [], []]\n",
      "[[], [], [], []]\n",
      "[[], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "print(homonyms_detection_list_w2v)\n",
    "print(homonyms_detection_context_list_w2v)\n",
    "print(histexp_detection_list_w2v)\n",
    "print(histexp_detection_context_list_w2v)\n",
    "print(OOV_detection_list_w2v)\n",
    "print(OOV_detection_context_list_w2v)\n",
    "print(infreq_detection_list_w2v)\n",
    "print(infreq_detection_context_list_w2v)\n",
    "print(RWE_detection_list_w2v)\n",
    "print(RWE_detection_context_list_w2v)\n",
    "print(all_detection_list_w2v)\n",
    "print(none_detection_list_w2v)\n",
    "print(none_detection_context_list_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "84002a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with results\n",
    "# lists of all TP, FN, FP, TN detection:\n",
    "homonyms_detection_list_w2v = [[],[],[],[]]\n",
    "homonyms_detection_context_list_w2v = [[],[],[],[]]\n",
    "histexp_detection_list_w2v = [[],[],[],[]]\n",
    "histexp_detection_context_list_w2v = [[],[],[],[]]\n",
    "OOV_detection_list_w2v = [[],[],[],[]]\n",
    "OOV_detection_context_list_w2v = [[],[],[],[]]\n",
    "infreq_detection_list_w2v = [[],[],[],[]]\n",
    "infreq_detection_context_list_w2v = [[],[],[],[]]\n",
    "RWE_detection_list_w2v = [[],[],[],[]]\n",
    "RWE_detection_context_list_w2v = [[],[],[],[]]\n",
    "all_detection_list_w2v = [[],[],[],[]]\n",
    "none_detection_list_w2v = [[],[],[],[]]\n",
    "none_detection_context_list_w2v = [[],[],[],[]]\n",
    "\n",
    "# list of all right / wrong correction\n",
    "homonyms_correction_list_w2v = [[],[]]\n",
    "homonyms_correction_context_list_w2v = [[],[]]\n",
    "histexp_correction_list_w2v = [[],[]]\n",
    "histexp_correction_context_list_w2v = [[],[]]\n",
    "OOV_correction_list_w2v = [[],[]]\n",
    "OOV_correction_context_list_w2v = [[],[]]\n",
    "infreq_correction_list_w2v = [[],[]]\n",
    "infreq_correction_context_list_w2v = [[],[]]\n",
    "RWE_correction_list_w2v = [[],[]]\n",
    "RWE_correction_context_list_w2v = [[],[]]\n",
    "all_correction_list_w2v = [[],[]]\n",
    "none_correction_list_w2v = [[],[],[],[]]\n",
    "none_correction_context_list_w2v = [[],[],[],[]]\n",
    "\n",
    "#list of outputs corrected texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "af491998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>homonyms_detection TP</th>\n",
       "      <th>homonyms_detection FN</th>\n",
       "      <th>homonyms_detection FP</th>\n",
       "      <th>homonyms_detection TN</th>\n",
       "      <th>homonyms_detection context TP</th>\n",
       "      <th>homonyms_detection context FN</th>\n",
       "      <th>homonyms_detection context FP</th>\n",
       "      <th>homonyms_detection context TN</th>\n",
       "      <th>histexp_detection TP</th>\n",
       "      <th>histexp_detection FN</th>\n",
       "      <th>...</th>\n",
       "      <th>none_detection FN</th>\n",
       "      <th>none_detection FP</th>\n",
       "      <th>none_detection TN</th>\n",
       "      <th>none_detection context TP</th>\n",
       "      <th>none_detection context FN</th>\n",
       "      <th>none_detection context FP</th>\n",
       "      <th>none_detection context TN</th>\n",
       "      <th>identifier</th>\n",
       "      <th>century</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1600s</td>\n",
       "      <td>Meertens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   homonyms_detection TP  homonyms_detection FN  homonyms_detection FP  \\\n",
       "0                      1                      1                      6   \n",
       "\n",
       "   homonyms_detection TN  homonyms_detection context TP  \\\n",
       "0                      2                              4   \n",
       "\n",
       "   homonyms_detection context FN  homonyms_detection context FP  \\\n",
       "0                              0                             14   \n",
       "\n",
       "   homonyms_detection context TN  histexp_detection TP  histexp_detection FN  \\\n",
       "0                              2                     5                     3   \n",
       "\n",
       "   ...  none_detection FN  none_detection FP  none_detection TN  \\\n",
       "0  ...                  2                  3                  0   \n",
       "\n",
       "   none_detection context TP  none_detection context FN  \\\n",
       "0                          0                          0   \n",
       "\n",
       "   none_detection context FP  none_detection context TN  identifier  century  \\\n",
       "0                          0                          0         111    1600s   \n",
       "\n",
       "     source  \n",
       "0  Meertens  \n",
       "\n",
       "[1 rows x 55 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'homonyms_detection TP': homonyms_detection_list_w2v[0], 'homonyms_detection FN': homonyms_detection_list_w2v[1], 'homonyms_detection FP': homonyms_detection_list_w2v[2], 'homonyms_detection TN': homonyms_detection_list_w2v[3], \\\n",
    "    'homonyms_detection context TP': homonyms_detection_context_list_w2v[0], 'homonyms_detection context FN': homonyms_detection_context_list_w2v[1], 'homonyms_detection context FP': homonyms_detection_context_list_w2v[2], 'homonyms_detection context TN': homonyms_detection_context_list_w2v[3], \\\n",
    "    'histexp_detection TP': histexp_detection_list_w2v[0], 'histexp_detection FN': histexp_detection_list_w2v[1], 'histexp_detection FP': histexp_detection_list_w2v[2], 'histexp_detection TN': histexp_detection_list_w2v[3], \\\n",
    "    'histexp_detection context TP': histexp_detection_context_list_w2v[0], 'histexp_detection context FN': histexp_detection_context_list_w2v[1], 'histexp_detection context FP': histexp_detection_context_list_w2v[2], 'histexp_detection context TN': histexp_detection_context_list_w2v[3], \\\n",
    "    'OOV_detection TP': OOV_detection_list_w2v[0], 'OOV_detection FN': OOV_detection_list_w2v[1], 'OOV_detection FP': OOV_detection_list_w2v[2], 'OOV_detection TN': OOV_detection_list_w2v[3], \\\n",
    "    'OOV_detection context TP': OOV_detection_context_list_w2v[0], 'OOV_detection context FN': OOV_detection_context_list_w2v[1], 'OOV_detection context FP': OOV_detection_context_list_w2v[2], 'OOV_detection context TN': OOV_detection_context_list_w2v[3], \\\n",
    "    'infreq_detection TP': infreq_detection_list_w2v[0], 'infreq_detection FN': infreq_detection_list_w2v[1], 'infreq_detection FP': infreq_detection_list_w2v[2], 'infreq_detection TN': infreq_detection_list_w2v[3], \\\n",
    "    'infreq_detection context TP': infreq_detection_context_list_w2v[0], 'infreq_detection context FN': infreq_detection_context_list_w2v[1], 'infreq_detection context FP': infreq_detection_context_list_w2v[2], 'infreq_detection context TN': infreq_detection_context_list_w2v[3], \\\n",
    "    'RWE_detection TP': RWE_detection_list_w2v[0], 'RWE_detection FN': RWE_detection_list_w2v[1], 'RWE_detection FP': RWE_detection_list_w2v[2], 'RWE_detection TN': RWE_detection_list_w2v[3], \\\n",
    "    'RWE_detection context TP': RWE_detection_context_list_w2v[0], 'RWE_detection context FN': RWE_detection_context_list_w2v[1], 'RWE_detection context FP': RWE_detection_context_list_w2v[2], 'RWE_detection context TN': RWE_detection_context_list_w2v[3], \\\n",
    "    'all_detection TP': all_detection_list_w2v[0], 'all_detection FN': all_detection_list_w2v[1], 'all_detection FP': all_detection_list_w2v[2], 'all_detection TN': all_detection_list_w2v[3], \\\n",
    "    'none_detection TP': none_detection_list_w2v[0], 'none_detection FN': none_detection_list_w2v[1], 'none_detection FP': none_detection_list_w2v[2], 'none_detection TN': none_detection_list_w2v[3], \\\n",
    "    'none_detection context TP': none_detection_context_list_w2v[0], 'none_detection context FN': none_detection_context_list_w2v[1], 'none_detection context FP': none_detection_context_list_w2v[2], 'none_detection context TN': none_detection_context_list_w2v[3], \\\n",
    "    'identifier': list(df_probeer[df_probeer[\"set\"] == 'test']['identifier']), 'century': list(df_probeer[df_probeer[\"set\"] == 'test']['century']), 'source': list(df_probeer[df_probeer[\"set\"] == 'test']['source'])  }\n",
    "w2v_detection = pd.DataFrame(data=d)\n",
    "\n",
    "w2v_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4ca4567d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>homonyms_correction right</th>\n",
       "      <th>homonyms_correction wrong</th>\n",
       "      <th>homonyms_correction context right</th>\n",
       "      <th>homonyms_correction context wrong</th>\n",
       "      <th>histexp_correction right</th>\n",
       "      <th>histexp_correction wrong</th>\n",
       "      <th>histexp_correction context right</th>\n",
       "      <th>histexp_correction context wrong</th>\n",
       "      <th>OOV_correction right</th>\n",
       "      <th>OOV_correction wrong</th>\n",
       "      <th>...</th>\n",
       "      <th>RWE_correction context wrong</th>\n",
       "      <th>all_correction right</th>\n",
       "      <th>all_correction wrong</th>\n",
       "      <th>none_correction right</th>\n",
       "      <th>none_correction wrong</th>\n",
       "      <th>none_correction context right</th>\n",
       "      <th>none_correction context wrong</th>\n",
       "      <th>identifier</th>\n",
       "      <th>century</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1600s</td>\n",
       "      <td>Meertens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   homonyms_correction right  homonyms_correction wrong  \\\n",
       "0                          1                          1   \n",
       "\n",
       "   homonyms_correction context right  homonyms_correction context wrong  \\\n",
       "0                                  1                                  3   \n",
       "\n",
       "   histexp_correction right  histexp_correction wrong  \\\n",
       "0                         5                         3   \n",
       "\n",
       "   histexp_correction context right  histexp_correction context wrong  \\\n",
       "0                                 1                                 5   \n",
       "\n",
       "   OOV_correction right  OOV_correction wrong  ...  \\\n",
       "0                     0                     0  ...   \n",
       "\n",
       "   RWE_correction context wrong  all_correction right  all_correction wrong  \\\n",
       "0                             0                     7                     5   \n",
       "\n",
       "   none_correction right  none_correction wrong  \\\n",
       "0                      2                      2   \n",
       "\n",
       "   none_correction context right  none_correction context wrong  identifier  \\\n",
       "0                              0                              0         111   \n",
       "\n",
       "   century    source  \n",
       "0    1600s  Meertens  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'homonyms_correction right': homonyms_correction_list_w2v[0], 'homonyms_correction wrong': homonyms_correction_list_w2v[1],\\\n",
    "    'homonyms_correction context right': homonyms_correction_context_list_w2v[0], 'homonyms_correction context wrong': homonyms_correction_context_list_w2v[1], \\\n",
    "    'histexp_correction right': histexp_correction_list_w2v[0], 'histexp_correction wrong': histexp_correction_list_w2v[1], \\\n",
    "    'histexp_correction context right': histexp_correction_context_list_w2v[0], 'histexp_correction context wrong': histexp_correction_context_list_w2v[1], \\\n",
    "    'OOV_correction right': OOV_correction_list_w2v[0], 'OOV_correction wrong': OOV_correction_list_w2v[1],\\\n",
    "    'OOV_correction context right': OOV_correction_context_list_w2v[0], 'OOV_correction context wrong': OOV_correction_context_list_w2v[1],\\\n",
    "    'infreq_correction right': infreq_correction_list_w2v[0], 'infreq_correction wrong': infreq_correction_list_w2v[1],\\\n",
    "    'infreq_correction context right': infreq_correction_context_list_w2v[0], 'infreq_correction context wrong': infreq_correction_context_list_w2v[1], \\\n",
    "    'RWE_correction right': RWE_correction_list_w2v[0], 'RWE_correction wrong': RWE_correction_list_w2v[1],\\\n",
    "    'RWE_correction context right': RWE_correction_context_list_w2v[0], 'RWE_correction context wrong': RWE_correction_context_list_w2v[1],\\\n",
    "    'all_correction right': all_correction_list_w2v[0], 'all_correction wrong': all_correction_list_w2v[1],\\\n",
    "    'none_correction right': none_correction_list_w2v[0], 'none_correction wrong': none_correction_list_w2v[1],\\\n",
    "    'none_correction context right': none_correction_context_list_w2v[0], 'none_correction context wrong': none_correction_context_list_w2v[1], \\\n",
    "     'identifier': list(df_probeer[df_probeer[\"set\"] == 'test']['identifier']), 'century': list(df_probeer[df_probeer[\"set\"] == 'test']['century']), 'source': list(df_probeer[df_probeer[\"set\"] == 'test']['source'])}\n",
    "w2v_correction = pd.DataFrame(data=d)\n",
    "\n",
    "w2v_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0201100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_categories_w2v = \"homonyms_detection_w2v, histexp_detection_w2v, OOV_detection_w2v, infreq_detection_w2v, RWE_detection_w2v, all_detection_w2v, none_detection_w2v, homonyms_detection context_w2v, histexp_detection context_w2v, OOV_detection context_w2v, infreq_detection context_w2v, RWE_detection context_w2v, none_detection context_w2v\".replace('_w2v', '').split(', ')\n",
    "\n",
    "for category in detection_categories_w2v:\n",
    "    TP, FN, FP, TN = int(w2v_detection[f'{category} TP']), int(w2v_detection[f'{category} FN']),  int(w2v_detection[f'{category} FP']),  int(w2v_detection[f'{category} TN']),    \n",
    "    try:\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        F1 = 2*((precision*recall)/(precision+recall))\n",
    "    except ZeroDivisionError:\n",
    "        if (TP == 0) and (FP == 0) and (FN == 0):\n",
    "            precision = recall = F1 = 1\n",
    "        elif (TP == 0) and ((FP > 0) or (FN > 0)):\n",
    "            precision = recall = F1 = 0 \n",
    "    try:\n",
    "        accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "    except ZeroDivisionError:\n",
    "        accuracy = np.nan\n",
    "    w2v_detection[f'{category} precision'] = precision\n",
    "    w2v_detection[f'{category} recall'] = recall\n",
    "    w2v_detection[f'{category} F1'] = F1\n",
    "    w2v_detection[f'{category} accuracy'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f597384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_categories_w2v = \"homonyms_correction_w2v, histexp_correction_w2v, OOV_correction_w2v, infreq_correction_w2v, RWE_correction_w2v, all_correction_w2v, none_correction_w2v, homonyms_correction context_w2v, histexp_correction context_w2v, OOV_correction context_w2v, infreq_correction context_w2v, RWE_correction context_w2v, none_correction context_w2v\".replace('_w2v', '').split(', ')\n",
    "\n",
    "for category in correction_categories_w2v:\n",
    "    right, wrong = int(w2v_correction[f'{category} right']), int(w2v_correction[f'{category} wrong'])    \n",
    "    try:\n",
    "        accuracy = right/(right+wrong)\n",
    "    except ZeroDivisionError:\n",
    "        accuracy = np.nan\n",
    "    w2v_correction[f'{category} accuracy'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fa54299d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>homonyms_correction right</th>\n",
       "      <th>homonyms_correction wrong</th>\n",
       "      <th>homonyms_correction context right</th>\n",
       "      <th>homonyms_correction context wrong</th>\n",
       "      <th>histexp_correction right</th>\n",
       "      <th>histexp_correction wrong</th>\n",
       "      <th>histexp_correction context right</th>\n",
       "      <th>histexp_correction context wrong</th>\n",
       "      <th>OOV_correction right</th>\n",
       "      <th>OOV_correction wrong</th>\n",
       "      <th>OOV_correction context right</th>\n",
       "      <th>OOV_correction context wrong</th>\n",
       "      <th>infreq_correction right</th>\n",
       "      <th>infreq_correction wrong</th>\n",
       "      <th>infreq_correction context right</th>\n",
       "      <th>infreq_correction context wrong</th>\n",
       "      <th>RWE_correction right</th>\n",
       "      <th>RWE_correction wrong</th>\n",
       "      <th>RWE_correction context right</th>\n",
       "      <th>RWE_correction context wrong</th>\n",
       "      <th>all_correction right</th>\n",
       "      <th>all_correction wrong</th>\n",
       "      <th>none_correction right</th>\n",
       "      <th>none_correction wrong</th>\n",
       "      <th>none_correction context right</th>\n",
       "      <th>none_correction context wrong</th>\n",
       "      <th>identifier</th>\n",
       "      <th>century</th>\n",
       "      <th>source</th>\n",
       "      <th>homonyms_correction accuracy</th>\n",
       "      <th>histexp_correction accuracy</th>\n",
       "      <th>OOV_correction accuracy</th>\n",
       "      <th>infreq_correction accuracy</th>\n",
       "      <th>RWE_correction accuracy</th>\n",
       "      <th>all_correction accuracy</th>\n",
       "      <th>none_correction accuracy</th>\n",
       "      <th>homonyms_correction context accuracy</th>\n",
       "      <th>histexp_correction context accuracy</th>\n",
       "      <th>OOV_correction context accuracy</th>\n",
       "      <th>infreq_correction context accuracy</th>\n",
       "      <th>RWE_correction context accuracy</th>\n",
       "      <th>none_correction context accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1600s</td>\n",
       "      <td>Meertens</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   homonyms_correction right  homonyms_correction wrong  \\\n",
       "0                          1                          1   \n",
       "\n",
       "   homonyms_correction context right  homonyms_correction context wrong  \\\n",
       "0                                  1                                  3   \n",
       "\n",
       "   histexp_correction right  histexp_correction wrong  \\\n",
       "0                         5                         3   \n",
       "\n",
       "   histexp_correction context right  histexp_correction context wrong  \\\n",
       "0                                 1                                 5   \n",
       "\n",
       "   OOV_correction right  OOV_correction wrong  OOV_correction context right  \\\n",
       "0                     0                     0                             1   \n",
       "\n",
       "   OOV_correction context wrong  infreq_correction right  \\\n",
       "0                             4                        0   \n",
       "\n",
       "   infreq_correction wrong  infreq_correction context right  \\\n",
       "0                        0                                0   \n",
       "\n",
       "   infreq_correction context wrong  RWE_correction right  \\\n",
       "0                                0                     5   \n",
       "\n",
       "   RWE_correction wrong  RWE_correction context right  \\\n",
       "0                     3                             0   \n",
       "\n",
       "   RWE_correction context wrong  all_correction right  all_correction wrong  \\\n",
       "0                             0                     7                     5   \n",
       "\n",
       "   none_correction right  none_correction wrong  \\\n",
       "0                      2                      2   \n",
       "\n",
       "   none_correction context right  none_correction context wrong identifier  \\\n",
       "0                              0                              0        111   \n",
       "\n",
       "  century    source  homonyms_correction accuracy  \\\n",
       "0   1600s  Meertens                           0.5   \n",
       "\n",
       "   histexp_correction accuracy  OOV_correction accuracy  \\\n",
       "0                        0.625                      NaN   \n",
       "\n",
       "   infreq_correction accuracy  RWE_correction accuracy  \\\n",
       "0                         NaN                    0.625   \n",
       "\n",
       "   all_correction accuracy  none_correction accuracy  \\\n",
       "0                 0.583333                       0.5   \n",
       "\n",
       "   homonyms_correction context accuracy  histexp_correction context accuracy  \\\n",
       "0                                  0.25                             0.166667   \n",
       "\n",
       "   OOV_correction context accuracy  infreq_correction context accuracy  \\\n",
       "0                              0.2                                 NaN   \n",
       "\n",
       "   RWE_correction context accuracy  none_correction context accuracy  \n",
       "0                              NaN                               NaN  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "w2v_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "56aef808",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_categories_w2v = \"homonyms_detection_w2v, histexp_detection_w2v, OOV_detection_w2v, infreq_detection_w2v, RWE_detection_w2v, all_detection_w2v, none_detection_w2v, homonyms_detection_context_w2v, histexp_detection_context_w2v, OOV_detection_context_w2v, infreq_detection_context_w2v, RWE_detection_context_w2v, none_detection_context_w2v\".replace('_w2v', '').split(', ')\n",
    "correction_categories_w2v = \"homonyms_correction_w2v, histexp_correction_w2v, OOV_correction_w2v, infreq_correction_w2v, RWE_correction_w2v, all_correction_w2v, none_correction_w2v, homonyms_correction_context_w2v, histexp_correction_context_w2v, OOV_correction_context_w2v, infreq_correction_context_w2v, RWE_correction_context_w2v, none_correction_context_w2v\".replace('_w2v', '').split(', ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8a27ce81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>century</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111</td>\n",
       "      <td>1600s</td>\n",
       "      <td>Meertens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  identifier century    source\n",
       "0        111   1600s  Meertens"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3bf991d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = w2v_detection.filter(regex='homonyms|OOV|all').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c3378cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'corrected document': new_documents, 'gt text': list(df_probeer['gt text']),'identifier': list(df_probeer[df_probeer[\"set\"] == 'test']['identifier']), 'century': list(df_probeer[df_probeer[\"set\"] == 'test']['century']), 'source': list(df_probeer[df_probeer[\"set\"] == 'test']['source']), \\\n",
    "    'old WER': [0.20], 'old CER': [0.30]}\n",
    "whole_task_w2v = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4c41c9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corrected document</th>\n",
       "      <th>gt text</th>\n",
       "      <th>identifier</th>\n",
       "      <th>century</th>\n",
       "      <th>source</th>\n",
       "      <th>old WER</th>\n",
       "      <th>old CER</th>\n",
       "      <th>CER after correction</th>\n",
       "      <th>WER after correction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12 den schekman of korige van is den verplantt...</td>\n",
       "      <td>12 een koekenpan of kortweg pan is een platte ...</td>\n",
       "      <td>111</td>\n",
       "      <td>1600s</td>\n",
       "      <td>Meertens</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  corrected document  \\\n",
       "0  12 den schekman of korige van is den verplantt...   \n",
       "\n",
       "                                             gt text identifier century  \\\n",
       "0  12 een koekenpan of kortweg pan is een platte ...        111   1600s   \n",
       "\n",
       "     source  old WER  old CER  CER after correction  WER after correction  \n",
       "0  Meertens      0.2      0.3                   0.0                   0.0  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_task_w2v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1f8414cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12 een koekenpan of kortweg pan is een platte ...\n",
       "Name: gt text, dtype: object"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "291084c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corrected document</th>\n",
       "      <th>gt text</th>\n",
       "      <th>identifier</th>\n",
       "      <th>century</th>\n",
       "      <th>source</th>\n",
       "      <th>old WER</th>\n",
       "      <th>old CER</th>\n",
       "      <th>CER after correction</th>\n",
       "      <th>WER after correction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12 den schekman of korige van is den verplantt...</td>\n",
       "      <td>12 een koekenpan of kortweg pan is een platte ...</td>\n",
       "      <td>111</td>\n",
       "      <td>1600s</td>\n",
       "      <td>Meertens</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>33.47</td>\n",
       "      <td>66.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  corrected document  \\\n",
       "0  12 den schekman of korige van is den verplantt...   \n",
       "\n",
       "                                             gt text identifier century  \\\n",
       "0  12 een koekenpan of kortweg pan is een platte ...        111   1600s   \n",
       "\n",
       "     source  old WER  old CER  CER after correction  WER after correction  \n",
       "0  Meertens      0.2      0.3                 33.47                 66.67  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jar_file = \"ocrevalUAtion-1.3.4-jar-with-dependencies.jar\"\n",
    "\n",
    "def evaluation(index, row):\n",
    "    ID = row['identifier']\n",
    "    page = 'None'\n",
    "    corrected_OCR = re.sub(' +', ' ', str(row['corrected document'].replace('.', '')))\n",
    "    gt_text = re.sub(' +', ' ', str(row['gt text'].replace('.', '')))\n",
    "    filename_ocr = f\"{ID}_{page}_OCR.txt\"\n",
    "    #file_ocr = open(os.path.join(save_path, filename),\"w+\", encoding=\"utf-8\")\n",
    "    file_ocr = open(filename_ocr,\"w+\", encoding=\"utf-8\")\n",
    "    file_ocr.write(corrected_OCR)\n",
    "    file_ocr.close()\n",
    "    \n",
    "    filename_gt = f\"{ID}_{page}_GT.txt\"\n",
    "    #file_gt = open(os.path.join(save_path, filename),\"w+\", encoding=\"utf-8\")\n",
    "    file_gt = open(filename_gt,\"w+\", encoding=\"utf-8\")\n",
    "    file_gt.write(gt_text)\n",
    "    file_gt.close()\n",
    "    \n",
    "    #output = ID + '_' + page + \".html\"\n",
    "    output = f\"{ID}_{page}.html\"\n",
    "    \n",
    "    #process = subprocess.call(\"/home/nvanthof/jdk-16.0.1/bin/java -cp \" + jar_file  + \" eu.digitisation.Main -gt \" + filename_gt + \" -ocr \"+ filename_ocr +\" -o \" + output + \"\")\n",
    "    #os.system(\"/home/nvanthof/jdk-16.0.1/bin/java -cp /home/nvanthof/ocrevalUAtion-1.3.4-jar-with-dependencies.jar eu.digitisation.Main -gt /home/nvanthof/ddd.010728187.mpeg21.a0005_None_GT.txt -ocr /home/nvanthof/ddd.010728187.mpeg21.a0005_None_OCR.txt  -o /home/nvanthof/OUTPUT2.html\")\n",
    "    command = f\"/home/nvanthof/jdk-16.0.1/bin/java -cp /home/nvanthof/ocrevalUAtion-1.3.4-jar-with-dependencies.jar eu.digitisation.Main -gt /home/nvanthof/{filename_gt} -ocr /home/nvanthof/{filename_ocr}  -o /home/nvanthof/{output}\"\n",
    "    os.system(command)\n",
    "    sleep(5)\n",
    "    \n",
    "    soup = BeautifulSoup(open(output, encoding='utf-8'))\n",
    "    table = soup.find(\"table\", attrs={'border': '1'})\n",
    "    # Split the filename, and extract the identifier and pagenr together as identifier \n",
    "    # Find the first table (this is the table in which the scores are stored)\n",
    "    # Find the tags in which 'CER', 'WER', and 'WER (order independent)' are stored and take the next tag to get the score \n",
    "    cer = table.find('td', text='CER')\n",
    "    cerScore = cer.findNext('td')\n",
    "    wer = table.find('td', text='WER')\n",
    "    werScore = wer.findNext('td')\n",
    "    werOI = table.find('td', text='WER (order independent)')\n",
    "    werOIScore = werOI.findNext('td')\n",
    "    \n",
    "    os.remove(filename_gt)\n",
    "    os.remove(filename_ocr)\n",
    "    os.remove(output)\n",
    "    return float(cerScore.text), float(werScore.text)   \n",
    "    \n",
    "    return cerScore.text, werScore.text\n",
    "\n",
    "for index, row in whole_task_w2v.iterrows():\n",
    "    print(index)\n",
    "    if index%1000 == 0:\n",
    "        print(index)\n",
    "    whole_task_w2v.at[index, 'CER after correction'], whole_task_w2v.at[index, 'WER after correction'] = evaluation(index, row)\n",
    "    \n",
    "whole_task_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "09467110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corrected document</th>\n",
       "      <th>gt text</th>\n",
       "      <th>identifier</th>\n",
       "      <th>century</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12 den schekman of korige van is den verplantt...</td>\n",
       "      <td>12 een koekenpan of kortweg pan is een platte ...</td>\n",
       "      <td>111</td>\n",
       "      <td>1600s</td>\n",
       "      <td>Meertens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  corrected document  \\\n",
       "0  12 den schekman of korige van is den verplantt...   \n",
       "\n",
       "                                             gt text identifier century  \\\n",
       "0  12 een koekenpan of kortweg pan is een platte ...        111   1600s   \n",
       "\n",
       "     source  \n",
       "0  Meertens  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_task_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "49a16bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wt augs-burgh den 21 dito dat den generael coninghs-merck ende land-graef frederich van hessen de keyserlicke arriergarde van eenige regimenten cavallerije ende infanterije inghevallen sijn ende geruineert hebben continueert hebben daer nevens noch 6 stucken ende meest alle bagagie verovert melander is inden eersten doodgebleven davantguarde hadde sich aende smutter bij de beyersche gesalveert aldaer oock noch eenige uyren met verlies aen wedersijden ghevochten is den 16 deses quamen de keysersche ende beyersche voor dese stad die den vyand volcht schietende noch sterck met canon op den anderen des nachts gingen de keysersche over den lech in beyeren oft den vyand volgen sal leert den tijdt '"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gt_file:\n",
    "re.sub(' +', ' ', str(df['gt text'][0].replace('.', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1fc80951",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_text = \"\"\"12 Een koekenpan of kortweg pan is een platte pan met een lang handvat.\n",
    "De pan ontleent zijn naam aan het feit dat in zo'n pan 12 pannenkoeken worden gebakken. Ook ander voedsel, zoals vlees, wordt in een koekenpan gebraden 12 coninghs-merck\"\"\".lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "230fa08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probeer['gt text'] = [gt_text]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
